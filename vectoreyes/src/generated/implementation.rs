// @generated
// rustfmt-format_generated_files: false
// This file was auto-generated by generate.py DO NOT MODIFY
macro_rules! select_impl { (scalar {$($scalar:item)*} avx2 {$($avx2:item)*}) => { $(#[cfg(any(miri, not(all(target_arch="x86_64", all(target_feature = "aes", target_feature = "avx", target_feature = "avx2", target_feature = "pclmulqdq", target_feature = "sse2", target_feature = "sse4.1", target_feature = "sse4.2", target_feature = "ssse3")))))] $scalar)* $(#[cfg(all(not(miri), all(target_arch="x86_64", all(target_feature = "aes", target_feature = "avx", target_feature = "avx2", target_feature = "pclmulqdq", target_feature = "sse2", target_feature = "sse4.1", target_feature = "sse4.2", target_feature = "ssse3"))))] $avx2)* }; }
macro_rules! select_impl_block { (scalar $scalar:block avx2 $avx2:block) => { #[cfg(any(miri, not(all(target_arch="x86_64", all(target_feature = "aes", target_feature = "avx", target_feature = "avx2", target_feature = "pclmulqdq", target_feature = "sse2", target_feature = "sse4.1", target_feature = "sse4.2", target_feature = "ssse3")))))] $scalar #[cfg(all(not(miri), all(target_arch="x86_64", all(target_feature = "aes", target_feature = "avx", target_feature = "avx2", target_feature = "pclmulqdq", target_feature = "sse2", target_feature = "sse4.1", target_feature = "sse4.2", target_feature = "ssse3"))))] $avx2 }; }
use crate::array_utils::*;
use crate::SimdBase;
use std::ops::*;
#[doc = "We need to use a macro to explicitly match every possible immediate value, and then dispatch to\na block which has a const generic value for the intrinsic immediate. The inputs to our\nfunctions need to be manipulated before they passed to the raw intrinsic.\nIt used to be possible to directly manipulate the const before passing it to the intrinsic.\nHowever, https://git.io/JsukV broke compatibility with stable, so we need to do this instead."]
#[allow(unused_macros)]
macro_rules! constify_imm { ($func:path => ( $($normal_args:expr,)* @@ [0..256] $imm_arg:expr )) => { // Hopefully this gets optimized out...
 match $imm_arg { 0 => $func($($normal_args,)* 0), 1 => $func($($normal_args,)* 1), 2 => $func($($normal_args,)* 2), 3 => $func($($normal_args,)* 3), 4 => $func($($normal_args,)* 4), 5 => $func($($normal_args,)* 5), 6 => $func($($normal_args,)* 6), 7 => $func($($normal_args,)* 7), 8 => $func($($normal_args,)* 8), 9 => $func($($normal_args,)* 9), 10 => $func($($normal_args,)* 10), 11 => $func($($normal_args,)* 11), 12 => $func($($normal_args,)* 12), 13 => $func($($normal_args,)* 13), 14 => $func($($normal_args,)* 14), 15 => $func($($normal_args,)* 15), 16 => $func($($normal_args,)* 16), 17 => $func($($normal_args,)* 17), 18 => $func($($normal_args,)* 18), 19 => $func($($normal_args,)* 19), 20 => $func($($normal_args,)* 20), 21 => $func($($normal_args,)* 21), 22 => $func($($normal_args,)* 22), 23 => $func($($normal_args,)* 23), 24 => $func($($normal_args,)* 24), 25 => $func($($normal_args,)* 25), 26 => $func($($normal_args,)* 26), 27 => $func($($normal_args,)* 27), 28 => $func($($normal_args,)* 28), 29 => $func($($normal_args,)* 29), 30 => $func($($normal_args,)* 30), 31 => $func($($normal_args,)* 31), 32 => $func($($normal_args,)* 32), 33 => $func($($normal_args,)* 33), 34 => $func($($normal_args,)* 34), 35 => $func($($normal_args,)* 35), 36 => $func($($normal_args,)* 36), 37 => $func($($normal_args,)* 37), 38 => $func($($normal_args,)* 38), 39 => $func($($normal_args,)* 39), 40 => $func($($normal_args,)* 40), 41 => $func($($normal_args,)* 41), 42 => $func($($normal_args,)* 42), 43 => $func($($normal_args,)* 43), 44 => $func($($normal_args,)* 44), 45 => $func($($normal_args,)* 45), 46 => $func($($normal_args,)* 46), 47 => $func($($normal_args,)* 47), 48 => $func($($normal_args,)* 48), 49 => $func($($normal_args,)* 49), 50 => $func($($normal_args,)* 50), 51 => $func($($normal_args,)* 51), 52 => $func($($normal_args,)* 52), 53 => $func($($normal_args,)* 53), 54 => $func($($normal_args,)* 54), 55 => $func($($normal_args,)* 55), 56 => $func($($normal_args,)* 56), 57 => $func($($normal_args,)* 57), 58 => $func($($normal_args,)* 58), 59 => $func($($normal_args,)* 59), 60 => $func($($normal_args,)* 60), 61 => $func($($normal_args,)* 61), 62 => $func($($normal_args,)* 62), 63 => $func($($normal_args,)* 63), 64 => $func($($normal_args,)* 64), 65 => $func($($normal_args,)* 65), 66 => $func($($normal_args,)* 66), 67 => $func($($normal_args,)* 67), 68 => $func($($normal_args,)* 68), 69 => $func($($normal_args,)* 69), 70 => $func($($normal_args,)* 70), 71 => $func($($normal_args,)* 71), 72 => $func($($normal_args,)* 72), 73 => $func($($normal_args,)* 73), 74 => $func($($normal_args,)* 74), 75 => $func($($normal_args,)* 75), 76 => $func($($normal_args,)* 76), 77 => $func($($normal_args,)* 77), 78 => $func($($normal_args,)* 78), 79 => $func($($normal_args,)* 79), 80 => $func($($normal_args,)* 80), 81 => $func($($normal_args,)* 81), 82 => $func($($normal_args,)* 82), 83 => $func($($normal_args,)* 83), 84 => $func($($normal_args,)* 84), 85 => $func($($normal_args,)* 85), 86 => $func($($normal_args,)* 86), 87 => $func($($normal_args,)* 87), 88 => $func($($normal_args,)* 88), 89 => $func($($normal_args,)* 89), 90 => $func($($normal_args,)* 90), 91 => $func($($normal_args,)* 91), 92 => $func($($normal_args,)* 92), 93 => $func($($normal_args,)* 93), 94 => $func($($normal_args,)* 94), 95 => $func($($normal_args,)* 95), 96 => $func($($normal_args,)* 96), 97 => $func($($normal_args,)* 97), 98 => $func($($normal_args,)* 98), 99 => $func($($normal_args,)* 99), 100 => $func($($normal_args,)* 100), 101 => $func($($normal_args,)* 101), 102 => $func($($normal_args,)* 102), 103 => $func($($normal_args,)* 103), 104 => $func($($normal_args,)* 104), 105 => $func($($normal_args,)* 105), 106 => $func($($normal_args,)* 106), 107 => $func($($normal_args,)* 107), 108 => $func($($normal_args,)* 108), 109 => $func($($normal_args,)* 109), 110 => $func($($normal_args,)* 110), 111 => $func($($normal_args,)* 111), 112 => $func($($normal_args,)* 112), 113 => $func($($normal_args,)* 113), 114 => $func($($normal_args,)* 114), 115 => $func($($normal_args,)* 115), 116 => $func($($normal_args,)* 116), 117 => $func($($normal_args,)* 117), 118 => $func($($normal_args,)* 118), 119 => $func($($normal_args,)* 119), 120 => $func($($normal_args,)* 120), 121 => $func($($normal_args,)* 121), 122 => $func($($normal_args,)* 122), 123 => $func($($normal_args,)* 123), 124 => $func($($normal_args,)* 124), 125 => $func($($normal_args,)* 125), 126 => $func($($normal_args,)* 126), 127 => $func($($normal_args,)* 127), 128 => $func($($normal_args,)* 128), 129 => $func($($normal_args,)* 129), 130 => $func($($normal_args,)* 130), 131 => $func($($normal_args,)* 131), 132 => $func($($normal_args,)* 132), 133 => $func($($normal_args,)* 133), 134 => $func($($normal_args,)* 134), 135 => $func($($normal_args,)* 135), 136 => $func($($normal_args,)* 136), 137 => $func($($normal_args,)* 137), 138 => $func($($normal_args,)* 138), 139 => $func($($normal_args,)* 139), 140 => $func($($normal_args,)* 140), 141 => $func($($normal_args,)* 141), 142 => $func($($normal_args,)* 142), 143 => $func($($normal_args,)* 143), 144 => $func($($normal_args,)* 144), 145 => $func($($normal_args,)* 145), 146 => $func($($normal_args,)* 146), 147 => $func($($normal_args,)* 147), 148 => $func($($normal_args,)* 148), 149 => $func($($normal_args,)* 149), 150 => $func($($normal_args,)* 150), 151 => $func($($normal_args,)* 151), 152 => $func($($normal_args,)* 152), 153 => $func($($normal_args,)* 153), 154 => $func($($normal_args,)* 154), 155 => $func($($normal_args,)* 155), 156 => $func($($normal_args,)* 156), 157 => $func($($normal_args,)* 157), 158 => $func($($normal_args,)* 158), 159 => $func($($normal_args,)* 159), 160 => $func($($normal_args,)* 160), 161 => $func($($normal_args,)* 161), 162 => $func($($normal_args,)* 162), 163 => $func($($normal_args,)* 163), 164 => $func($($normal_args,)* 164), 165 => $func($($normal_args,)* 165), 166 => $func($($normal_args,)* 166), 167 => $func($($normal_args,)* 167), 168 => $func($($normal_args,)* 168), 169 => $func($($normal_args,)* 169), 170 => $func($($normal_args,)* 170), 171 => $func($($normal_args,)* 171), 172 => $func($($normal_args,)* 172), 173 => $func($($normal_args,)* 173), 174 => $func($($normal_args,)* 174), 175 => $func($($normal_args,)* 175), 176 => $func($($normal_args,)* 176), 177 => $func($($normal_args,)* 177), 178 => $func($($normal_args,)* 178), 179 => $func($($normal_args,)* 179), 180 => $func($($normal_args,)* 180), 181 => $func($($normal_args,)* 181), 182 => $func($($normal_args,)* 182), 183 => $func($($normal_args,)* 183), 184 => $func($($normal_args,)* 184), 185 => $func($($normal_args,)* 185), 186 => $func($($normal_args,)* 186), 187 => $func($($normal_args,)* 187), 188 => $func($($normal_args,)* 188), 189 => $func($($normal_args,)* 189), 190 => $func($($normal_args,)* 190), 191 => $func($($normal_args,)* 191), 192 => $func($($normal_args,)* 192), 193 => $func($($normal_args,)* 193), 194 => $func($($normal_args,)* 194), 195 => $func($($normal_args,)* 195), 196 => $func($($normal_args,)* 196), 197 => $func($($normal_args,)* 197), 198 => $func($($normal_args,)* 198), 199 => $func($($normal_args,)* 199), 200 => $func($($normal_args,)* 200), 201 => $func($($normal_args,)* 201), 202 => $func($($normal_args,)* 202), 203 => $func($($normal_args,)* 203), 204 => $func($($normal_args,)* 204), 205 => $func($($normal_args,)* 205), 206 => $func($($normal_args,)* 206), 207 => $func($($normal_args,)* 207), 208 => $func($($normal_args,)* 208), 209 => $func($($normal_args,)* 209), 210 => $func($($normal_args,)* 210), 211 => $func($($normal_args,)* 211), 212 => $func($($normal_args,)* 212), 213 => $func($($normal_args,)* 213), 214 => $func($($normal_args,)* 214), 215 => $func($($normal_args,)* 215), 216 => $func($($normal_args,)* 216), 217 => $func($($normal_args,)* 217), 218 => $func($($normal_args,)* 218), 219 => $func($($normal_args,)* 219), 220 => $func($($normal_args,)* 220), 221 => $func($($normal_args,)* 221), 222 => $func($($normal_args,)* 222), 223 => $func($($normal_args,)* 223), 224 => $func($($normal_args,)* 224), 225 => $func($($normal_args,)* 225), 226 => $func($($normal_args,)* 226), 227 => $func($($normal_args,)* 227), 228 => $func($($normal_args,)* 228), 229 => $func($($normal_args,)* 229), 230 => $func($($normal_args,)* 230), 231 => $func($($normal_args,)* 231), 232 => $func($($normal_args,)* 232), 233 => $func($($normal_args,)* 233), 234 => $func($($normal_args,)* 234), 235 => $func($($normal_args,)* 235), 236 => $func($($normal_args,)* 236), 237 => $func($($normal_args,)* 237), 238 => $func($($normal_args,)* 238), 239 => $func($($normal_args,)* 239), 240 => $func($($normal_args,)* 240), 241 => $func($($normal_args,)* 241), 242 => $func($($normal_args,)* 242), 243 => $func($($normal_args,)* 243), 244 => $func($($normal_args,)* 244), 245 => $func($($normal_args,)* 245), 246 => $func($($normal_args,)* 246), 247 => $func($($normal_args,)* 247), 248 => $func($($normal_args,)* 248), 249 => $func($($normal_args,)* 249), 250 => $func($($normal_args,)* 250), 251 => $func($($normal_args,)* 251), 252 => $func($($normal_args,)* 252), 253 => $func($($normal_args,)* 253), 254 => $func($($normal_args,)* 254), 255 => $func($($normal_args,)* 255), _ => panic!("Invalid immediate: {}. Expected immediate to satisfy: 0..256", $imm_arg), } }; ($func:path => ( $($normal_args:expr,)* @@ [0..32] $imm_arg:expr )) => { // Hopefully this gets optimized out...
 match $imm_arg { 0 => $func($($normal_args,)* 0), 1 => $func($($normal_args,)* 1), 2 => $func($($normal_args,)* 2), 3 => $func($($normal_args,)* 3), 4 => $func($($normal_args,)* 4), 5 => $func($($normal_args,)* 5), 6 => $func($($normal_args,)* 6), 7 => $func($($normal_args,)* 7), 8 => $func($($normal_args,)* 8), 9 => $func($($normal_args,)* 9), 10 => $func($($normal_args,)* 10), 11 => $func($($normal_args,)* 11), 12 => $func($($normal_args,)* 12), 13 => $func($($normal_args,)* 13), 14 => $func($($normal_args,)* 14), 15 => $func($($normal_args,)* 15), 16 => $func($($normal_args,)* 16), 17 => $func($($normal_args,)* 17), 18 => $func($($normal_args,)* 18), 19 => $func($($normal_args,)* 19), 20 => $func($($normal_args,)* 20), 21 => $func($($normal_args,)* 21), 22 => $func($($normal_args,)* 22), 23 => $func($($normal_args,)* 23), 24 => $func($($normal_args,)* 24), 25 => $func($($normal_args,)* 25), 26 => $func($($normal_args,)* 26), 27 => $func($($normal_args,)* 27), 28 => $func($($normal_args,)* 28), 29 => $func($($normal_args,)* 29), 30 => $func($($normal_args,)* 30), 31 => $func($($normal_args,)* 31), _ => panic!("Invalid immediate: {}. Expected immediate to satisfy: 0..32", $imm_arg), } }; ($func:path => ( $($normal_args:expr,)* @@ [0..16] $imm_arg:expr )) => { // Hopefully this gets optimized out...
 match $imm_arg { 0 => $func($($normal_args,)* 0), 1 => $func($($normal_args,)* 1), 2 => $func($($normal_args,)* 2), 3 => $func($($normal_args,)* 3), 4 => $func($($normal_args,)* 4), 5 => $func($($normal_args,)* 5), 6 => $func($($normal_args,)* 6), 7 => $func($($normal_args,)* 7), 8 => $func($($normal_args,)* 8), 9 => $func($($normal_args,)* 9), 10 => $func($($normal_args,)* 10), 11 => $func($($normal_args,)* 11), 12 => $func($($normal_args,)* 12), 13 => $func($($normal_args,)* 13), 14 => $func($($normal_args,)* 14), 15 => $func($($normal_args,)* 15), _ => panic!("Invalid immediate: {}. Expected immediate to satisfy: 0..16", $imm_arg), } }; ($func:path => ( $($normal_args:expr,)* @@ [0..8] $imm_arg:expr )) => { // Hopefully this gets optimized out...
 match $imm_arg { 0 => $func($($normal_args,)* 0), 1 => $func($($normal_args,)* 1), 2 => $func($($normal_args,)* 2), 3 => $func($($normal_args,)* 3), 4 => $func($($normal_args,)* 4), 5 => $func($($normal_args,)* 5), 6 => $func($($normal_args,)* 6), 7 => $func($($normal_args,)* 7), _ => panic!("Invalid immediate: {}. Expected immediate to satisfy: 0..8", $imm_arg), } }; ($func:path => ( $($normal_args:expr,)* @@ [0..4] $imm_arg:expr )) => { // Hopefully this gets optimized out...
 match $imm_arg { 0 => $func($($normal_args,)* 0), 1 => $func($($normal_args,)* 1), 2 => $func($($normal_args,)* 2), 3 => $func($($normal_args,)* 3), _ => panic!("Invalid immediate: {}. Expected immediate to satisfy: 0..4", $imm_arg), } }; ($func:path => ( $($normal_args:expr,)* @@ [0..2] $imm_arg:expr )) => { // Hopefully this gets optimized out...
 match $imm_arg { 0 => $func($($normal_args,)* 0), 1 => $func($($normal_args,)* 1), _ => panic!("Invalid immediate: {}. Expected immediate to satisfy: 0..2", $imm_arg), } }; ($func:path => ( $($normal_args:expr,)* @@ [[1, 2, 4, 8]] $imm_arg:expr )) => { // Hopefully this gets optimized out...
 match $imm_arg { 1 => $func($($normal_args,)* 1), 2 => $func($($normal_args,)* 2), 4 => $func($($normal_args,)* 4), 8 => $func($($normal_args,)* 8), _ => panic!("Invalid immediate: {}. Expected immediate to satisfy: [1, 2, 4, 8]", $imm_arg), } }; }
#[doc = "The backend that is used to evaluate vector operations."]
#[allow(dead_code)]
pub const VECTOR_BACKEND: crate::VectorBackend = {
    select_impl_block! { scalar { crate::VectorBackend::Scalar } avx2 { crate::VectorBackend::Avx2 { micro_architecture: { #[cfg(vectoreyes_target_cpu="skylake")] { crate::MicroArchitecture::Skylake } #[cfg(vectoreyes_target_cpu="skylake-avx512")] { crate::MicroArchitecture::SkylakeAvx512 } #[cfg(vectoreyes_target_cpu="cascadelake")] { crate::MicroArchitecture::CascadeLake } #[cfg(vectoreyes_target_cpu="znver1")] { crate::MicroArchitecture::AmdZenVer1 } #[cfg(vectoreyes_target_cpu="znver2")] { crate::MicroArchitecture::AmdZenVer2 } #[cfg(vectoreyes_target_cpu="znver3")] { crate::MicroArchitecture::AmdZenVer3 } #[cfg(not(any( vectoreyes_target_cpu="skylake", vectoreyes_target_cpu="skylake-avx512", vectoreyes_target_cpu="cascadelake", vectoreyes_target_cpu="znver1", vectoreyes_target_cpu="znver2", vectoreyes_target_cpu="znver3", )))] { crate::MicroArchitecture::Unknown } }, } } }
};
select_impl! { scalar { type I8x16Internal = [i8 ; 16]; } avx2 { type I8x16Internal = ::std::arch::x86_64::__m128i; } }
#[doc = "`[i8; 16]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct I8x16(I8x16Internal);
unsafe impl bytemuck::Pod for I8x16 {}
unsafe impl bytemuck::Zeroable for I8x16 {}
impl PartialEq for I8x16 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for I8x16 {}
impl Default for I8x16 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for I8x16 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for I8x16 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "I8x16({:?})", <[i8; 16]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for I8x16 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for I8x16 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 16];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <i8 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<I8x16> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> I8x16 {
        let mut out = I8x16::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for I8x16 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[i8; 16]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for I8x16 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[i8; 16]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for I8x16 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n     self.as_array()[8] ^ rhs.as_array()[8],\n     self.as_array()[9] ^ rhs.as_array()[9],\n     self.as_array()[10] ^ rhs.as_array()[10],\n     self.as_array()[11] ^ rhs.as_array()[11],\n     self.as_array()[12] ^ rhs.as_array()[12],\n     self.as_array()[13] ^ rhs.as_array()[13],\n     self.as_array()[14] ^ rhs.as_array()[14],\n     self.as_array()[15] ^ rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_xor_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_si128)\n\n\n * `PXOR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], self.as_array()[8] ^ rhs.as_array()[8], self.as_array()[9] ^ rhs.as_array()[9], self.as_array()[10] ^ rhs.as_array()[10], self.as_array()[11] ^ rhs.as_array()[11], self.as_array()[12] ^ rhs.as_array()[12], self.as_array()[13] ^ rhs.as_array()[13], self.as_array()[14] ^ rhs.as_array()[14], self.as_array()[15] ^ rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm_xor_si128 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for I8x16 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n     self.as_array()[8] | rhs.as_array()[8],\n     self.as_array()[9] | rhs.as_array()[9],\n     self.as_array()[10] | rhs.as_array()[10],\n     self.as_array()[11] | rhs.as_array()[11],\n     self.as_array()[12] | rhs.as_array()[12],\n     self.as_array()[13] | rhs.as_array()[13],\n     self.as_array()[14] | rhs.as_array()[14],\n     self.as_array()[15] | rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_or_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_or_si128)\n\n\n * `POR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], self.as_array()[8] | rhs.as_array()[8], self.as_array()[9] | rhs.as_array()[9], self.as_array()[10] | rhs.as_array()[10], self.as_array()[11] | rhs.as_array()[11], self.as_array()[12] | rhs.as_array()[12], self.as_array()[13] | rhs.as_array()[13], self.as_array()[14] | rhs.as_array()[14], self.as_array()[15] | rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm_or_si128 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for I8x16 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n     self.as_array()[8] & rhs.as_array()[8],\n     self.as_array()[9] & rhs.as_array()[9],\n     self.as_array()[10] & rhs.as_array()[10],\n     self.as_array()[11] & rhs.as_array()[11],\n     self.as_array()[12] & rhs.as_array()[12],\n     self.as_array()[13] & rhs.as_array()[13],\n     self.as_array()[14] & rhs.as_array()[14],\n     self.as_array()[15] & rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_and_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_si128)\n\n\n * `PAND xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], self.as_array()[8] & rhs.as_array()[8], self.as_array()[9] & rhs.as_array()[9], self.as_array()[10] & rhs.as_array()[10], self.as_array()[11] & rhs.as_array()[11], self.as_array()[12] & rhs.as_array()[12], self.as_array()[13] & rhs.as_array()[13], self.as_array()[14] & rhs.as_array()[14], self.as_array()[15] & rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm_and_si128 (self.0, rhs.0)) } }
    }
}
impl AddAssign for I8x16 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_add(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_add(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_add(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_add(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_add(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_add(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_add(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_add(rhs.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_add_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_epi8)\n\n\n * `PADDB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), self.as_array()[8].wrapping_add(rhs.as_array()[8]), self.as_array()[9].wrapping_add(rhs.as_array()[9]), self.as_array()[10].wrapping_add(rhs.as_array()[10]), self.as_array()[11].wrapping_add(rhs.as_array()[11]), self.as_array()[12].wrapping_add(rhs.as_array()[12]), self.as_array()[13].wrapping_add(rhs.as_array()[13]), self.as_array()[14].wrapping_add(rhs.as_array()[14]), self.as_array()[15].wrapping_add(rhs.as_array()[15]), ]) } avx2 { Self( avx2::_mm_add_epi8 (self.0, rhs.0)) } }
    }
}
impl SubAssign for I8x16 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_sub(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_sub(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_sub(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_sub(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_sub(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_sub(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_sub(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_sub(rhs.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sub_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_epi8)\n\n\n * `PSUBB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), self.as_array()[8].wrapping_sub(rhs.as_array()[8]), self.as_array()[9].wrapping_sub(rhs.as_array()[9]), self.as_array()[10].wrapping_sub(rhs.as_array()[10]), self.as_array()[11].wrapping_sub(rhs.as_array()[11]), self.as_array()[12].wrapping_sub(rhs.as_array()[12]), self.as_array()[13].wrapping_sub(rhs.as_array()[13]), self.as_array()[14].wrapping_sub(rhs.as_array()[14]), self.as_array()[15].wrapping_sub(rhs.as_array()[15]), ]) } avx2 { Self( avx2::_mm_sub_epi8 (self.0, rhs.0)) } }
    }
}
impl I8x16 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: I8x16 =\n     I8x16::from_array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as i8, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [i8; 16]) -> I8x16 {
        select_impl_block! { scalar { I8x16(array) } avx2 { I8x16(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[i8; 16]> for I8x16 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_loadu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si128)\n\n\n * `MOVDQU xmm, m128`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [i8; 16]) -> I8x16 {
        select_impl_block! { scalar { I8x16(array) } avx2 { I8x16(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_loadu_si128 (array.as_ptr() as *const ::std::arch::x86_64::__m128i) }) } }
    }
}
impl From<I8x16> for [i8; 16] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_storeu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_si128)\n\n\n * `MOVDQU m128, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I8x16) -> [i8; 16] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [i8; 16] = [0; 16]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_storeu_si128 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m128i, vector.0) } out } }
    }
}
impl From<I16x8> for I8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x8\nas little endian bits of I8x16."]
    #[inline(always)]
    fn from(x: I16x8) -> I8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x4> for I8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x4\nas little endian bits of I8x16."]
    #[inline(always)]
    fn from(x: I32x4) -> I8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x2> for I8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x2\nas little endian bits of I8x16."]
    #[inline(always)]
    fn from(x: I64x2) -> I8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for I8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x16\nas little endian bits of I8x16."]
    #[inline(always)]
    fn from(x: U8x16) -> I8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x8> for I8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x8\nas little endian bits of I8x16."]
    #[inline(always)]
    fn from(x: U16x8) -> I8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x4> for I8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x4\nas little endian bits of I8x16."]
    #[inline(always)]
    fn from(x: U32x4) -> I8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x2> for I8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x2\nas little endian bits of I8x16."]
    #[inline(always)]
    fn from(x: U64x2) -> I8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl crate::SimdSaturatingArithmetic for I8x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_add(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_adds_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_adds_epi8)\n\n\n * `PADDSB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_add(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_add(*src); } Self::from(out) } avx2 { Self( avx2::_mm_adds_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_sub(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_subs_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_subs_epi8)\n\n\n * `PSUBSB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_sub(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_sub(*src); } Self::from(out) } avx2 { Self( avx2::_mm_subs_epi8 (self.0, other.0)) } }
    }
} // Static shift
impl ShlAssign<u64> for I8x16 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I8x16\n # {\n if amount >= 8 {\n     I8x16::ZERO\n } else {\n     I8x16::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n         self.as_array()[8] << amount,\n         self.as_array()[9] << amount,\n         self.as_array()[10] << amount,\n         self.as_array()[11] << amount,\n         self.as_array()[12] << amount,\n         self.as_array()[13] << amount,\n         self.as_array()[14] << amount,\n         self.as_array()[15] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: u64) -> I8x16 {
        select_impl_block! { scalar { if amount >= 8 { I8x16::ZERO } else { I8x16::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, ]) } } avx2 { if amount >= 8 { I8x16::ZERO } else { I8x16::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, ]) } } }
    }
} // Variable shift
impl ShlAssign<I8x16> for I8x16 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: I8x16) {
        *self = (*self) << amount;
    }
}
impl Shl<I8x16> for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I8x16  ,\n # )  -> I8x16\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..8).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n I8x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: I8x16) -> I8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x << amm } else { 0 }; } I8x16::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x << amm } else { 0 }; } I8x16::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for I8x16 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I8x16\n # {\n if amount >= 8 {\n     let mut out = self.as_array();\n     for x in out.iter_mut() {\n         *x = if *x < 0 { -1 } else { 0 };\n     }\n     I8x16::from(out)\n } else {\n     I8x16::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n         self.as_array()[8] >> amount,\n         self.as_array()[9] >> amount,\n         self.as_array()[10] >> amount,\n         self.as_array()[11] >> amount,\n         self.as_array()[12] >> amount,\n         self.as_array()[13] >> amount,\n         self.as_array()[14] >> amount,\n         self.as_array()[15] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: u64) -> I8x16 {
        select_impl_block! { scalar { if amount >= 8 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I8x16::from(out) } else { I8x16::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, ]) } } avx2 { if amount >= 8 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I8x16::from(out) } else { I8x16::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, ]) } } }
    }
} // Variable shift
impl ShrAssign<I8x16> for I8x16 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: I8x16) {
        *self = (*self) >> amount;
    }
}
impl Shr<I8x16> for I8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I8x16  ,\n # )  -> I8x16\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..8).contains(&amm) {\n         *x >> amm\n     } else if *x < 0 {\n         -1\n     }  else {\n         0\n     };\n }\n I8x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: I8x16) -> I8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I8x16::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I8x16::from(out) } }
    }
}
impl SimdBase for I8x16 {
    type Scalar = i8;
    type Array = [i8; 16];
    type Signed = I8x16;
    type Unsigned = U8x16;
    const LANES: usize = 16;
    const ZERO: Self = Self::from_array([0; 16]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_testz_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128)\n\n\n * `PTEST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm_testz_si128 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i8  ,\n # )  -> I8x16\n # {\n let mut out = [0; 16];\n out[0] = scalar;\n I8x16::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: i8) -> I8x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; out[0] = scalar; I8x16::from(out) } avx2 { Self( avx2::_mm_set_epi8 ( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, scalar as i8, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i8\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i8\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_extract_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_extract_epi8)\n\n\n * `PEXTRB r32, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> i8 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm_extract_epi8 ::<I>(self.0) as i8 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i8  ,\n # )  -> I8x16\n # {\n I8x16::from([scalar; 16])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set1_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: i8) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([scalar; 16]) } avx2 { Self( avx2::_mm_set1_epi8 (scalar as i8)) } }
    }
    type BroadcastLoInput = I8x16;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([vector.as_array()[0]; 16])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_broadcastb_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_broadcastb_epi8)\n\n\n * `VPBROADCASTB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([vector.as_array()[0]; 16]) } avx2 { Self( avx2::_mm_broadcastb_epi8 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     if self.as_array()[0] == other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  -1  } else { 0 },\n     if self.as_array()[8] == other.as_array()[8] {  -1  } else { 0 },\n     if self.as_array()[9] == other.as_array()[9] {  -1  } else { 0 },\n     if self.as_array()[10] == other.as_array()[10] {  -1  } else { 0 },\n     if self.as_array()[11] == other.as_array()[11] {  -1  } else { 0 },\n     if self.as_array()[12] == other.as_array()[12] {  -1  } else { 0 },\n     if self.as_array()[13] == other.as_array()[13] {  -1  } else { 0 },\n     if self.as_array()[14] == other.as_array()[14] {  -1  } else { 0 },\n     if self.as_array()[15] == other.as_array()[15] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpeq_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpeq_epi8)\n\n\n * `PCMPEQB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ if self.as_array()[0] == other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] == other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] == other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] == other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] == other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] == other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] == other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] == other.as_array()[7] { -1 } else { 0 }, if self.as_array()[8] == other.as_array()[8] { -1 } else { 0 }, if self.as_array()[9] == other.as_array()[9] { -1 } else { 0 }, if self.as_array()[10] == other.as_array()[10] { -1 } else { 0 }, if self.as_array()[11] == other.as_array()[11] { -1 } else { 0 }, if self.as_array()[12] == other.as_array()[12] { -1 } else { 0 }, if self.as_array()[13] == other.as_array()[13] { -1 } else { 0 }, if self.as_array()[14] == other.as_array()[14] { -1 } else { 0 }, if self.as_array()[15] == other.as_array()[15] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpeq_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n     self.as_array()[8] & (!other.as_array()[8]),\n     self.as_array()[9] & (!other.as_array()[9]),\n     self.as_array()[10] & (!other.as_array()[10]),\n     self.as_array()[11] & (!other.as_array()[11]),\n     self.as_array()[12] & (!other.as_array()[12]),\n     self.as_array()[13] & (!other.as_array()[13]),\n     self.as_array()[14] & (!other.as_array()[14]),\n     self.as_array()[15] & (!other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_andnot_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_si128)\n\n\n * `PANDN xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), self.as_array()[8] & (!other.as_array()[8]), self.as_array()[9] & (!other.as_array()[9]), self.as_array()[10] & (!other.as_array()[10]), self.as_array()[11] & (!other.as_array()[11]), self.as_array()[12] & (!other.as_array()[12]), self.as_array()[13] & (!other.as_array()[13]), self.as_array()[14] & (!other.as_array()[14]), self.as_array()[15] & (!other.as_array()[15]), ]) } avx2 { Self( avx2::_mm_andnot_si128 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     if self.as_array()[0] > other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  -1  } else { 0 },\n     if self.as_array()[8] > other.as_array()[8] {  -1  } else { 0 },\n     if self.as_array()[9] > other.as_array()[9] {  -1  } else { 0 },\n     if self.as_array()[10] > other.as_array()[10] {  -1  } else { 0 },\n     if self.as_array()[11] > other.as_array()[11] {  -1  } else { 0 },\n     if self.as_array()[12] > other.as_array()[12] {  -1  } else { 0 },\n     if self.as_array()[13] > other.as_array()[13] {  -1  } else { 0 },\n     if self.as_array()[14] > other.as_array()[14] {  -1  } else { 0 },\n     if self.as_array()[15] > other.as_array()[15] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpgt_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpgt_epi8)\n\n\n * `PCMPGTB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_gt(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ if self.as_array()[0] > other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] > other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] > other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] > other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] > other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] > other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] > other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] > other.as_array()[7] { -1 } else { 0 }, if self.as_array()[8] > other.as_array()[8] { -1 } else { 0 }, if self.as_array()[9] > other.as_array()[9] { -1 } else { 0 }, if self.as_array()[10] > other.as_array()[10] { -1 } else { 0 }, if self.as_array()[11] > other.as_array()[11] { -1 } else { 0 }, if self.as_array()[12] > other.as_array()[12] { -1 } else { 0 }, if self.as_array()[13] > other.as_array()[13] { -1 } else { 0 }, if self.as_array()[14] > other.as_array()[14] { -1 } else { 0 }, if self.as_array()[15] > other.as_array()[15] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpgt_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x16\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n I8x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> I8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I8x16::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I8x16::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x16\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n I8x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> I8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I8x16::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I8x16::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpacklo_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpacklo_epi8)\n\n\n * `PUNPCKLBW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], ]) } avx2 { Self( avx2::_mm_unpacklo_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     // Lane# 0\n     self.as_array()[8],\n     other.as_array()[8],\n     self.as_array()[9],\n     other.as_array()[9],\n     self.as_array()[10],\n     other.as_array()[10],\n     self.as_array()[11],\n     other.as_array()[11],\n     self.as_array()[12],\n     other.as_array()[12],\n     self.as_array()[13],\n     other.as_array()[13],\n     self.as_array()[14],\n     other.as_array()[14],\n     self.as_array()[15],\n     other.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpackhi_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpackhi_epi8)\n\n\n * `PUNPCKHBW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ // Lane# 0
        self.as_array()[8], other.as_array()[8], self.as_array()[9], other.as_array()[9], self.as_array()[10], other.as_array()[10], self.as_array()[11], other.as_array()[11], self.as_array()[12], other.as_array()[12], self.as_array()[13], other.as_array()[13], self.as_array()[14], other.as_array()[14], self.as_array()[15], other.as_array()[15], ]) } avx2 { Self( avx2::_mm_unpackhi_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n     self.as_array()[8].max(other.as_array()[8]),\n     self.as_array()[9].max(other.as_array()[9]),\n     self.as_array()[10].max(other.as_array()[10]),\n     self.as_array()[11].max(other.as_array()[11]),\n     self.as_array()[12].max(other.as_array()[12]),\n     self.as_array()[13].max(other.as_array()[13]),\n     self.as_array()[14].max(other.as_array()[14]),\n     self.as_array()[15].max(other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_max_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epi8)\n\n\n * `PMAXSB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), self.as_array()[8].max(other.as_array()[8]), self.as_array()[9].max(other.as_array()[9]), self.as_array()[10].max(other.as_array()[10]), self.as_array()[11].max(other.as_array()[11]), self.as_array()[12].max(other.as_array()[12]), self.as_array()[13].max(other.as_array()[13]), self.as_array()[14].max(other.as_array()[14]), self.as_array()[15].max(other.as_array()[15]), ]) } avx2 { Self( avx2::_mm_max_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x16  ,\n # )  -> I8x16\n # {\n I8x16::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n     self.as_array()[8].min(other.as_array()[8]),\n     self.as_array()[9].min(other.as_array()[9]),\n     self.as_array()[10].min(other.as_array()[10]),\n     self.as_array()[11].min(other.as_array()[11]),\n     self.as_array()[12].min(other.as_array()[12]),\n     self.as_array()[13].min(other.as_array()[13]),\n     self.as_array()[14].min(other.as_array()[14]),\n     self.as_array()[15].min(other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_min_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_epi8)\n\n\n * `PMINSB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: I8x16) -> I8x16 {
        select_impl_block! { scalar { I8x16::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), self.as_array()[8].min(other.as_array()[8]), self.as_array()[9].min(other.as_array()[9]), self.as_array()[10].min(other.as_array()[10]), self.as_array()[11].min(other.as_array()[11]), self.as_array()[12].min(other.as_array()[12]), self.as_array()[13].min(other.as_array()[13]), self.as_array()[14].min(other.as_array()[14]), self.as_array()[15].min(other.as_array()[15]), ]) } avx2 { Self( avx2::_mm_min_epi8 (self.0, other.0)) } }
    }
}
impl crate::SimdBase8 for I8x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x16\n # {\n let mut out = [0; 16];\n for (out_lane, src_lane) in out\n     .chunks_exact_mut(16)\n     .zip(self.as_array().chunks_exact(16))\n {\n     out_lane[AMOUNT..].copy_from_slice(&src_lane[0..16 - AMOUNT]);\n }\n I8x16::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_slli_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_slli_si128)\n\n\n * `PSLLDQ xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_bytes_left<const AMOUNT: usize>(&self) -> I8x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; for (out_lane, src_lane) in out .chunks_exact_mut(16) .zip(self.as_array().chunks_exact(16)) { out_lane[AMOUNT..].copy_from_slice(&src_lane[0..16 - AMOUNT]); } I8x16::from(out) } avx2 { Self( avx2::_mm_slli_si128 ::<AMOUNT>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x16\n # {\n let mut out = [0; 16];\n for (out_lane, src_lane) in out\n     .chunks_exact_mut(16)\n     .zip(self.as_array().chunks_exact(16))\n {\n     out_lane[0..16 - AMOUNT].copy_from_slice(&src_lane[AMOUNT..]);\n }\n I8x16::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srli_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srli_si128)\n\n\n * `PSRLDQ xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_bytes_right<const AMOUNT: usize>(&self) -> I8x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; for (out_lane, src_lane) in out .chunks_exact_mut(16) .zip(self.as_array().chunks_exact(16)) { out_lane[0..16 - AMOUNT].copy_from_slice(&src_lane[AMOUNT..]); } I8x16::from(out) } avx2 { Self( avx2::_mm_srli_si128 ::<AMOUNT>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> u32\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> u32\n # {\n let mut out: u32 = 0;\n for (i, value) in self.as_array().iter().copied().enumerate() {\n     out |= u32::from((value as u8) >> 7) << i;\n }\n out\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_movemask_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_movemask_epi8)\n\n\n * `PMOVMSKB r32, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn most_significant_bits(&self) -> u32 {
        select_impl_block! { scalar { let mut out: u32 = 0; for (i, value) in self.as_array().iter().copied().enumerate() { out |= u32::from((value as u8) >> 7) << i; } out } avx2 { avx2::_mm_movemask_epi8 (self.0) as u32 } }
    }
}
impl I8x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         order  : U8x16  ,\n # )  -> I8x16\n # ;}\n # impl SomeTraitForDoc for I8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         order  : U8x16  ,\n # )  -> I8x16\n # {\n let mut arr = [0; 16];\n for (lane_dst, (lane_src, order)) in\n     arr.chunks_exact_mut(16).zip(\n         self.as_array().chunks_exact(16)\n         .zip(order.as_array().chunks_exact(16))\n     )\n {\n     for (dst, idx) in lane_dst.iter_mut().zip(order) {\n         let idx = *idx;\n         *dst = if (idx >> 7) == 1 {\n             0\n         } else {\n             lane_src[(idx as usize) % 16]\n         };\n     }\n }\n arr.into()\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_shuffle_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shuffle_epi8)\n\n\n * `PSHUFB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    pub fn shuffle(&self, order: U8x16) -> I8x16 {
        select_impl_block! { scalar { let mut arr = [0; 16]; for (lane_dst, (lane_src, order)) in arr.chunks_exact_mut(16).zip( self.as_array().chunks_exact(16) .zip(order.as_array().chunks_exact(16)) ) { for (dst, idx) in lane_dst.iter_mut().zip(order) { let idx = *idx; *dst = if (idx >> 7) == 1 { 0 } else { lane_src[(idx as usize) % 16] }; } } arr.into() } avx2 { Self( avx2::_mm_shuffle_epi8 (self.0, order.0)) } }
    }
}
select_impl! { scalar { type I8x32Internal = [i8 ; 32]; } avx2 { type I8x32Internal = ::std::arch::x86_64::__m256i; } }
#[doc = "`[i8; 32]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct I8x32(I8x32Internal);
unsafe impl bytemuck::Pod for I8x32 {}
unsafe impl bytemuck::Zeroable for I8x32 {}
impl PartialEq for I8x32 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for I8x32 {}
impl Default for I8x32 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for I8x32 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for I8x32 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "I8x32({:?})", <[i8; 32]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for I8x32 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for I8x32 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 32];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <i8 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<I8x32> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> I8x32 {
        let mut out = I8x32::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for I8x32 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[i8; 32]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for I8x32 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[i8; 32]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for I8x32 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n     self.as_array()[8] ^ rhs.as_array()[8],\n     self.as_array()[9] ^ rhs.as_array()[9],\n     self.as_array()[10] ^ rhs.as_array()[10],\n     self.as_array()[11] ^ rhs.as_array()[11],\n     self.as_array()[12] ^ rhs.as_array()[12],\n     self.as_array()[13] ^ rhs.as_array()[13],\n     self.as_array()[14] ^ rhs.as_array()[14],\n     self.as_array()[15] ^ rhs.as_array()[15],\n     self.as_array()[16] ^ rhs.as_array()[16],\n     self.as_array()[17] ^ rhs.as_array()[17],\n     self.as_array()[18] ^ rhs.as_array()[18],\n     self.as_array()[19] ^ rhs.as_array()[19],\n     self.as_array()[20] ^ rhs.as_array()[20],\n     self.as_array()[21] ^ rhs.as_array()[21],\n     self.as_array()[22] ^ rhs.as_array()[22],\n     self.as_array()[23] ^ rhs.as_array()[23],\n     self.as_array()[24] ^ rhs.as_array()[24],\n     self.as_array()[25] ^ rhs.as_array()[25],\n     self.as_array()[26] ^ rhs.as_array()[26],\n     self.as_array()[27] ^ rhs.as_array()[27],\n     self.as_array()[28] ^ rhs.as_array()[28],\n     self.as_array()[29] ^ rhs.as_array()[29],\n     self.as_array()[30] ^ rhs.as_array()[30],\n     self.as_array()[31] ^ rhs.as_array()[31],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_xor_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_xor_si256)\n\n\n * `VPXOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], self.as_array()[8] ^ rhs.as_array()[8], self.as_array()[9] ^ rhs.as_array()[9], self.as_array()[10] ^ rhs.as_array()[10], self.as_array()[11] ^ rhs.as_array()[11], self.as_array()[12] ^ rhs.as_array()[12], self.as_array()[13] ^ rhs.as_array()[13], self.as_array()[14] ^ rhs.as_array()[14], self.as_array()[15] ^ rhs.as_array()[15], self.as_array()[16] ^ rhs.as_array()[16], self.as_array()[17] ^ rhs.as_array()[17], self.as_array()[18] ^ rhs.as_array()[18], self.as_array()[19] ^ rhs.as_array()[19], self.as_array()[20] ^ rhs.as_array()[20], self.as_array()[21] ^ rhs.as_array()[21], self.as_array()[22] ^ rhs.as_array()[22], self.as_array()[23] ^ rhs.as_array()[23], self.as_array()[24] ^ rhs.as_array()[24], self.as_array()[25] ^ rhs.as_array()[25], self.as_array()[26] ^ rhs.as_array()[26], self.as_array()[27] ^ rhs.as_array()[27], self.as_array()[28] ^ rhs.as_array()[28], self.as_array()[29] ^ rhs.as_array()[29], self.as_array()[30] ^ rhs.as_array()[30], self.as_array()[31] ^ rhs.as_array()[31], ]) } avx2 { Self( avx2::_mm256_xor_si256 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for I8x32 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n     self.as_array()[8] | rhs.as_array()[8],\n     self.as_array()[9] | rhs.as_array()[9],\n     self.as_array()[10] | rhs.as_array()[10],\n     self.as_array()[11] | rhs.as_array()[11],\n     self.as_array()[12] | rhs.as_array()[12],\n     self.as_array()[13] | rhs.as_array()[13],\n     self.as_array()[14] | rhs.as_array()[14],\n     self.as_array()[15] | rhs.as_array()[15],\n     self.as_array()[16] | rhs.as_array()[16],\n     self.as_array()[17] | rhs.as_array()[17],\n     self.as_array()[18] | rhs.as_array()[18],\n     self.as_array()[19] | rhs.as_array()[19],\n     self.as_array()[20] | rhs.as_array()[20],\n     self.as_array()[21] | rhs.as_array()[21],\n     self.as_array()[22] | rhs.as_array()[22],\n     self.as_array()[23] | rhs.as_array()[23],\n     self.as_array()[24] | rhs.as_array()[24],\n     self.as_array()[25] | rhs.as_array()[25],\n     self.as_array()[26] | rhs.as_array()[26],\n     self.as_array()[27] | rhs.as_array()[27],\n     self.as_array()[28] | rhs.as_array()[28],\n     self.as_array()[29] | rhs.as_array()[29],\n     self.as_array()[30] | rhs.as_array()[30],\n     self.as_array()[31] | rhs.as_array()[31],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_or_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_or_si256)\n\n\n * `VPOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], self.as_array()[8] | rhs.as_array()[8], self.as_array()[9] | rhs.as_array()[9], self.as_array()[10] | rhs.as_array()[10], self.as_array()[11] | rhs.as_array()[11], self.as_array()[12] | rhs.as_array()[12], self.as_array()[13] | rhs.as_array()[13], self.as_array()[14] | rhs.as_array()[14], self.as_array()[15] | rhs.as_array()[15], self.as_array()[16] | rhs.as_array()[16], self.as_array()[17] | rhs.as_array()[17], self.as_array()[18] | rhs.as_array()[18], self.as_array()[19] | rhs.as_array()[19], self.as_array()[20] | rhs.as_array()[20], self.as_array()[21] | rhs.as_array()[21], self.as_array()[22] | rhs.as_array()[22], self.as_array()[23] | rhs.as_array()[23], self.as_array()[24] | rhs.as_array()[24], self.as_array()[25] | rhs.as_array()[25], self.as_array()[26] | rhs.as_array()[26], self.as_array()[27] | rhs.as_array()[27], self.as_array()[28] | rhs.as_array()[28], self.as_array()[29] | rhs.as_array()[29], self.as_array()[30] | rhs.as_array()[30], self.as_array()[31] | rhs.as_array()[31], ]) } avx2 { Self( avx2::_mm256_or_si256 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for I8x32 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n     self.as_array()[8] & rhs.as_array()[8],\n     self.as_array()[9] & rhs.as_array()[9],\n     self.as_array()[10] & rhs.as_array()[10],\n     self.as_array()[11] & rhs.as_array()[11],\n     self.as_array()[12] & rhs.as_array()[12],\n     self.as_array()[13] & rhs.as_array()[13],\n     self.as_array()[14] & rhs.as_array()[14],\n     self.as_array()[15] & rhs.as_array()[15],\n     self.as_array()[16] & rhs.as_array()[16],\n     self.as_array()[17] & rhs.as_array()[17],\n     self.as_array()[18] & rhs.as_array()[18],\n     self.as_array()[19] & rhs.as_array()[19],\n     self.as_array()[20] & rhs.as_array()[20],\n     self.as_array()[21] & rhs.as_array()[21],\n     self.as_array()[22] & rhs.as_array()[22],\n     self.as_array()[23] & rhs.as_array()[23],\n     self.as_array()[24] & rhs.as_array()[24],\n     self.as_array()[25] & rhs.as_array()[25],\n     self.as_array()[26] & rhs.as_array()[26],\n     self.as_array()[27] & rhs.as_array()[27],\n     self.as_array()[28] & rhs.as_array()[28],\n     self.as_array()[29] & rhs.as_array()[29],\n     self.as_array()[30] & rhs.as_array()[30],\n     self.as_array()[31] & rhs.as_array()[31],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_and_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_and_si256)\n\n\n * `VPAND ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], self.as_array()[8] & rhs.as_array()[8], self.as_array()[9] & rhs.as_array()[9], self.as_array()[10] & rhs.as_array()[10], self.as_array()[11] & rhs.as_array()[11], self.as_array()[12] & rhs.as_array()[12], self.as_array()[13] & rhs.as_array()[13], self.as_array()[14] & rhs.as_array()[14], self.as_array()[15] & rhs.as_array()[15], self.as_array()[16] & rhs.as_array()[16], self.as_array()[17] & rhs.as_array()[17], self.as_array()[18] & rhs.as_array()[18], self.as_array()[19] & rhs.as_array()[19], self.as_array()[20] & rhs.as_array()[20], self.as_array()[21] & rhs.as_array()[21], self.as_array()[22] & rhs.as_array()[22], self.as_array()[23] & rhs.as_array()[23], self.as_array()[24] & rhs.as_array()[24], self.as_array()[25] & rhs.as_array()[25], self.as_array()[26] & rhs.as_array()[26], self.as_array()[27] & rhs.as_array()[27], self.as_array()[28] & rhs.as_array()[28], self.as_array()[29] & rhs.as_array()[29], self.as_array()[30] & rhs.as_array()[30], self.as_array()[31] & rhs.as_array()[31], ]) } avx2 { Self( avx2::_mm256_and_si256 (self.0, rhs.0)) } }
    }
}
impl AddAssign for I8x32 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_add(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_add(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_add(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_add(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_add(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_add(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_add(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_add(rhs.as_array()[15]),\n     self.as_array()[16].wrapping_add(rhs.as_array()[16]),\n     self.as_array()[17].wrapping_add(rhs.as_array()[17]),\n     self.as_array()[18].wrapping_add(rhs.as_array()[18]),\n     self.as_array()[19].wrapping_add(rhs.as_array()[19]),\n     self.as_array()[20].wrapping_add(rhs.as_array()[20]),\n     self.as_array()[21].wrapping_add(rhs.as_array()[21]),\n     self.as_array()[22].wrapping_add(rhs.as_array()[22]),\n     self.as_array()[23].wrapping_add(rhs.as_array()[23]),\n     self.as_array()[24].wrapping_add(rhs.as_array()[24]),\n     self.as_array()[25].wrapping_add(rhs.as_array()[25]),\n     self.as_array()[26].wrapping_add(rhs.as_array()[26]),\n     self.as_array()[27].wrapping_add(rhs.as_array()[27]),\n     self.as_array()[28].wrapping_add(rhs.as_array()[28]),\n     self.as_array()[29].wrapping_add(rhs.as_array()[29]),\n     self.as_array()[30].wrapping_add(rhs.as_array()[30]),\n     self.as_array()[31].wrapping_add(rhs.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_add_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_add_epi8)\n\n\n * `VPADDB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), self.as_array()[8].wrapping_add(rhs.as_array()[8]), self.as_array()[9].wrapping_add(rhs.as_array()[9]), self.as_array()[10].wrapping_add(rhs.as_array()[10]), self.as_array()[11].wrapping_add(rhs.as_array()[11]), self.as_array()[12].wrapping_add(rhs.as_array()[12]), self.as_array()[13].wrapping_add(rhs.as_array()[13]), self.as_array()[14].wrapping_add(rhs.as_array()[14]), self.as_array()[15].wrapping_add(rhs.as_array()[15]), self.as_array()[16].wrapping_add(rhs.as_array()[16]), self.as_array()[17].wrapping_add(rhs.as_array()[17]), self.as_array()[18].wrapping_add(rhs.as_array()[18]), self.as_array()[19].wrapping_add(rhs.as_array()[19]), self.as_array()[20].wrapping_add(rhs.as_array()[20]), self.as_array()[21].wrapping_add(rhs.as_array()[21]), self.as_array()[22].wrapping_add(rhs.as_array()[22]), self.as_array()[23].wrapping_add(rhs.as_array()[23]), self.as_array()[24].wrapping_add(rhs.as_array()[24]), self.as_array()[25].wrapping_add(rhs.as_array()[25]), self.as_array()[26].wrapping_add(rhs.as_array()[26]), self.as_array()[27].wrapping_add(rhs.as_array()[27]), self.as_array()[28].wrapping_add(rhs.as_array()[28]), self.as_array()[29].wrapping_add(rhs.as_array()[29]), self.as_array()[30].wrapping_add(rhs.as_array()[30]), self.as_array()[31].wrapping_add(rhs.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_add_epi8 (self.0, rhs.0)) } }
    }
}
impl SubAssign for I8x32 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_sub(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_sub(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_sub(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_sub(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_sub(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_sub(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_sub(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_sub(rhs.as_array()[15]),\n     self.as_array()[16].wrapping_sub(rhs.as_array()[16]),\n     self.as_array()[17].wrapping_sub(rhs.as_array()[17]),\n     self.as_array()[18].wrapping_sub(rhs.as_array()[18]),\n     self.as_array()[19].wrapping_sub(rhs.as_array()[19]),\n     self.as_array()[20].wrapping_sub(rhs.as_array()[20]),\n     self.as_array()[21].wrapping_sub(rhs.as_array()[21]),\n     self.as_array()[22].wrapping_sub(rhs.as_array()[22]),\n     self.as_array()[23].wrapping_sub(rhs.as_array()[23]),\n     self.as_array()[24].wrapping_sub(rhs.as_array()[24]),\n     self.as_array()[25].wrapping_sub(rhs.as_array()[25]),\n     self.as_array()[26].wrapping_sub(rhs.as_array()[26]),\n     self.as_array()[27].wrapping_sub(rhs.as_array()[27]),\n     self.as_array()[28].wrapping_sub(rhs.as_array()[28]),\n     self.as_array()[29].wrapping_sub(rhs.as_array()[29]),\n     self.as_array()[30].wrapping_sub(rhs.as_array()[30]),\n     self.as_array()[31].wrapping_sub(rhs.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sub_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sub_epi8)\n\n\n * `VPSUBB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), self.as_array()[8].wrapping_sub(rhs.as_array()[8]), self.as_array()[9].wrapping_sub(rhs.as_array()[9]), self.as_array()[10].wrapping_sub(rhs.as_array()[10]), self.as_array()[11].wrapping_sub(rhs.as_array()[11]), self.as_array()[12].wrapping_sub(rhs.as_array()[12]), self.as_array()[13].wrapping_sub(rhs.as_array()[13]), self.as_array()[14].wrapping_sub(rhs.as_array()[14]), self.as_array()[15].wrapping_sub(rhs.as_array()[15]), self.as_array()[16].wrapping_sub(rhs.as_array()[16]), self.as_array()[17].wrapping_sub(rhs.as_array()[17]), self.as_array()[18].wrapping_sub(rhs.as_array()[18]), self.as_array()[19].wrapping_sub(rhs.as_array()[19]), self.as_array()[20].wrapping_sub(rhs.as_array()[20]), self.as_array()[21].wrapping_sub(rhs.as_array()[21]), self.as_array()[22].wrapping_sub(rhs.as_array()[22]), self.as_array()[23].wrapping_sub(rhs.as_array()[23]), self.as_array()[24].wrapping_sub(rhs.as_array()[24]), self.as_array()[25].wrapping_sub(rhs.as_array()[25]), self.as_array()[26].wrapping_sub(rhs.as_array()[26]), self.as_array()[27].wrapping_sub(rhs.as_array()[27]), self.as_array()[28].wrapping_sub(rhs.as_array()[28]), self.as_array()[29].wrapping_sub(rhs.as_array()[29]), self.as_array()[30].wrapping_sub(rhs.as_array()[30]), self.as_array()[31].wrapping_sub(rhs.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_sub_epi8 (self.0, rhs.0)) } }
    }
}
impl I8x32 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: I8x32 =\n     I8x32::from_array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as i8, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [i8; 32]) -> I8x32 {
        select_impl_block! { scalar { I8x32(array) } avx2 { I8x32(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[i8; 32]> for I8x32 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_loadu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_loadu_si256)\n\n\n * `VMOVDQU ymm, m256`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [i8; 32]) -> I8x32 {
        select_impl_block! { scalar { I8x32(array) } avx2 { I8x32(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_loadu_si256 (array.as_ptr() as *const ::std::arch::x86_64::__m256i) }) } }
    }
}
impl From<I8x32> for [i8; 32] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_storeu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_storeu_si256)\n\n\n * `VMOVDQU m256, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I8x32) -> [i8; 32] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [i8; 32] = [0; 32]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_storeu_si256 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m256i, vector.0) } out } }
    }
}
impl From<I16x16> for I8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x16\nas little endian bits of I8x32."]
    #[inline(always)]
    fn from(x: I16x16) -> I8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x8> for I8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x8\nas little endian bits of I8x32."]
    #[inline(always)]
    fn from(x: I32x8) -> I8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x4> for I8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x4\nas little endian bits of I8x32."]
    #[inline(always)]
    fn from(x: I64x4) -> I8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x32> for I8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x32\nas little endian bits of I8x32."]
    #[inline(always)]
    fn from(x: U8x32) -> I8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x16> for I8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x16\nas little endian bits of I8x32."]
    #[inline(always)]
    fn from(x: U16x16) -> I8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x8> for I8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x8\nas little endian bits of I8x32."]
    #[inline(always)]
    fn from(x: U32x8) -> I8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x4> for I8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x4\nas little endian bits of I8x32."]
    #[inline(always)]
    fn from(x: U64x4) -> I8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I8x16> for I8x32 {
    #[doc = " NOTE: this will _zero_ the upper bits of the destination. Other\n intrinsics are more effcient, but leave the upper bits undefined.\n At present, these more effcient intrinsics are not exposed.\n\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I8x32\n # {\n let mut out = [0; 32];\n out[0..16].copy_from_slice(&vector.as_array());\n I8x32::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_zextsi128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_zextsi128_si256)\n\n\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I8x16) -> I8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; out[0..16].copy_from_slice(&vector.as_array()); I8x32::from(out) } avx2 { Self( avx2::_mm256_zextsi128_si256 (vector.0)) } }
    }
}
impl From<[I8x16; 2]> for I8x32 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vectors  : [I8x16; 2]  ,\n # )  -> I8x32\n # {\n let mut out = [0; 32];\n out[0..16].copy_from_slice(&vectors[0].as_array());\n out[16..].copy_from_slice(&vectors[1].as_array());\n I8x32::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_m128i`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_m128i)\n\n\n * `VINSERTF128 ymm, ymm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vectors: [I8x16; 2]) -> I8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; out[0..16].copy_from_slice(&vectors[0].as_array()); out[16..].copy_from_slice(&vectors[1].as_array()); I8x32::from(out) } avx2 { Self( avx2::_mm256_set_m128i (vectors[1].0, vectors[0].0)) } }
    }
} // TODO: this doesn't show up in the docs.
impl From<I8x32> for [I8x16; 2] {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x32  ,\n # )  -> [I8x16; 2]\n # {\n let mut lo = [0; 16];\n let mut hi = [0; 16];\n lo.copy_from_slice(&vector.as_array()[0..16]);\n hi.copy_from_slice(&vector.as_array()[16..]);\n [I8x16::from(lo), I8x16::from(hi)]\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extracti128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extracti128_si256)\n\n\n * `VEXTRACTI128 xmm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I8x32) -> [I8x16; 2] {
        select_impl_block! { scalar { let mut lo = [0; 16]; let mut hi = [0; 16]; lo.copy_from_slice(&vector.as_array()[0..16]); hi.copy_from_slice(&vector.as_array()[16..]); [I8x16::from(lo), I8x16::from(hi)] } avx2 { [ I8x16( avx2::_mm256_extracti128_si256 ::<0>(vector.0)), I8x16( avx2::_mm256_extracti128_si256 ::<1>(vector.0)), ] } }
    }
}
impl crate::SimdSaturatingArithmetic for I8x32 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_add(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_adds_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_adds_epi8)\n\n\n * `VPADDSB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_add(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_add(*src); } Self::from(out) } avx2 { Self( avx2::_mm256_adds_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_sub(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_subs_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_subs_epi8)\n\n\n * `VPSUBSB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_sub(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_sub(*src); } Self::from(out) } avx2 { Self( avx2::_mm256_subs_epi8 (self.0, other.0)) } }
    }
} // Static shift
impl ShlAssign<u64> for I8x32 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I8x32\n # {\n if amount >= 8 {\n     I8x32::ZERO\n } else {\n     I8x32::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n         self.as_array()[8] << amount,\n         self.as_array()[9] << amount,\n         self.as_array()[10] << amount,\n         self.as_array()[11] << amount,\n         self.as_array()[12] << amount,\n         self.as_array()[13] << amount,\n         self.as_array()[14] << amount,\n         self.as_array()[15] << amount,\n         self.as_array()[16] << amount,\n         self.as_array()[17] << amount,\n         self.as_array()[18] << amount,\n         self.as_array()[19] << amount,\n         self.as_array()[20] << amount,\n         self.as_array()[21] << amount,\n         self.as_array()[22] << amount,\n         self.as_array()[23] << amount,\n         self.as_array()[24] << amount,\n         self.as_array()[25] << amount,\n         self.as_array()[26] << amount,\n         self.as_array()[27] << amount,\n         self.as_array()[28] << amount,\n         self.as_array()[29] << amount,\n         self.as_array()[30] << amount,\n         self.as_array()[31] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: u64) -> I8x32 {
        select_impl_block! { scalar { if amount >= 8 { I8x32::ZERO } else { I8x32::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, self.as_array()[16] << amount, self.as_array()[17] << amount, self.as_array()[18] << amount, self.as_array()[19] << amount, self.as_array()[20] << amount, self.as_array()[21] << amount, self.as_array()[22] << amount, self.as_array()[23] << amount, self.as_array()[24] << amount, self.as_array()[25] << amount, self.as_array()[26] << amount, self.as_array()[27] << amount, self.as_array()[28] << amount, self.as_array()[29] << amount, self.as_array()[30] << amount, self.as_array()[31] << amount, ]) } } avx2 { if amount >= 8 { I8x32::ZERO } else { I8x32::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, self.as_array()[16] << amount, self.as_array()[17] << amount, self.as_array()[18] << amount, self.as_array()[19] << amount, self.as_array()[20] << amount, self.as_array()[21] << amount, self.as_array()[22] << amount, self.as_array()[23] << amount, self.as_array()[24] << amount, self.as_array()[25] << amount, self.as_array()[26] << amount, self.as_array()[27] << amount, self.as_array()[28] << amount, self.as_array()[29] << amount, self.as_array()[30] << amount, self.as_array()[31] << amount, ]) } } }
    }
} // Variable shift
impl ShlAssign<I8x32> for I8x32 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: I8x32) {
        *self = (*self) << amount;
    }
}
impl Shl<I8x32> for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I8x32  ,\n # )  -> I8x32\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..8).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n I8x32::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: I8x32) -> I8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x << amm } else { 0 }; } I8x32::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x << amm } else { 0 }; } I8x32::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for I8x32 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I8x32\n # {\n if amount >= 8 {\n     let mut out = self.as_array();\n     for x in out.iter_mut() {\n         *x = if *x < 0 { -1 } else { 0 };\n     }\n     I8x32::from(out)\n } else {\n     I8x32::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n         self.as_array()[8] >> amount,\n         self.as_array()[9] >> amount,\n         self.as_array()[10] >> amount,\n         self.as_array()[11] >> amount,\n         self.as_array()[12] >> amount,\n         self.as_array()[13] >> amount,\n         self.as_array()[14] >> amount,\n         self.as_array()[15] >> amount,\n         self.as_array()[16] >> amount,\n         self.as_array()[17] >> amount,\n         self.as_array()[18] >> amount,\n         self.as_array()[19] >> amount,\n         self.as_array()[20] >> amount,\n         self.as_array()[21] >> amount,\n         self.as_array()[22] >> amount,\n         self.as_array()[23] >> amount,\n         self.as_array()[24] >> amount,\n         self.as_array()[25] >> amount,\n         self.as_array()[26] >> amount,\n         self.as_array()[27] >> amount,\n         self.as_array()[28] >> amount,\n         self.as_array()[29] >> amount,\n         self.as_array()[30] >> amount,\n         self.as_array()[31] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: u64) -> I8x32 {
        select_impl_block! { scalar { if amount >= 8 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I8x32::from(out) } else { I8x32::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, self.as_array()[16] >> amount, self.as_array()[17] >> amount, self.as_array()[18] >> amount, self.as_array()[19] >> amount, self.as_array()[20] >> amount, self.as_array()[21] >> amount, self.as_array()[22] >> amount, self.as_array()[23] >> amount, self.as_array()[24] >> amount, self.as_array()[25] >> amount, self.as_array()[26] >> amount, self.as_array()[27] >> amount, self.as_array()[28] >> amount, self.as_array()[29] >> amount, self.as_array()[30] >> amount, self.as_array()[31] >> amount, ]) } } avx2 { if amount >= 8 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I8x32::from(out) } else { I8x32::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, self.as_array()[16] >> amount, self.as_array()[17] >> amount, self.as_array()[18] >> amount, self.as_array()[19] >> amount, self.as_array()[20] >> amount, self.as_array()[21] >> amount, self.as_array()[22] >> amount, self.as_array()[23] >> amount, self.as_array()[24] >> amount, self.as_array()[25] >> amount, self.as_array()[26] >> amount, self.as_array()[27] >> amount, self.as_array()[28] >> amount, self.as_array()[29] >> amount, self.as_array()[30] >> amount, self.as_array()[31] >> amount, ]) } } }
    }
} // Variable shift
impl ShrAssign<I8x32> for I8x32 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: I8x32) {
        *self = (*self) >> amount;
    }
}
impl Shr<I8x32> for I8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I8x32  ,\n # )  -> I8x32\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..8).contains(&amm) {\n         *x >> amm\n     } else if *x < 0 {\n         -1\n     }  else {\n         0\n     };\n }\n I8x32::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: I8x32) -> I8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I8x32::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I8x32::from(out) } }
    }
}
impl SimdBase for I8x32 {
    type Scalar = i8;
    type Array = [i8; 32];
    type Signed = I8x32;
    type Unsigned = U8x32;
    const LANES: usize = 32;
    const ZERO: Self = Self::from_array([0; 32]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_testz_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_testz_si256)\n\n\n * `VPTEST ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm256_testz_si256 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i8  ,\n # )  -> I8x32\n # {\n let mut out = [0; 32];\n out[0] = scalar;\n I8x32::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: i8) -> I8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; out[0] = scalar; I8x32::from(out) } avx2 { Self( avx2::_mm256_set_epi8 ( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, scalar as i8, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i8\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i8\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extract_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extract_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> i8 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm256_extract_epi8 ::<I>(self.0) as i8 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i8  ,\n # )  -> I8x32\n # {\n I8x32::from([scalar; 32])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set1_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: i8) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([scalar; 32]) } avx2 { Self( avx2::_mm256_set1_epi8 (scalar as i8)) } }
    }
    type BroadcastLoInput = I8x16;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I8x32\n # {\n I8x32::from([vector.as_array()[0]; 32])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_broadcastb_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_broadcastb_epi8)\n\n\n * `VPBROADCASTB ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: I8x16) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([vector.as_array()[0]; 32]) } avx2 { Self( avx2::_mm256_broadcastb_epi8 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     if self.as_array()[0] == other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  -1  } else { 0 },\n     if self.as_array()[8] == other.as_array()[8] {  -1  } else { 0 },\n     if self.as_array()[9] == other.as_array()[9] {  -1  } else { 0 },\n     if self.as_array()[10] == other.as_array()[10] {  -1  } else { 0 },\n     if self.as_array()[11] == other.as_array()[11] {  -1  } else { 0 },\n     if self.as_array()[12] == other.as_array()[12] {  -1  } else { 0 },\n     if self.as_array()[13] == other.as_array()[13] {  -1  } else { 0 },\n     if self.as_array()[14] == other.as_array()[14] {  -1  } else { 0 },\n     if self.as_array()[15] == other.as_array()[15] {  -1  } else { 0 },\n     if self.as_array()[16] == other.as_array()[16] {  -1  } else { 0 },\n     if self.as_array()[17] == other.as_array()[17] {  -1  } else { 0 },\n     if self.as_array()[18] == other.as_array()[18] {  -1  } else { 0 },\n     if self.as_array()[19] == other.as_array()[19] {  -1  } else { 0 },\n     if self.as_array()[20] == other.as_array()[20] {  -1  } else { 0 },\n     if self.as_array()[21] == other.as_array()[21] {  -1  } else { 0 },\n     if self.as_array()[22] == other.as_array()[22] {  -1  } else { 0 },\n     if self.as_array()[23] == other.as_array()[23] {  -1  } else { 0 },\n     if self.as_array()[24] == other.as_array()[24] {  -1  } else { 0 },\n     if self.as_array()[25] == other.as_array()[25] {  -1  } else { 0 },\n     if self.as_array()[26] == other.as_array()[26] {  -1  } else { 0 },\n     if self.as_array()[27] == other.as_array()[27] {  -1  } else { 0 },\n     if self.as_array()[28] == other.as_array()[28] {  -1  } else { 0 },\n     if self.as_array()[29] == other.as_array()[29] {  -1  } else { 0 },\n     if self.as_array()[30] == other.as_array()[30] {  -1  } else { 0 },\n     if self.as_array()[31] == other.as_array()[31] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpeq_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpeq_epi8)\n\n\n * `VPCMPEQB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ if self.as_array()[0] == other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] == other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] == other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] == other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] == other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] == other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] == other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] == other.as_array()[7] { -1 } else { 0 }, if self.as_array()[8] == other.as_array()[8] { -1 } else { 0 }, if self.as_array()[9] == other.as_array()[9] { -1 } else { 0 }, if self.as_array()[10] == other.as_array()[10] { -1 } else { 0 }, if self.as_array()[11] == other.as_array()[11] { -1 } else { 0 }, if self.as_array()[12] == other.as_array()[12] { -1 } else { 0 }, if self.as_array()[13] == other.as_array()[13] { -1 } else { 0 }, if self.as_array()[14] == other.as_array()[14] { -1 } else { 0 }, if self.as_array()[15] == other.as_array()[15] { -1 } else { 0 }, if self.as_array()[16] == other.as_array()[16] { -1 } else { 0 }, if self.as_array()[17] == other.as_array()[17] { -1 } else { 0 }, if self.as_array()[18] == other.as_array()[18] { -1 } else { 0 }, if self.as_array()[19] == other.as_array()[19] { -1 } else { 0 }, if self.as_array()[20] == other.as_array()[20] { -1 } else { 0 }, if self.as_array()[21] == other.as_array()[21] { -1 } else { 0 }, if self.as_array()[22] == other.as_array()[22] { -1 } else { 0 }, if self.as_array()[23] == other.as_array()[23] { -1 } else { 0 }, if self.as_array()[24] == other.as_array()[24] { -1 } else { 0 }, if self.as_array()[25] == other.as_array()[25] { -1 } else { 0 }, if self.as_array()[26] == other.as_array()[26] { -1 } else { 0 }, if self.as_array()[27] == other.as_array()[27] { -1 } else { 0 }, if self.as_array()[28] == other.as_array()[28] { -1 } else { 0 }, if self.as_array()[29] == other.as_array()[29] { -1 } else { 0 }, if self.as_array()[30] == other.as_array()[30] { -1 } else { 0 }, if self.as_array()[31] == other.as_array()[31] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpeq_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n     self.as_array()[8] & (!other.as_array()[8]),\n     self.as_array()[9] & (!other.as_array()[9]),\n     self.as_array()[10] & (!other.as_array()[10]),\n     self.as_array()[11] & (!other.as_array()[11]),\n     self.as_array()[12] & (!other.as_array()[12]),\n     self.as_array()[13] & (!other.as_array()[13]),\n     self.as_array()[14] & (!other.as_array()[14]),\n     self.as_array()[15] & (!other.as_array()[15]),\n     self.as_array()[16] & (!other.as_array()[16]),\n     self.as_array()[17] & (!other.as_array()[17]),\n     self.as_array()[18] & (!other.as_array()[18]),\n     self.as_array()[19] & (!other.as_array()[19]),\n     self.as_array()[20] & (!other.as_array()[20]),\n     self.as_array()[21] & (!other.as_array()[21]),\n     self.as_array()[22] & (!other.as_array()[22]),\n     self.as_array()[23] & (!other.as_array()[23]),\n     self.as_array()[24] & (!other.as_array()[24]),\n     self.as_array()[25] & (!other.as_array()[25]),\n     self.as_array()[26] & (!other.as_array()[26]),\n     self.as_array()[27] & (!other.as_array()[27]),\n     self.as_array()[28] & (!other.as_array()[28]),\n     self.as_array()[29] & (!other.as_array()[29]),\n     self.as_array()[30] & (!other.as_array()[30]),\n     self.as_array()[31] & (!other.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_andnot_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_andnot_si256)\n\n\n * `VPANDN ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), self.as_array()[8] & (!other.as_array()[8]), self.as_array()[9] & (!other.as_array()[9]), self.as_array()[10] & (!other.as_array()[10]), self.as_array()[11] & (!other.as_array()[11]), self.as_array()[12] & (!other.as_array()[12]), self.as_array()[13] & (!other.as_array()[13]), self.as_array()[14] & (!other.as_array()[14]), self.as_array()[15] & (!other.as_array()[15]), self.as_array()[16] & (!other.as_array()[16]), self.as_array()[17] & (!other.as_array()[17]), self.as_array()[18] & (!other.as_array()[18]), self.as_array()[19] & (!other.as_array()[19]), self.as_array()[20] & (!other.as_array()[20]), self.as_array()[21] & (!other.as_array()[21]), self.as_array()[22] & (!other.as_array()[22]), self.as_array()[23] & (!other.as_array()[23]), self.as_array()[24] & (!other.as_array()[24]), self.as_array()[25] & (!other.as_array()[25]), self.as_array()[26] & (!other.as_array()[26]), self.as_array()[27] & (!other.as_array()[27]), self.as_array()[28] & (!other.as_array()[28]), self.as_array()[29] & (!other.as_array()[29]), self.as_array()[30] & (!other.as_array()[30]), self.as_array()[31] & (!other.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_andnot_si256 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     if self.as_array()[0] > other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  -1  } else { 0 },\n     if self.as_array()[8] > other.as_array()[8] {  -1  } else { 0 },\n     if self.as_array()[9] > other.as_array()[9] {  -1  } else { 0 },\n     if self.as_array()[10] > other.as_array()[10] {  -1  } else { 0 },\n     if self.as_array()[11] > other.as_array()[11] {  -1  } else { 0 },\n     if self.as_array()[12] > other.as_array()[12] {  -1  } else { 0 },\n     if self.as_array()[13] > other.as_array()[13] {  -1  } else { 0 },\n     if self.as_array()[14] > other.as_array()[14] {  -1  } else { 0 },\n     if self.as_array()[15] > other.as_array()[15] {  -1  } else { 0 },\n     if self.as_array()[16] > other.as_array()[16] {  -1  } else { 0 },\n     if self.as_array()[17] > other.as_array()[17] {  -1  } else { 0 },\n     if self.as_array()[18] > other.as_array()[18] {  -1  } else { 0 },\n     if self.as_array()[19] > other.as_array()[19] {  -1  } else { 0 },\n     if self.as_array()[20] > other.as_array()[20] {  -1  } else { 0 },\n     if self.as_array()[21] > other.as_array()[21] {  -1  } else { 0 },\n     if self.as_array()[22] > other.as_array()[22] {  -1  } else { 0 },\n     if self.as_array()[23] > other.as_array()[23] {  -1  } else { 0 },\n     if self.as_array()[24] > other.as_array()[24] {  -1  } else { 0 },\n     if self.as_array()[25] > other.as_array()[25] {  -1  } else { 0 },\n     if self.as_array()[26] > other.as_array()[26] {  -1  } else { 0 },\n     if self.as_array()[27] > other.as_array()[27] {  -1  } else { 0 },\n     if self.as_array()[28] > other.as_array()[28] {  -1  } else { 0 },\n     if self.as_array()[29] > other.as_array()[29] {  -1  } else { 0 },\n     if self.as_array()[30] > other.as_array()[30] {  -1  } else { 0 },\n     if self.as_array()[31] > other.as_array()[31] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpgt_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpgt_epi8)\n\n\n * `VPCMPGTB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_gt(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ if self.as_array()[0] > other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] > other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] > other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] > other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] > other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] > other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] > other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] > other.as_array()[7] { -1 } else { 0 }, if self.as_array()[8] > other.as_array()[8] { -1 } else { 0 }, if self.as_array()[9] > other.as_array()[9] { -1 } else { 0 }, if self.as_array()[10] > other.as_array()[10] { -1 } else { 0 }, if self.as_array()[11] > other.as_array()[11] { -1 } else { 0 }, if self.as_array()[12] > other.as_array()[12] { -1 } else { 0 }, if self.as_array()[13] > other.as_array()[13] { -1 } else { 0 }, if self.as_array()[14] > other.as_array()[14] { -1 } else { 0 }, if self.as_array()[15] > other.as_array()[15] { -1 } else { 0 }, if self.as_array()[16] > other.as_array()[16] { -1 } else { 0 }, if self.as_array()[17] > other.as_array()[17] { -1 } else { 0 }, if self.as_array()[18] > other.as_array()[18] { -1 } else { 0 }, if self.as_array()[19] > other.as_array()[19] { -1 } else { 0 }, if self.as_array()[20] > other.as_array()[20] { -1 } else { 0 }, if self.as_array()[21] > other.as_array()[21] { -1 } else { 0 }, if self.as_array()[22] > other.as_array()[22] { -1 } else { 0 }, if self.as_array()[23] > other.as_array()[23] { -1 } else { 0 }, if self.as_array()[24] > other.as_array()[24] { -1 } else { 0 }, if self.as_array()[25] > other.as_array()[25] { -1 } else { 0 }, if self.as_array()[26] > other.as_array()[26] { -1 } else { 0 }, if self.as_array()[27] > other.as_array()[27] { -1 } else { 0 }, if self.as_array()[28] > other.as_array()[28] { -1 } else { 0 }, if self.as_array()[29] > other.as_array()[29] { -1 } else { 0 }, if self.as_array()[30] > other.as_array()[30] { -1 } else { 0 }, if self.as_array()[31] > other.as_array()[31] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpgt_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x32\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n I8x32::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> I8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I8x32::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I8x32::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x32\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n I8x32::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> I8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I8x32::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I8x32::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n     // Lane# 1\n     self.as_array()[16],\n     other.as_array()[16],\n     self.as_array()[17],\n     other.as_array()[17],\n     self.as_array()[18],\n     other.as_array()[18],\n     self.as_array()[19],\n     other.as_array()[19],\n     self.as_array()[20],\n     other.as_array()[20],\n     self.as_array()[21],\n     other.as_array()[21],\n     self.as_array()[22],\n     other.as_array()[22],\n     self.as_array()[23],\n     other.as_array()[23],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpacklo_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpacklo_epi8)\n\n\n * `VPUNPCKLBW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], // Lane# 1
        self.as_array()[16], other.as_array()[16], self.as_array()[17], other.as_array()[17], self.as_array()[18], other.as_array()[18], self.as_array()[19], other.as_array()[19], self.as_array()[20], other.as_array()[20], self.as_array()[21], other.as_array()[21], self.as_array()[22], other.as_array()[22], self.as_array()[23], other.as_array()[23], ]) } avx2 { Self( avx2::_mm256_unpacklo_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     // Lane# 0\n     self.as_array()[8],\n     other.as_array()[8],\n     self.as_array()[9],\n     other.as_array()[9],\n     self.as_array()[10],\n     other.as_array()[10],\n     self.as_array()[11],\n     other.as_array()[11],\n     self.as_array()[12],\n     other.as_array()[12],\n     self.as_array()[13],\n     other.as_array()[13],\n     self.as_array()[14],\n     other.as_array()[14],\n     self.as_array()[15],\n     other.as_array()[15],\n     // Lane# 1\n     self.as_array()[24],\n     other.as_array()[24],\n     self.as_array()[25],\n     other.as_array()[25],\n     self.as_array()[26],\n     other.as_array()[26],\n     self.as_array()[27],\n     other.as_array()[27],\n     self.as_array()[28],\n     other.as_array()[28],\n     self.as_array()[29],\n     other.as_array()[29],\n     self.as_array()[30],\n     other.as_array()[30],\n     self.as_array()[31],\n     other.as_array()[31],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpackhi_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpackhi_epi8)\n\n\n * `VPUNPCKHBW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ // Lane# 0
        self.as_array()[8], other.as_array()[8], self.as_array()[9], other.as_array()[9], self.as_array()[10], other.as_array()[10], self.as_array()[11], other.as_array()[11], self.as_array()[12], other.as_array()[12], self.as_array()[13], other.as_array()[13], self.as_array()[14], other.as_array()[14], self.as_array()[15], other.as_array()[15], // Lane# 1
        self.as_array()[24], other.as_array()[24], self.as_array()[25], other.as_array()[25], self.as_array()[26], other.as_array()[26], self.as_array()[27], other.as_array()[27], self.as_array()[28], other.as_array()[28], self.as_array()[29], other.as_array()[29], self.as_array()[30], other.as_array()[30], self.as_array()[31], other.as_array()[31], ]) } avx2 { Self( avx2::_mm256_unpackhi_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n     self.as_array()[8].max(other.as_array()[8]),\n     self.as_array()[9].max(other.as_array()[9]),\n     self.as_array()[10].max(other.as_array()[10]),\n     self.as_array()[11].max(other.as_array()[11]),\n     self.as_array()[12].max(other.as_array()[12]),\n     self.as_array()[13].max(other.as_array()[13]),\n     self.as_array()[14].max(other.as_array()[14]),\n     self.as_array()[15].max(other.as_array()[15]),\n     self.as_array()[16].max(other.as_array()[16]),\n     self.as_array()[17].max(other.as_array()[17]),\n     self.as_array()[18].max(other.as_array()[18]),\n     self.as_array()[19].max(other.as_array()[19]),\n     self.as_array()[20].max(other.as_array()[20]),\n     self.as_array()[21].max(other.as_array()[21]),\n     self.as_array()[22].max(other.as_array()[22]),\n     self.as_array()[23].max(other.as_array()[23]),\n     self.as_array()[24].max(other.as_array()[24]),\n     self.as_array()[25].max(other.as_array()[25]),\n     self.as_array()[26].max(other.as_array()[26]),\n     self.as_array()[27].max(other.as_array()[27]),\n     self.as_array()[28].max(other.as_array()[28]),\n     self.as_array()[29].max(other.as_array()[29]),\n     self.as_array()[30].max(other.as_array()[30]),\n     self.as_array()[31].max(other.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_max_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_max_epi8)\n\n\n * `VPMAXSB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), self.as_array()[8].max(other.as_array()[8]), self.as_array()[9].max(other.as_array()[9]), self.as_array()[10].max(other.as_array()[10]), self.as_array()[11].max(other.as_array()[11]), self.as_array()[12].max(other.as_array()[12]), self.as_array()[13].max(other.as_array()[13]), self.as_array()[14].max(other.as_array()[14]), self.as_array()[15].max(other.as_array()[15]), self.as_array()[16].max(other.as_array()[16]), self.as_array()[17].max(other.as_array()[17]), self.as_array()[18].max(other.as_array()[18]), self.as_array()[19].max(other.as_array()[19]), self.as_array()[20].max(other.as_array()[20]), self.as_array()[21].max(other.as_array()[21]), self.as_array()[22].max(other.as_array()[22]), self.as_array()[23].max(other.as_array()[23]), self.as_array()[24].max(other.as_array()[24]), self.as_array()[25].max(other.as_array()[25]), self.as_array()[26].max(other.as_array()[26]), self.as_array()[27].max(other.as_array()[27]), self.as_array()[28].max(other.as_array()[28]), self.as_array()[29].max(other.as_array()[29]), self.as_array()[30].max(other.as_array()[30]), self.as_array()[31].max(other.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_max_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I8x32  ,\n # )  -> I8x32\n # {\n I8x32::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n     self.as_array()[8].min(other.as_array()[8]),\n     self.as_array()[9].min(other.as_array()[9]),\n     self.as_array()[10].min(other.as_array()[10]),\n     self.as_array()[11].min(other.as_array()[11]),\n     self.as_array()[12].min(other.as_array()[12]),\n     self.as_array()[13].min(other.as_array()[13]),\n     self.as_array()[14].min(other.as_array()[14]),\n     self.as_array()[15].min(other.as_array()[15]),\n     self.as_array()[16].min(other.as_array()[16]),\n     self.as_array()[17].min(other.as_array()[17]),\n     self.as_array()[18].min(other.as_array()[18]),\n     self.as_array()[19].min(other.as_array()[19]),\n     self.as_array()[20].min(other.as_array()[20]),\n     self.as_array()[21].min(other.as_array()[21]),\n     self.as_array()[22].min(other.as_array()[22]),\n     self.as_array()[23].min(other.as_array()[23]),\n     self.as_array()[24].min(other.as_array()[24]),\n     self.as_array()[25].min(other.as_array()[25]),\n     self.as_array()[26].min(other.as_array()[26]),\n     self.as_array()[27].min(other.as_array()[27]),\n     self.as_array()[28].min(other.as_array()[28]),\n     self.as_array()[29].min(other.as_array()[29]),\n     self.as_array()[30].min(other.as_array()[30]),\n     self.as_array()[31].min(other.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_min_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_min_epi8)\n\n\n * `VPMINSB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: I8x32) -> I8x32 {
        select_impl_block! { scalar { I8x32::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), self.as_array()[8].min(other.as_array()[8]), self.as_array()[9].min(other.as_array()[9]), self.as_array()[10].min(other.as_array()[10]), self.as_array()[11].min(other.as_array()[11]), self.as_array()[12].min(other.as_array()[12]), self.as_array()[13].min(other.as_array()[13]), self.as_array()[14].min(other.as_array()[14]), self.as_array()[15].min(other.as_array()[15]), self.as_array()[16].min(other.as_array()[16]), self.as_array()[17].min(other.as_array()[17]), self.as_array()[18].min(other.as_array()[18]), self.as_array()[19].min(other.as_array()[19]), self.as_array()[20].min(other.as_array()[20]), self.as_array()[21].min(other.as_array()[21]), self.as_array()[22].min(other.as_array()[22]), self.as_array()[23].min(other.as_array()[23]), self.as_array()[24].min(other.as_array()[24]), self.as_array()[25].min(other.as_array()[25]), self.as_array()[26].min(other.as_array()[26]), self.as_array()[27].min(other.as_array()[27]), self.as_array()[28].min(other.as_array()[28]), self.as_array()[29].min(other.as_array()[29]), self.as_array()[30].min(other.as_array()[30]), self.as_array()[31].min(other.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_min_epi8 (self.0, other.0)) } }
    }
}
impl crate::SimdBase8 for I8x32 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x32\n # {\n let mut out = [0; 32];\n for (out_lane, src_lane) in out\n     .chunks_exact_mut(16)\n     .zip(self.as_array().chunks_exact(16))\n {\n     out_lane[AMOUNT..].copy_from_slice(&src_lane[0..16 - AMOUNT]);\n }\n I8x32::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_slli_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_slli_si256)\n\n\n * `VPSLLDQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_bytes_left<const AMOUNT: usize>(&self) -> I8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; for (out_lane, src_lane) in out .chunks_exact_mut(16) .zip(self.as_array().chunks_exact(16)) { out_lane[AMOUNT..].copy_from_slice(&src_lane[0..16 - AMOUNT]); } I8x32::from(out) } avx2 { Self( avx2::_mm256_slli_si256 ::<AMOUNT>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I8x32\n # {\n let mut out = [0; 32];\n for (out_lane, src_lane) in out\n     .chunks_exact_mut(16)\n     .zip(self.as_array().chunks_exact(16))\n {\n     out_lane[0..16 - AMOUNT].copy_from_slice(&src_lane[AMOUNT..]);\n }\n I8x32::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srli_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srli_si256)\n\n\n * `VPSRLDQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_bytes_right<const AMOUNT: usize>(&self) -> I8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; for (out_lane, src_lane) in out .chunks_exact_mut(16) .zip(self.as_array().chunks_exact(16)) { out_lane[0..16 - AMOUNT].copy_from_slice(&src_lane[AMOUNT..]); } I8x32::from(out) } avx2 { Self( avx2::_mm256_srli_si256 ::<AMOUNT>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> u32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> u32\n # {\n let mut out: u32 = 0;\n for (i, value) in self.as_array().iter().copied().enumerate() {\n     out |= u32::from((value as u8) >> 7) << i;\n }\n out\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_movemask_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_movemask_epi8)\n\n\n * `VPMOVMSKB r32, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn most_significant_bits(&self) -> u32 {
        select_impl_block! { scalar { let mut out: u32 = 0; for (i, value) in self.as_array().iter().copied().enumerate() { out |= u32::from((value as u8) >> 7) << i; } out } avx2 { avx2::_mm256_movemask_epi8 (self.0) as u32 } }
    }
}
impl I8x32 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         order  : U8x32  ,\n # )  -> I8x32\n # ;}\n # impl SomeTraitForDoc for I8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         order  : U8x32  ,\n # )  -> I8x32\n # {\n let mut arr = [0; 32];\n for (lane_dst, (lane_src, order)) in\n     arr.chunks_exact_mut(16).zip(\n         self.as_array().chunks_exact(16)\n         .zip(order.as_array().chunks_exact(16))\n     )\n {\n     for (dst, idx) in lane_dst.iter_mut().zip(order) {\n         let idx = *idx;\n         *dst = if (idx >> 7) == 1 {\n             0\n         } else {\n             lane_src[(idx as usize) % 16]\n         };\n     }\n }\n arr.into()\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_shuffle_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shuffle_epi8)\n\n\n * `VPSHUFB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    pub fn shuffle(&self, order: U8x32) -> I8x32 {
        select_impl_block! { scalar { let mut arr = [0; 32]; for (lane_dst, (lane_src, order)) in arr.chunks_exact_mut(16).zip( self.as_array().chunks_exact(16) .zip(order.as_array().chunks_exact(16)) ) { for (dst, idx) in lane_dst.iter_mut().zip(order) { let idx = *idx; *dst = if (idx >> 7) == 1 { 0 } else { lane_src[(idx as usize) % 16] }; } } arr.into() } avx2 { Self( avx2::_mm256_shuffle_epi8 (self.0, order.0)) } }
    }
}
select_impl! { scalar { type I16x8Internal = [i16 ; 8]; } avx2 { type I16x8Internal = ::std::arch::x86_64::__m128i; } }
#[doc = "`[i16; 8]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct I16x8(I16x8Internal);
unsafe impl bytemuck::Pod for I16x8 {}
unsafe impl bytemuck::Zeroable for I16x8 {}
impl PartialEq for I16x8 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for I16x8 {}
impl Default for I16x8 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for I16x8 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for I16x8 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "I16x8({:?})", <[i16; 8]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for I16x8 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for I16x8 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 8];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <i16 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<I16x8> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> I16x8 {
        let mut out = I16x8::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for I16x8 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[i16; 8]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for I16x8 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[i16; 8]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for I16x8 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_xor_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_si128)\n\n\n * `PXOR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm_xor_si128 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for I16x8 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_or_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_or_si128)\n\n\n * `POR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm_or_si128 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for I16x8 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_and_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_si128)\n\n\n * `PAND xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm_and_si128 (self.0, rhs.0)) } }
    }
}
impl AddAssign for I16x8 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_add_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_epi16)\n\n\n * `PADDW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), ]) } avx2 { Self( avx2::_mm_add_epi16 (self.0, rhs.0)) } }
    }
}
impl SubAssign for I16x8 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sub_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_epi16)\n\n\n * `PSUBW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), ]) } avx2 { Self( avx2::_mm_sub_epi16 (self.0, rhs.0)) } }
    }
}
impl I16x8 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: I16x8 =\n     I16x8::from_array([0, 1, 2, 3, 4, 5, 6, 7]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as i16, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [i16; 8]) -> I16x8 {
        select_impl_block! { scalar { I16x8(array) } avx2 { I16x8(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[i16; 8]> for I16x8 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_loadu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si128)\n\n\n * `MOVDQU xmm, m128`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [i16; 8]) -> I16x8 {
        select_impl_block! { scalar { I16x8(array) } avx2 { I16x8(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_loadu_si128 (array.as_ptr() as *const ::std::arch::x86_64::__m128i) }) } }
    }
}
impl From<I16x8> for [i16; 8] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_storeu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_si128)\n\n\n * `MOVDQU m128, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I16x8) -> [i16; 8] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [i16; 8] = [0; 8]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_storeu_si128 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m128i, vector.0) } out } }
    }
}
impl From<I8x16> for I16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x16\nas little endian bits of I16x8."]
    #[inline(always)]
    fn from(x: I8x16) -> I16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x4> for I16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x4\nas little endian bits of I16x8."]
    #[inline(always)]
    fn from(x: I32x4) -> I16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x2> for I16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x2\nas little endian bits of I16x8."]
    #[inline(always)]
    fn from(x: I64x2) -> I16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for I16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x16\nas little endian bits of I16x8."]
    #[inline(always)]
    fn from(x: U8x16) -> I16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x8> for I16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x8\nas little endian bits of I16x8."]
    #[inline(always)]
    fn from(x: U16x8) -> I16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x4> for I16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x4\nas little endian bits of I16x8."]
    #[inline(always)]
    fn from(x: U32x4) -> I16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x2> for I16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x2\nas little endian bits of I16x8."]
    #[inline(always)]
    fn from(x: U64x2) -> I16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl crate::ExtendingCast<I8x16> for I16x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I16x8\n # {\n I16x8::from([\n         i16::from(vector.as_array()[0]),\n         i16::from(vector.as_array()[1]),\n         i16::from(vector.as_array()[2]),\n         i16::from(vector.as_array()[3]),\n         i16::from(vector.as_array()[4]),\n         i16::from(vector.as_array()[5]),\n         i16::from(vector.as_array()[6]),\n         i16::from(vector.as_array()[7]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepi8_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepi8_epi16)\n\n\n * `PMOVSXBW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I8x16) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ i16::from(vector.as_array()[0]), i16::from(vector.as_array()[1]), i16::from(vector.as_array()[2]), i16::from(vector.as_array()[3]), i16::from(vector.as_array()[4]), i16::from(vector.as_array()[5]), i16::from(vector.as_array()[6]), i16::from(vector.as_array()[7]), ]) } avx2 { Self( avx2::_mm_cvtepi8_epi16 (vector.0)) } }
    }
}
impl crate::SimdSaturatingArithmetic for I16x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_add(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_adds_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_adds_epi16)\n\n\n * `PADDSW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_add(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_add(*src); } Self::from(out) } avx2 { Self( avx2::_mm_adds_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_sub(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_subs_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_subs_epi16)\n\n\n * `PSUBSW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_sub(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_sub(*src); } Self::from(out) } avx2 { Self( avx2::_mm_subs_epi16 (self.0, other.0)) } }
    }
} // Static shift
impl ShlAssign<u64> for I16x8 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I16x8\n # {\n if amount >= 16 {\n     I16x8::ZERO\n } else {\n     I16x8::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_sll_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sll_epi16)\n\n\n * `PSLLW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> I16x8 {
        select_impl_block! { scalar { if amount >= 16 { I16x8::ZERO } else { I16x8::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_sll_epi16 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<I16x8> for I16x8 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: I16x8) {
        *self = (*self) << amount;
    }
}
impl Shl<I16x8> for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I16x8  ,\n # )  -> I16x8\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..16).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n I16x8::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: I16x8) -> I16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x << amm } else { 0 }; } I16x8::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x << amm } else { 0 }; } I16x8::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for I16x8 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I16x8\n # {\n if amount >= 16 {\n     let mut out = self.as_array();\n     for x in out.iter_mut() {\n         *x = if *x < 0 { -1 } else { 0 };\n     }\n     I16x8::from(out)\n } else {\n     I16x8::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_sra_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sra_epi16)\n\n\n * `PSRAW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> I16x8 {
        select_impl_block! { scalar { if amount >= 16 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I16x8::from(out) } else { I16x8::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_sra_epi16 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<I16x8> for I16x8 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: I16x8) {
        *self = (*self) >> amount;
    }
}
impl Shr<I16x8> for I16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I16x8  ,\n # )  -> I16x8\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..16).contains(&amm) {\n         *x >> amm\n     } else if *x < 0 {\n         -1\n     }  else {\n         0\n     };\n }\n I16x8::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: I16x8) -> I16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I16x8::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I16x8::from(out) } }
    }
}
impl SimdBase for I16x8 {
    type Scalar = i16;
    type Array = [i16; 8];
    type Signed = I16x8;
    type Unsigned = U16x8;
    const LANES: usize = 8;
    const ZERO: Self = Self::from_array([0; 8]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_testz_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128)\n\n\n * `PTEST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm_testz_si128 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i16  ,\n # )  -> I16x8\n # {\n let mut out = [0; 8];\n out[0] = scalar;\n I16x8::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: i16) -> I16x8 {
        select_impl_block! { scalar { let mut out = [0; 8]; out[0] = scalar; I16x8::from(out) } avx2 { Self( avx2::_mm_set_epi16 ( 0, 0, 0, 0, 0, 0, 0, scalar as i16, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i16\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i16\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_extract_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_extract_epi16)\n\n\n * `PEXTRW r32, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> i16 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm_extract_epi16 ::<I>(self.0) as i16 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i16  ,\n # )  -> I16x8\n # {\n I16x8::from([scalar; 8])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set1_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: i16) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([scalar; 8]) } avx2 { Self( avx2::_mm_set1_epi16 (scalar as i16)) } }
    }
    type BroadcastLoInput = I16x8;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([vector.as_array()[0]; 8])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_broadcastw_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_broadcastw_epi16)\n\n\n * `VPBROADCASTW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([vector.as_array()[0]; 8]) } avx2 { Self( avx2::_mm_broadcastw_epi16 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     if self.as_array()[0] == other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpeq_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpeq_epi16)\n\n\n * `PCMPEQW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ if self.as_array()[0] == other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] == other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] == other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] == other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] == other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] == other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] == other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] == other.as_array()[7] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpeq_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_andnot_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_si128)\n\n\n * `PANDN xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), ]) } avx2 { Self( avx2::_mm_andnot_si128 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     if self.as_array()[0] > other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpgt_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpgt_epi16)\n\n\n * `PCMPGTW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_gt(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ if self.as_array()[0] > other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] > other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] > other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] > other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] > other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] > other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] > other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] > other.as_array()[7] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpgt_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x8\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n I16x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_slli_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_slli_epi16)\n\n\n * `PSLLW xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> I16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I16x8::from(out) } avx2 { const USELESS_ARRAY: [u8; 16] = [0; 16]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_slli_epi16 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x8\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n I16x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srai_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srai_epi16)\n\n\n * `PSRAW xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> I16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I16x8::from(out) } avx2 { const USELESS_ARRAY: [u8; 16] = [0; 16]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_srai_epi16 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpacklo_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpacklo_epi16)\n\n\n * `PUNPCKLWD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], ]) } avx2 { Self( avx2::_mm_unpacklo_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     // Lane# 0\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpackhi_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpackhi_epi16)\n\n\n * `PUNPCKHWD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ // Lane# 0
        self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], ]) } avx2 { Self( avx2::_mm_unpackhi_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_max_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epi16)\n\n\n * `PMAXSW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), ]) } avx2 { Self( avx2::_mm_max_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_min_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_epi16)\n\n\n * `PMINSW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: I16x8) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), ]) } avx2 { Self( avx2::_mm_min_epi16 (self.0, other.0)) } }
    }
}
impl crate::SimdBase16 for I16x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     // 128-bit Lane #0\n     self.as_array()[I0 + 0 * 8],\n     self.as_array()[I1 + 0 * 8],\n     self.as_array()[I2 + 0 * 8],\n     self.as_array()[I3 + 0 * 8],\n     self.as_array()[4 + 0 * 8],\n     self.as_array()[5 + 0 * 8],\n     self.as_array()[6 + 0 * 8],\n     self.as_array()[7 + 0 * 8],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_shufflelo_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shufflelo_epi16)\n\n\n * `PSHUFLW xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle_lo<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(
        &self,
    ) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ // 128-bit Lane #0
        self.as_array()[I0 + 0 * 8], self.as_array()[I1 + 0 * 8], self.as_array()[I2 + 0 * 8], self.as_array()[I3 + 0 * 8], self.as_array()[4 + 0 * 8], self.as_array()[5 + 0 * 8], self.as_array()[6 + 0 * 8], self.as_array()[7 + 0 * 8], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm_shufflelo_epi16 ::<I3, I2, I1, I0>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x8\n # {\n I16x8::from([\n     // 128-bit Lane #0\n     self.as_array()[0 + 0 * 8],\n     self.as_array()[1 + 0 * 8],\n     self.as_array()[2 + 0 * 8],\n     self.as_array()[3 + 0 * 8],\n     self.as_array()[I0 + 4 + 0 * 8],\n     self.as_array()[I1 + 4 + 0 * 8],\n     self.as_array()[I2 + 4 + 0 * 8],\n     self.as_array()[I3 + 4 + 0 * 8],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_shufflehi_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shufflehi_epi16)\n\n\n * `PSHUFHW xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle_hi<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(
        &self,
    ) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ // 128-bit Lane #0
        self.as_array()[0 + 0 * 8], self.as_array()[1 + 0 * 8], self.as_array()[2 + 0 * 8], self.as_array()[3 + 0 * 8], self.as_array()[I0 + 4 + 0 * 8], self.as_array()[I1 + 4 + 0 * 8], self.as_array()[I2 + 4 + 0 * 8], self.as_array()[I3 + 4 + 0 * 8], ]) } avx2 { if I0 > 4 { panic!("I0 ({}) > 4", I0); } if I1 > 4 { panic!("I1 ({}) > 4", I1); } if I2 > 4 { panic!("I2 ({}) > 4", I2); } if I3 > 4 { panic!("I3 ({}) > 4", I3); } Self( avx2::_mm_shufflehi_epi16 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
impl crate::SimdBase8x for I16x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const B7: bool,\n #             const B6: bool,\n #             const B5: bool,\n #             const B4: bool,\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : I16x8  ,\n # )  -> I16x8\n # ;}\n # impl SomeTraitForDoc for I16x8 {\n # fn the_doc_function\n #     <\n #             const B7: bool,\n #             const B6: bool,\n #             const B5: bool,\n #             const B4: bool,\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : I16x8  ,\n # )  -> I16x8\n # {\n I16x8::from([\n         (if B0 { if_true } else { *self }).as_array()[0],\n         (if B1 { if_true } else { *self }).as_array()[1],\n         (if B2 { if_true } else { *self }).as_array()[2],\n         (if B3 { if_true } else { *self }).as_array()[3],\n         (if B4 { if_true } else { *self }).as_array()[4],\n         (if B5 { if_true } else { *self }).as_array()[5],\n         (if B6 { if_true } else { *self }).as_array()[6],\n         (if B7 { if_true } else { *self }).as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_blend_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_blend_epi16)\n\n\n * `PBLENDW xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn blend<
        const B7: bool,
        const B6: bool,
        const B5: bool,
        const B4: bool,
        const B3: bool,
        const B2: bool,
        const B1: bool,
        const B0: bool,
    >(
        &self,
        if_true: I16x8,
    ) -> I16x8 {
        select_impl_block! { scalar { I16x8::from([ (if B0 { if_true } else { *self }).as_array()[0], (if B1 { if_true } else { *self }).as_array()[1], (if B2 { if_true } else { *self }).as_array()[2], (if B3 { if_true } else { *self }).as_array()[3], (if B4 { if_true } else { *self }).as_array()[4], (if B5 { if_true } else { *self }).as_array()[5], (if B6 { if_true } else { *self }).as_array()[6], (if B7 { if_true } else { *self }).as_array()[7], ]) } avx2 { Self( avx2::_mm_blend_epi16 ::<B7, B6, B5, B4, B3, B2, B1, B0>(self.0, if_true.0)) } }
    }
}
select_impl! { scalar { type I16x16Internal = [i16 ; 16]; } avx2 { type I16x16Internal = ::std::arch::x86_64::__m256i; } }
#[doc = "`[i16; 16]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct I16x16(I16x16Internal);
unsafe impl bytemuck::Pod for I16x16 {}
unsafe impl bytemuck::Zeroable for I16x16 {}
impl PartialEq for I16x16 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for I16x16 {}
impl Default for I16x16 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for I16x16 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for I16x16 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "I16x16({:?})", <[i16; 16]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for I16x16 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for I16x16 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 16];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <i16 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<I16x16> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> I16x16 {
        let mut out = I16x16::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for I16x16 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[i16; 16]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for I16x16 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[i16; 16]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for I16x16 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n     self.as_array()[8] ^ rhs.as_array()[8],\n     self.as_array()[9] ^ rhs.as_array()[9],\n     self.as_array()[10] ^ rhs.as_array()[10],\n     self.as_array()[11] ^ rhs.as_array()[11],\n     self.as_array()[12] ^ rhs.as_array()[12],\n     self.as_array()[13] ^ rhs.as_array()[13],\n     self.as_array()[14] ^ rhs.as_array()[14],\n     self.as_array()[15] ^ rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_xor_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_xor_si256)\n\n\n * `VPXOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], self.as_array()[8] ^ rhs.as_array()[8], self.as_array()[9] ^ rhs.as_array()[9], self.as_array()[10] ^ rhs.as_array()[10], self.as_array()[11] ^ rhs.as_array()[11], self.as_array()[12] ^ rhs.as_array()[12], self.as_array()[13] ^ rhs.as_array()[13], self.as_array()[14] ^ rhs.as_array()[14], self.as_array()[15] ^ rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm256_xor_si256 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for I16x16 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n     self.as_array()[8] | rhs.as_array()[8],\n     self.as_array()[9] | rhs.as_array()[9],\n     self.as_array()[10] | rhs.as_array()[10],\n     self.as_array()[11] | rhs.as_array()[11],\n     self.as_array()[12] | rhs.as_array()[12],\n     self.as_array()[13] | rhs.as_array()[13],\n     self.as_array()[14] | rhs.as_array()[14],\n     self.as_array()[15] | rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_or_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_or_si256)\n\n\n * `VPOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], self.as_array()[8] | rhs.as_array()[8], self.as_array()[9] | rhs.as_array()[9], self.as_array()[10] | rhs.as_array()[10], self.as_array()[11] | rhs.as_array()[11], self.as_array()[12] | rhs.as_array()[12], self.as_array()[13] | rhs.as_array()[13], self.as_array()[14] | rhs.as_array()[14], self.as_array()[15] | rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm256_or_si256 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for I16x16 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n     self.as_array()[8] & rhs.as_array()[8],\n     self.as_array()[9] & rhs.as_array()[9],\n     self.as_array()[10] & rhs.as_array()[10],\n     self.as_array()[11] & rhs.as_array()[11],\n     self.as_array()[12] & rhs.as_array()[12],\n     self.as_array()[13] & rhs.as_array()[13],\n     self.as_array()[14] & rhs.as_array()[14],\n     self.as_array()[15] & rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_and_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_and_si256)\n\n\n * `VPAND ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], self.as_array()[8] & rhs.as_array()[8], self.as_array()[9] & rhs.as_array()[9], self.as_array()[10] & rhs.as_array()[10], self.as_array()[11] & rhs.as_array()[11], self.as_array()[12] & rhs.as_array()[12], self.as_array()[13] & rhs.as_array()[13], self.as_array()[14] & rhs.as_array()[14], self.as_array()[15] & rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm256_and_si256 (self.0, rhs.0)) } }
    }
}
impl AddAssign for I16x16 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_add(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_add(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_add(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_add(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_add(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_add(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_add(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_add(rhs.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_add_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_add_epi16)\n\n\n * `VPADDW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), self.as_array()[8].wrapping_add(rhs.as_array()[8]), self.as_array()[9].wrapping_add(rhs.as_array()[9]), self.as_array()[10].wrapping_add(rhs.as_array()[10]), self.as_array()[11].wrapping_add(rhs.as_array()[11]), self.as_array()[12].wrapping_add(rhs.as_array()[12]), self.as_array()[13].wrapping_add(rhs.as_array()[13]), self.as_array()[14].wrapping_add(rhs.as_array()[14]), self.as_array()[15].wrapping_add(rhs.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_add_epi16 (self.0, rhs.0)) } }
    }
}
impl SubAssign for I16x16 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_sub(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_sub(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_sub(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_sub(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_sub(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_sub(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_sub(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_sub(rhs.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sub_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sub_epi16)\n\n\n * `VPSUBW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), self.as_array()[8].wrapping_sub(rhs.as_array()[8]), self.as_array()[9].wrapping_sub(rhs.as_array()[9]), self.as_array()[10].wrapping_sub(rhs.as_array()[10]), self.as_array()[11].wrapping_sub(rhs.as_array()[11]), self.as_array()[12].wrapping_sub(rhs.as_array()[12]), self.as_array()[13].wrapping_sub(rhs.as_array()[13]), self.as_array()[14].wrapping_sub(rhs.as_array()[14]), self.as_array()[15].wrapping_sub(rhs.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_sub_epi16 (self.0, rhs.0)) } }
    }
}
impl I16x16 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: I16x16 =\n     I16x16::from_array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as i16, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [i16; 16]) -> I16x16 {
        select_impl_block! { scalar { I16x16(array) } avx2 { I16x16(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[i16; 16]> for I16x16 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_loadu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_loadu_si256)\n\n\n * `VMOVDQU ymm, m256`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [i16; 16]) -> I16x16 {
        select_impl_block! { scalar { I16x16(array) } avx2 { I16x16(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_loadu_si256 (array.as_ptr() as *const ::std::arch::x86_64::__m256i) }) } }
    }
}
impl From<I16x16> for [i16; 16] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_storeu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_storeu_si256)\n\n\n * `VMOVDQU m256, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I16x16) -> [i16; 16] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [i16; 16] = [0; 16]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_storeu_si256 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m256i, vector.0) } out } }
    }
}
impl From<I8x32> for I16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x32\nas little endian bits of I16x16."]
    #[inline(always)]
    fn from(x: I8x32) -> I16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x8> for I16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x8\nas little endian bits of I16x16."]
    #[inline(always)]
    fn from(x: I32x8) -> I16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x4> for I16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x4\nas little endian bits of I16x16."]
    #[inline(always)]
    fn from(x: I64x4) -> I16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x32> for I16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x32\nas little endian bits of I16x16."]
    #[inline(always)]
    fn from(x: U8x32) -> I16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x16> for I16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x16\nas little endian bits of I16x16."]
    #[inline(always)]
    fn from(x: U16x16) -> I16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x8> for I16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x8\nas little endian bits of I16x16."]
    #[inline(always)]
    fn from(x: U32x8) -> I16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x4> for I16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x4\nas little endian bits of I16x16."]
    #[inline(always)]
    fn from(x: U64x4) -> I16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I8x16> for I16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n         i16::from(vector.as_array()[0]),\n         i16::from(vector.as_array()[1]),\n         i16::from(vector.as_array()[2]),\n         i16::from(vector.as_array()[3]),\n         i16::from(vector.as_array()[4]),\n         i16::from(vector.as_array()[5]),\n         i16::from(vector.as_array()[6]),\n         i16::from(vector.as_array()[7]),\n         i16::from(vector.as_array()[8]),\n         i16::from(vector.as_array()[9]),\n         i16::from(vector.as_array()[10]),\n         i16::from(vector.as_array()[11]),\n         i16::from(vector.as_array()[12]),\n         i16::from(vector.as_array()[13]),\n         i16::from(vector.as_array()[14]),\n         i16::from(vector.as_array()[15]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi8_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi8_epi16)\n\n\n * `VPMOVSXBW ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I8x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ i16::from(vector.as_array()[0]), i16::from(vector.as_array()[1]), i16::from(vector.as_array()[2]), i16::from(vector.as_array()[3]), i16::from(vector.as_array()[4]), i16::from(vector.as_array()[5]), i16::from(vector.as_array()[6]), i16::from(vector.as_array()[7]), i16::from(vector.as_array()[8]), i16::from(vector.as_array()[9]), i16::from(vector.as_array()[10]), i16::from(vector.as_array()[11]), i16::from(vector.as_array()[12]), i16::from(vector.as_array()[13]), i16::from(vector.as_array()[14]), i16::from(vector.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_cvtepi8_epi16 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I8x16> for I16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n         i16::from(vector.as_array()[0]),\n         i16::from(vector.as_array()[1]),\n         i16::from(vector.as_array()[2]),\n         i16::from(vector.as_array()[3]),\n         i16::from(vector.as_array()[4]),\n         i16::from(vector.as_array()[5]),\n         i16::from(vector.as_array()[6]),\n         i16::from(vector.as_array()[7]),\n         i16::from(vector.as_array()[8]),\n         i16::from(vector.as_array()[9]),\n         i16::from(vector.as_array()[10]),\n         i16::from(vector.as_array()[11]),\n         i16::from(vector.as_array()[12]),\n         i16::from(vector.as_array()[13]),\n         i16::from(vector.as_array()[14]),\n         i16::from(vector.as_array()[15]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi8_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi8_epi16)\n\n\n * `VPMOVSXBW ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I8x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ i16::from(vector.as_array()[0]), i16::from(vector.as_array()[1]), i16::from(vector.as_array()[2]), i16::from(vector.as_array()[3]), i16::from(vector.as_array()[4]), i16::from(vector.as_array()[5]), i16::from(vector.as_array()[6]), i16::from(vector.as_array()[7]), i16::from(vector.as_array()[8]), i16::from(vector.as_array()[9]), i16::from(vector.as_array()[10]), i16::from(vector.as_array()[11]), i16::from(vector.as_array()[12]), i16::from(vector.as_array()[13]), i16::from(vector.as_array()[14]), i16::from(vector.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_cvtepi8_epi16 (vector.0)) } }
    }
}
impl From<I16x8> for I16x16 {
    #[doc = " NOTE: this will _zero_ the upper bits of the destination. Other\n intrinsics are more effcient, but leave the upper bits undefined.\n At present, these more effcient intrinsics are not exposed.\n\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x8  ,\n # )  -> I16x16\n # {\n let mut out = [0; 16];\n out[0..8].copy_from_slice(&vector.as_array());\n I16x16::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_zextsi128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_zextsi128_si256)\n\n\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I16x8) -> I16x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; out[0..8].copy_from_slice(&vector.as_array()); I16x16::from(out) } avx2 { Self( avx2::_mm256_zextsi128_si256 (vector.0)) } }
    }
}
impl From<[I16x8; 2]> for I16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vectors  : [I16x8; 2]  ,\n # )  -> I16x16\n # {\n let mut out = [0; 16];\n out[0..8].copy_from_slice(&vectors[0].as_array());\n out[8..].copy_from_slice(&vectors[1].as_array());\n I16x16::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_m128i`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_m128i)\n\n\n * `VINSERTF128 ymm, ymm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vectors: [I16x8; 2]) -> I16x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; out[0..8].copy_from_slice(&vectors[0].as_array()); out[8..].copy_from_slice(&vectors[1].as_array()); I16x16::from(out) } avx2 { Self( avx2::_mm256_set_m128i (vectors[1].0, vectors[0].0)) } }
    }
} // TODO: this doesn't show up in the docs.
impl From<I16x16> for [I16x8; 2] {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x16  ,\n # )  -> [I16x8; 2]\n # {\n let mut lo = [0; 8];\n let mut hi = [0; 8];\n lo.copy_from_slice(&vector.as_array()[0..8]);\n hi.copy_from_slice(&vector.as_array()[8..]);\n [I16x8::from(lo), I16x8::from(hi)]\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extracti128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extracti128_si256)\n\n\n * `VEXTRACTI128 xmm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I16x16) -> [I16x8; 2] {
        select_impl_block! { scalar { let mut lo = [0; 8]; let mut hi = [0; 8]; lo.copy_from_slice(&vector.as_array()[0..8]); hi.copy_from_slice(&vector.as_array()[8..]); [I16x8::from(lo), I16x8::from(hi)] } avx2 { [ I16x8( avx2::_mm256_extracti128_si256 ::<0>(vector.0)), I16x8( avx2::_mm256_extracti128_si256 ::<1>(vector.0)), ] } }
    }
}
impl crate::SimdSaturatingArithmetic for I16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_add(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_adds_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_adds_epi16)\n\n\n * `VPADDSW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_add(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_add(*src); } Self::from(out) } avx2 { Self( avx2::_mm256_adds_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_sub(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_subs_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_subs_epi16)\n\n\n * `VPSUBSW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_sub(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_sub(*src); } Self::from(out) } avx2 { Self( avx2::_mm256_subs_epi16 (self.0, other.0)) } }
    }
} // Static shift
impl ShlAssign<u64> for I16x16 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I16x16\n # {\n if amount >= 16 {\n     I16x16::ZERO\n } else {\n     I16x16::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n         self.as_array()[8] << amount,\n         self.as_array()[9] << amount,\n         self.as_array()[10] << amount,\n         self.as_array()[11] << amount,\n         self.as_array()[12] << amount,\n         self.as_array()[13] << amount,\n         self.as_array()[14] << amount,\n         self.as_array()[15] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sll_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sll_epi16)\n\n\n * `VPSLLW ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> I16x16 {
        select_impl_block! { scalar { if amount >= 16 { I16x16::ZERO } else { I16x16::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_sll_epi16 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<I16x16> for I16x16 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: I16x16) {
        *self = (*self) << amount;
    }
}
impl Shl<I16x16> for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I16x16  ,\n # )  -> I16x16\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..16).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n I16x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: I16x16) -> I16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x << amm } else { 0 }; } I16x16::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x << amm } else { 0 }; } I16x16::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for I16x16 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I16x16\n # {\n if amount >= 16 {\n     let mut out = self.as_array();\n     for x in out.iter_mut() {\n         *x = if *x < 0 { -1 } else { 0 };\n     }\n     I16x16::from(out)\n } else {\n     I16x16::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n         self.as_array()[8] >> amount,\n         self.as_array()[9] >> amount,\n         self.as_array()[10] >> amount,\n         self.as_array()[11] >> amount,\n         self.as_array()[12] >> amount,\n         self.as_array()[13] >> amount,\n         self.as_array()[14] >> amount,\n         self.as_array()[15] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sra_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sra_epi16)\n\n\n * `VPSRAW ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> I16x16 {
        select_impl_block! { scalar { if amount >= 16 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I16x16::from(out) } else { I16x16::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_sra_epi16 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<I16x16> for I16x16 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: I16x16) {
        *self = (*self) >> amount;
    }
}
impl Shr<I16x16> for I16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I16x16  ,\n # )  -> I16x16\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..16).contains(&amm) {\n         *x >> amm\n     } else if *x < 0 {\n         -1\n     }  else {\n         0\n     };\n }\n I16x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: I16x16) -> I16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I16x16::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I16x16::from(out) } }
    }
}
impl SimdBase for I16x16 {
    type Scalar = i16;
    type Array = [i16; 16];
    type Signed = I16x16;
    type Unsigned = U16x16;
    const LANES: usize = 16;
    const ZERO: Self = Self::from_array([0; 16]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_testz_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_testz_si256)\n\n\n * `VPTEST ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm256_testz_si256 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i16  ,\n # )  -> I16x16\n # {\n let mut out = [0; 16];\n out[0] = scalar;\n I16x16::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: i16) -> I16x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; out[0] = scalar; I16x16::from(out) } avx2 { Self( avx2::_mm256_set_epi16 ( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, scalar as i16, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i16\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extract_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extract_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> i16 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm256_extract_epi16 ::<I>(self.0) as i16 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i16  ,\n # )  -> I16x16\n # {\n I16x16::from([scalar; 16])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set1_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: i16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([scalar; 16]) } avx2 { Self( avx2::_mm256_set1_epi16 (scalar as i16)) } }
    }
    type BroadcastLoInput = I16x8;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x8  ,\n # )  -> I16x16\n # {\n I16x16::from([vector.as_array()[0]; 16])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_broadcastw_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_broadcastw_epi16)\n\n\n * `VPBROADCASTW ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: I16x8) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([vector.as_array()[0]; 16]) } avx2 { Self( avx2::_mm256_broadcastw_epi16 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     if self.as_array()[0] == other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  -1  } else { 0 },\n     if self.as_array()[8] == other.as_array()[8] {  -1  } else { 0 },\n     if self.as_array()[9] == other.as_array()[9] {  -1  } else { 0 },\n     if self.as_array()[10] == other.as_array()[10] {  -1  } else { 0 },\n     if self.as_array()[11] == other.as_array()[11] {  -1  } else { 0 },\n     if self.as_array()[12] == other.as_array()[12] {  -1  } else { 0 },\n     if self.as_array()[13] == other.as_array()[13] {  -1  } else { 0 },\n     if self.as_array()[14] == other.as_array()[14] {  -1  } else { 0 },\n     if self.as_array()[15] == other.as_array()[15] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpeq_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpeq_epi16)\n\n\n * `VPCMPEQW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ if self.as_array()[0] == other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] == other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] == other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] == other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] == other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] == other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] == other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] == other.as_array()[7] { -1 } else { 0 }, if self.as_array()[8] == other.as_array()[8] { -1 } else { 0 }, if self.as_array()[9] == other.as_array()[9] { -1 } else { 0 }, if self.as_array()[10] == other.as_array()[10] { -1 } else { 0 }, if self.as_array()[11] == other.as_array()[11] { -1 } else { 0 }, if self.as_array()[12] == other.as_array()[12] { -1 } else { 0 }, if self.as_array()[13] == other.as_array()[13] { -1 } else { 0 }, if self.as_array()[14] == other.as_array()[14] { -1 } else { 0 }, if self.as_array()[15] == other.as_array()[15] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpeq_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n     self.as_array()[8] & (!other.as_array()[8]),\n     self.as_array()[9] & (!other.as_array()[9]),\n     self.as_array()[10] & (!other.as_array()[10]),\n     self.as_array()[11] & (!other.as_array()[11]),\n     self.as_array()[12] & (!other.as_array()[12]),\n     self.as_array()[13] & (!other.as_array()[13]),\n     self.as_array()[14] & (!other.as_array()[14]),\n     self.as_array()[15] & (!other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_andnot_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_andnot_si256)\n\n\n * `VPANDN ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), self.as_array()[8] & (!other.as_array()[8]), self.as_array()[9] & (!other.as_array()[9]), self.as_array()[10] & (!other.as_array()[10]), self.as_array()[11] & (!other.as_array()[11]), self.as_array()[12] & (!other.as_array()[12]), self.as_array()[13] & (!other.as_array()[13]), self.as_array()[14] & (!other.as_array()[14]), self.as_array()[15] & (!other.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_andnot_si256 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     if self.as_array()[0] > other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  -1  } else { 0 },\n     if self.as_array()[8] > other.as_array()[8] {  -1  } else { 0 },\n     if self.as_array()[9] > other.as_array()[9] {  -1  } else { 0 },\n     if self.as_array()[10] > other.as_array()[10] {  -1  } else { 0 },\n     if self.as_array()[11] > other.as_array()[11] {  -1  } else { 0 },\n     if self.as_array()[12] > other.as_array()[12] {  -1  } else { 0 },\n     if self.as_array()[13] > other.as_array()[13] {  -1  } else { 0 },\n     if self.as_array()[14] > other.as_array()[14] {  -1  } else { 0 },\n     if self.as_array()[15] > other.as_array()[15] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpgt_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpgt_epi16)\n\n\n * `VPCMPGTW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_gt(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ if self.as_array()[0] > other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] > other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] > other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] > other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] > other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] > other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] > other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] > other.as_array()[7] { -1 } else { 0 }, if self.as_array()[8] > other.as_array()[8] { -1 } else { 0 }, if self.as_array()[9] > other.as_array()[9] { -1 } else { 0 }, if self.as_array()[10] > other.as_array()[10] { -1 } else { 0 }, if self.as_array()[11] > other.as_array()[11] { -1 } else { 0 }, if self.as_array()[12] > other.as_array()[12] { -1 } else { 0 }, if self.as_array()[13] > other.as_array()[13] { -1 } else { 0 }, if self.as_array()[14] > other.as_array()[14] { -1 } else { 0 }, if self.as_array()[15] > other.as_array()[15] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpgt_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x16\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n I16x16::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_slli_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_slli_epi16)\n\n\n * `VPSLLW ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> I16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I16x16::from(out) } avx2 { const USELESS_ARRAY: [u8; 16] = [0; 16]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_slli_epi16 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x16\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n I16x16::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srai_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srai_epi16)\n\n\n * `VPSRAW ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> I16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I16x16::from(out) } avx2 { const USELESS_ARRAY: [u8; 16] = [0; 16]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_srai_epi16 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n     // Lane# 1\n     self.as_array()[8],\n     other.as_array()[8],\n     self.as_array()[9],\n     other.as_array()[9],\n     self.as_array()[10],\n     other.as_array()[10],\n     self.as_array()[11],\n     other.as_array()[11],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpacklo_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpacklo_epi16)\n\n\n * `VPUNPCKLWD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], // Lane# 1
        self.as_array()[8], other.as_array()[8], self.as_array()[9], other.as_array()[9], self.as_array()[10], other.as_array()[10], self.as_array()[11], other.as_array()[11], ]) } avx2 { Self( avx2::_mm256_unpacklo_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     // Lane# 0\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n     // Lane# 1\n     self.as_array()[12],\n     other.as_array()[12],\n     self.as_array()[13],\n     other.as_array()[13],\n     self.as_array()[14],\n     other.as_array()[14],\n     self.as_array()[15],\n     other.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpackhi_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpackhi_epi16)\n\n\n * `VPUNPCKHWD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ // Lane# 0
        self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], // Lane# 1
        self.as_array()[12], other.as_array()[12], self.as_array()[13], other.as_array()[13], self.as_array()[14], other.as_array()[14], self.as_array()[15], other.as_array()[15], ]) } avx2 { Self( avx2::_mm256_unpackhi_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n     self.as_array()[8].max(other.as_array()[8]),\n     self.as_array()[9].max(other.as_array()[9]),\n     self.as_array()[10].max(other.as_array()[10]),\n     self.as_array()[11].max(other.as_array()[11]),\n     self.as_array()[12].max(other.as_array()[12]),\n     self.as_array()[13].max(other.as_array()[13]),\n     self.as_array()[14].max(other.as_array()[14]),\n     self.as_array()[15].max(other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_max_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_max_epi16)\n\n\n * `VPMAXSW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), self.as_array()[8].max(other.as_array()[8]), self.as_array()[9].max(other.as_array()[9]), self.as_array()[10].max(other.as_array()[10]), self.as_array()[11].max(other.as_array()[11]), self.as_array()[12].max(other.as_array()[12]), self.as_array()[13].max(other.as_array()[13]), self.as_array()[14].max(other.as_array()[14]), self.as_array()[15].max(other.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_max_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I16x16  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n     self.as_array()[8].min(other.as_array()[8]),\n     self.as_array()[9].min(other.as_array()[9]),\n     self.as_array()[10].min(other.as_array()[10]),\n     self.as_array()[11].min(other.as_array()[11]),\n     self.as_array()[12].min(other.as_array()[12]),\n     self.as_array()[13].min(other.as_array()[13]),\n     self.as_array()[14].min(other.as_array()[14]),\n     self.as_array()[15].min(other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_min_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_min_epi16)\n\n\n * `VPMINSW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: I16x16) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), self.as_array()[8].min(other.as_array()[8]), self.as_array()[9].min(other.as_array()[9]), self.as_array()[10].min(other.as_array()[10]), self.as_array()[11].min(other.as_array()[11]), self.as_array()[12].min(other.as_array()[12]), self.as_array()[13].min(other.as_array()[13]), self.as_array()[14].min(other.as_array()[14]), self.as_array()[15].min(other.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_min_epi16 (self.0, other.0)) } }
    }
}
impl crate::SimdBase16 for I16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     // 128-bit Lane #0\n     self.as_array()[I0 + 0 * 8],\n     self.as_array()[I1 + 0 * 8],\n     self.as_array()[I2 + 0 * 8],\n     self.as_array()[I3 + 0 * 8],\n     self.as_array()[4 + 0 * 8],\n     self.as_array()[5 + 0 * 8],\n     self.as_array()[6 + 0 * 8],\n     self.as_array()[7 + 0 * 8],\n     // 128-bit Lane #1\n     self.as_array()[I0 + 1 * 8],\n     self.as_array()[I1 + 1 * 8],\n     self.as_array()[I2 + 1 * 8],\n     self.as_array()[I3 + 1 * 8],\n     self.as_array()[4 + 1 * 8],\n     self.as_array()[5 + 1 * 8],\n     self.as_array()[6 + 1 * 8],\n     self.as_array()[7 + 1 * 8],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_shufflelo_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shufflelo_epi16)\n\n\n * `VPSHUFLW ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle_lo<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(
        &self,
    ) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ // 128-bit Lane #0
        self.as_array()[I0 + 0 * 8], self.as_array()[I1 + 0 * 8], self.as_array()[I2 + 0 * 8], self.as_array()[I3 + 0 * 8], self.as_array()[4 + 0 * 8], self.as_array()[5 + 0 * 8], self.as_array()[6 + 0 * 8], self.as_array()[7 + 0 * 8], // 128-bit Lane #1
        self.as_array()[I0 + 1 * 8], self.as_array()[I1 + 1 * 8], self.as_array()[I2 + 1 * 8], self.as_array()[I3 + 1 * 8], self.as_array()[4 + 1 * 8], self.as_array()[5 + 1 * 8], self.as_array()[6 + 1 * 8], self.as_array()[7 + 1 * 8], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm256_shufflelo_epi16 ::<I3, I2, I1, I0>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x16\n # ;}\n # impl SomeTraitForDoc for I16x16 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I16x16\n # {\n I16x16::from([\n     // 128-bit Lane #0\n     self.as_array()[0 + 0 * 8],\n     self.as_array()[1 + 0 * 8],\n     self.as_array()[2 + 0 * 8],\n     self.as_array()[3 + 0 * 8],\n     self.as_array()[I0 + 4 + 0 * 8],\n     self.as_array()[I1 + 4 + 0 * 8],\n     self.as_array()[I2 + 4 + 0 * 8],\n     self.as_array()[I3 + 4 + 0 * 8],\n     // 128-bit Lane #1\n     self.as_array()[0 + 1 * 8],\n     self.as_array()[1 + 1 * 8],\n     self.as_array()[2 + 1 * 8],\n     self.as_array()[3 + 1 * 8],\n     self.as_array()[I0 + 4 + 1 * 8],\n     self.as_array()[I1 + 4 + 1 * 8],\n     self.as_array()[I2 + 4 + 1 * 8],\n     self.as_array()[I3 + 4 + 1 * 8],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_shufflehi_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shufflehi_epi16)\n\n\n * `VPSHUFHW ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle_hi<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(
        &self,
    ) -> I16x16 {
        select_impl_block! { scalar { I16x16::from([ // 128-bit Lane #0
        self.as_array()[0 + 0 * 8], self.as_array()[1 + 0 * 8], self.as_array()[2 + 0 * 8], self.as_array()[3 + 0 * 8], self.as_array()[I0 + 4 + 0 * 8], self.as_array()[I1 + 4 + 0 * 8], self.as_array()[I2 + 4 + 0 * 8], self.as_array()[I3 + 4 + 0 * 8], // 128-bit Lane #1
        self.as_array()[0 + 1 * 8], self.as_array()[1 + 1 * 8], self.as_array()[2 + 1 * 8], self.as_array()[3 + 1 * 8], self.as_array()[I0 + 4 + 1 * 8], self.as_array()[I1 + 4 + 1 * 8], self.as_array()[I2 + 4 + 1 * 8], self.as_array()[I3 + 4 + 1 * 8], ]) } avx2 { if I0 > 4 { panic!("I0 ({}) > 4", I0); } if I1 > 4 { panic!("I1 ({}) > 4", I1); } if I2 > 4 { panic!("I2 ({}) > 4", I2); } if I3 > 4 { panic!("I3 ({}) > 4", I3); } Self( avx2::_mm256_shufflehi_epi16 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
select_impl! { scalar { type I32x4Internal = [i32 ; 4]; } avx2 { type I32x4Internal = ::std::arch::x86_64::__m128i; } }
#[doc = "`[i32; 4]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct I32x4(I32x4Internal);
unsafe impl bytemuck::Pod for I32x4 {}
unsafe impl bytemuck::Zeroable for I32x4 {}
impl PartialEq for I32x4 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for I32x4 {}
impl Default for I32x4 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for I32x4 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for I32x4 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "I32x4({:?})", <[i32; 4]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for I32x4 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for I32x4 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 4];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <i32 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<I32x4> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> I32x4 {
        let mut out = I32x4::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for I32x4 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[i32; 4]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for I32x4 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[i32; 4]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for I32x4 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_xor_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_si128)\n\n\n * `PXOR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm_xor_si128 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for I32x4 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_or_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_or_si128)\n\n\n * `POR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm_or_si128 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for I32x4 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_and_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_si128)\n\n\n * `PAND xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm_and_si128 (self.0, rhs.0)) } }
    }
}
impl AddAssign for I32x4 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_add_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_epi32)\n\n\n * `PADDD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), ]) } avx2 { Self( avx2::_mm_add_epi32 (self.0, rhs.0)) } }
    }
}
impl SubAssign for I32x4 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sub_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_epi32)\n\n\n * `PSUBD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), ]) } avx2 { Self( avx2::_mm_sub_epi32 (self.0, rhs.0)) } }
    }
}
impl I32x4 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: I32x4 =\n     I32x4::from_array([0, 1, 2, 3]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as i32, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [i32; 4]) -> I32x4 {
        select_impl_block! { scalar { I32x4(array) } avx2 { I32x4(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[i32; 4]> for I32x4 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_loadu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si128)\n\n\n * `MOVDQU xmm, m128`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [i32; 4]) -> I32x4 {
        select_impl_block! { scalar { I32x4(array) } avx2 { I32x4(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_loadu_si128 (array.as_ptr() as *const ::std::arch::x86_64::__m128i) }) } }
    }
}
impl From<I32x4> for [i32; 4] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_storeu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_si128)\n\n\n * `MOVDQU m128, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I32x4) -> [i32; 4] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [i32; 4] = [0; 4]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_storeu_si128 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m128i, vector.0) } out } }
    }
}
impl From<I8x16> for I32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x16\nas little endian bits of I32x4."]
    #[inline(always)]
    fn from(x: I8x16) -> I32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x8> for I32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x8\nas little endian bits of I32x4."]
    #[inline(always)]
    fn from(x: I16x8) -> I32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x2> for I32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x2\nas little endian bits of I32x4."]
    #[inline(always)]
    fn from(x: I64x2) -> I32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for I32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x16\nas little endian bits of I32x4."]
    #[inline(always)]
    fn from(x: U8x16) -> I32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x8> for I32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x8\nas little endian bits of I32x4."]
    #[inline(always)]
    fn from(x: U16x8) -> I32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x4> for I32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x4\nas little endian bits of I32x4."]
    #[inline(always)]
    fn from(x: U32x4) -> I32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x2> for I32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x2\nas little endian bits of I32x4."]
    #[inline(always)]
    fn from(x: U64x2) -> I32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl crate::ExtendingCast<I8x16> for I32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I32x4\n # {\n I32x4::from([\n         i32::from(vector.as_array()[0]),\n         i32::from(vector.as_array()[1]),\n         i32::from(vector.as_array()[2]),\n         i32::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepi8_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepi8_epi32)\n\n\n * `PMOVSXBD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I8x16) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ i32::from(vector.as_array()[0]), i32::from(vector.as_array()[1]), i32::from(vector.as_array()[2]), i32::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm_cvtepi8_epi32 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I16x8> for I32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x8  ,\n # )  -> I32x4\n # {\n I32x4::from([\n         i32::from(vector.as_array()[0]),\n         i32::from(vector.as_array()[1]),\n         i32::from(vector.as_array()[2]),\n         i32::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepi16_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepi16_epi32)\n\n\n * `PMOVSXWD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I16x8) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ i32::from(vector.as_array()[0]), i32::from(vector.as_array()[1]), i32::from(vector.as_array()[2]), i32::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm_cvtepi16_epi32 (vector.0)) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I32x4> for I32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i32  ,\n #         indices  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_i32gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_i32gather_epi32)\n\n\n * `VPGATHERDD xmm, vm32x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i32, indices: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm_i32gather_epi32 ::<4>(base as *const i32, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i32  ,\n #         indices  : I32x4  ,\n #         mask  : I32x4  ,\n #         src  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     if ((mask.as_array()[0] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if ((mask.as_array()[2] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if ((mask.as_array()[3] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_mask_i32gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mask_i32gather_epi32)\n\n\n * `VPGATHERDD xmm, vm32x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i32, indices: I32x4, mask: I32x4, src: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ if ((mask.as_array()[0] as u32) >> 31) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u32) >> 31) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if ((mask.as_array()[2] as u32) >> 31) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if ((mask.as_array()[3] as u32) >> 31) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm_mask_i32gather_epi32 ::<4>( src.0, base as *const i32, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<U64x4> for I32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i32  ,\n #         indices  : U64x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i64gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i64gather_epi32)\n\n\n * `VPGATHERQD xmm, vm64y, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i32, indices: U64x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i64gather_epi32 ::<4>(base as *const i32, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i32  ,\n #         indices  : U64x4  ,\n #         mask  : I32x4  ,\n #         src  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     if ((mask.as_array()[0] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if ((mask.as_array()[2] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if ((mask.as_array()[3] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i64gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i64gather_epi32)\n\n\n * `VPGATHERQD xmm, vm64y, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i32, indices: U64x4, mask: I32x4, src: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ if ((mask.as_array()[0] as u32) >> 31) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u32) >> 31) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if ((mask.as_array()[2] as u32) >> 31) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if ((mask.as_array()[3] as u32) >> 31) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i64gather_epi32 ::<4>( src.0, base as *const i32, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I64x4> for I32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i32  ,\n #         indices  : I64x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i64gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i64gather_epi32)\n\n\n * `VPGATHERQD xmm, vm64y, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i32, indices: I64x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i64gather_epi32 ::<4>(base as *const i32, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i32  ,\n #         indices  : I64x4  ,\n #         mask  : I32x4  ,\n #         src  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     if ((mask.as_array()[0] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if ((mask.as_array()[2] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if ((mask.as_array()[3] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i64gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i64gather_epi32)\n\n\n * `VPGATHERQD xmm, vm64y, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i32, indices: I64x4, mask: I32x4, src: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ if ((mask.as_array()[0] as u32) >> 31) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u32) >> 31) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if ((mask.as_array()[2] as u32) >> 31) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if ((mask.as_array()[3] as u32) >> 31) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i64gather_epi32 ::<4>( src.0, base as *const i32, indices.0, mask.0, )) } }
    }
} // Static shift
impl ShlAssign<u64> for I32x4 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I32x4\n # {\n if amount >= 32 {\n     I32x4::ZERO\n } else {\n     I32x4::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_sll_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sll_epi32)\n\n\n * `PSLLD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> I32x4 {
        select_impl_block! { scalar { if amount >= 32 { I32x4::ZERO } else { I32x4::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_sll_epi32 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<I32x4> for I32x4 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: I32x4) {
        *self = (*self) << amount;
    }
}
impl Shl<I32x4> for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I32x4  ,\n # )  -> I32x4\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..32).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n I32x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sllv_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sllv_epi32)\n\n\n * `VPSLLVD xmm, xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: I32x4) -> I32x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..32).contains(&amm) { *x << amm } else { 0 }; } I32x4::from(out) } avx2 { Self( avx2::_mm_sllv_epi32 (self.0, amount.0)) } }
    }
} // Static shift
impl ShrAssign<u64> for I32x4 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I32x4\n # {\n if amount >= 32 {\n     let mut out = self.as_array();\n     for x in out.iter_mut() {\n         *x = if *x < 0 { -1 } else { 0 };\n     }\n     I32x4::from(out)\n } else {\n     I32x4::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_sra_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sra_epi32)\n\n\n * `PSRAD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> I32x4 {
        select_impl_block! { scalar { if amount >= 32 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I32x4::from(out) } else { I32x4::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_sra_epi32 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<I32x4> for I32x4 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: I32x4) {
        *self = (*self) >> amount;
    }
}
impl Shr<I32x4> for I32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I32x4  ,\n # )  -> I32x4\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..32).contains(&amm) {\n         *x >> amm\n     } else if *x < 0 {\n         -1\n     }  else {\n         0\n     };\n }\n I32x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srav_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srav_epi32)\n\n\n * `VPSRAVD xmm, xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: I32x4) -> I32x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..32).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I32x4::from(out) } avx2 { Self( avx2::_mm_srav_epi32 (self.0, amount.0)) } }
    }
}
impl SimdBase for I32x4 {
    type Scalar = i32;
    type Array = [i32; 4];
    type Signed = I32x4;
    type Unsigned = U32x4;
    const LANES: usize = 4;
    const ZERO: Self = Self::from_array([0; 4]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_testz_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128)\n\n\n * `PTEST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm_testz_si128 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i32  ,\n # )  -> I32x4\n # {\n let mut out = [0; 4];\n out[0] = scalar;\n I32x4::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: i32) -> I32x4 {
        select_impl_block! { scalar { let mut out = [0; 4]; out[0] = scalar; I32x4::from(out) } avx2 { Self( avx2::_mm_set_epi32 ( 0, 0, 0, scalar as i32, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i32\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i32\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_extract_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_extract_epi32)\n\n\n * `PEXTRD r32, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> i32 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm_extract_epi32 ::<I>(self.0) as i32 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i32  ,\n # )  -> I32x4\n # {\n I32x4::from([scalar; 4])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set1_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: i32) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([scalar; 4]) } avx2 { Self( avx2::_mm_set1_epi32 (scalar as i32)) } }
    }
    type BroadcastLoInput = I32x4;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([vector.as_array()[0]; 4])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_broadcastd_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_broadcastd_epi32)\n\n\n * `VPBROADCASTD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([vector.as_array()[0]; 4]) } avx2 { Self( avx2::_mm_broadcastd_epi32 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     if self.as_array()[0] == other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpeq_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpeq_epi32)\n\n\n * `PCMPEQD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ if self.as_array()[0] == other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] == other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] == other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] == other.as_array()[3] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpeq_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_andnot_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_si128)\n\n\n * `PANDN xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), ]) } avx2 { Self( avx2::_mm_andnot_si128 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     if self.as_array()[0] > other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpgt_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpgt_epi32)\n\n\n * `PCMPGTD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_gt(&self, other: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ if self.as_array()[0] > other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] > other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] > other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] > other.as_array()[3] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpgt_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x4\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n I32x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_slli_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_slli_epi32)\n\n\n * `PSLLD xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> I32x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I32x4::from(out) } avx2 { const USELESS_ARRAY: [u8; 32] = [0; 32]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_slli_epi32 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x4\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n I32x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srai_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srai_epi32)\n\n\n * `PSRAD xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> I32x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I32x4::from(out) } avx2 { const USELESS_ARRAY: [u8; 32] = [0; 32]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_srai_epi32 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpacklo_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpacklo_epi32)\n\n\n * `PUNPCKLDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], ]) } avx2 { Self( avx2::_mm_unpacklo_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     // Lane# 0\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpackhi_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpackhi_epi32)\n\n\n * `PUNPCKHDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ // Lane# 0
        self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], ]) } avx2 { Self( avx2::_mm_unpackhi_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_max_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epi32)\n\n\n * `PMAXSD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), ]) } avx2 { Self( avx2::_mm_max_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_min_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_epi32)\n\n\n * `PMINSD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: I32x4) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), ]) } avx2 { Self( avx2::_mm_min_epi32 (self.0, other.0)) } }
    }
}
impl crate::SimdBase32 for I32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x4\n # {\n I32x4::from([\n     // 128-bit Lane #0\n     self.as_array()[I0 + 0 * 4],\n     self.as_array()[I1 + 0 * 4],\n     self.as_array()[I2 + 0 * 4],\n     self.as_array()[I3 + 0 * 4],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_shuffle_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shuffle_epi32)\n\n\n * `PSHUFD xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(&self) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ // 128-bit Lane #0
        self.as_array()[I0 + 0 * 4], self.as_array()[I1 + 0 * 4], self.as_array()[I2 + 0 * 4], self.as_array()[I3 + 0 * 4], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm_shuffle_epi32 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
impl crate::SimdBase4x for I32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : I32x4  ,\n # )  -> I32x4\n # ;}\n # impl SomeTraitForDoc for I32x4 {\n # fn the_doc_function\n #     <\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : I32x4  ,\n # )  -> I32x4\n # {\n I32x4::from([\n         (if B0 { if_true } else { *self }).as_array()[0],\n         (if B1 { if_true } else { *self }).as_array()[1],\n         (if B2 { if_true } else { *self }).as_array()[2],\n         (if B3 { if_true } else { *self }).as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_blend_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_blend_epi32)\n\n\n * `VPBLENDD xmm, xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn blend<const B3: bool, const B2: bool, const B1: bool, const B0: bool>(
        &self,
        if_true: I32x4,
    ) -> I32x4 {
        select_impl_block! { scalar { I32x4::from([ (if B0 { if_true } else { *self }).as_array()[0], (if B1 { if_true } else { *self }).as_array()[1], (if B2 { if_true } else { *self }).as_array()[2], (if B3 { if_true } else { *self }).as_array()[3], ]) } avx2 { Self( avx2::_mm_blend_epi32 ::<B3, B2, B1, B0>(self.0, if_true.0)) } }
    }
}
select_impl! { scalar { type I32x8Internal = [i32 ; 8]; } avx2 { type I32x8Internal = ::std::arch::x86_64::__m256i; } }
#[doc = "`[i32; 8]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct I32x8(I32x8Internal);
unsafe impl bytemuck::Pod for I32x8 {}
unsafe impl bytemuck::Zeroable for I32x8 {}
impl PartialEq for I32x8 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for I32x8 {}
impl Default for I32x8 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for I32x8 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for I32x8 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "I32x8({:?})", <[i32; 8]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for I32x8 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for I32x8 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 8];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <i32 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<I32x8> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> I32x8 {
        let mut out = I32x8::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for I32x8 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[i32; 8]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for I32x8 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[i32; 8]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for I32x8 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_xor_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_xor_si256)\n\n\n * `VPXOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm256_xor_si256 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for I32x8 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_or_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_or_si256)\n\n\n * `VPOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm256_or_si256 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for I32x8 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_and_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_and_si256)\n\n\n * `VPAND ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm256_and_si256 (self.0, rhs.0)) } }
    }
}
impl AddAssign for I32x8 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_add_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_add_epi32)\n\n\n * `VPADDD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_add_epi32 (self.0, rhs.0)) } }
    }
}
impl SubAssign for I32x8 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sub_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sub_epi32)\n\n\n * `VPSUBD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_sub_epi32 (self.0, rhs.0)) } }
    }
}
impl I32x8 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: I32x8 =\n     I32x8::from_array([0, 1, 2, 3, 4, 5, 6, 7]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as i32, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [i32; 8]) -> I32x8 {
        select_impl_block! { scalar { I32x8(array) } avx2 { I32x8(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[i32; 8]> for I32x8 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_loadu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_loadu_si256)\n\n\n * `VMOVDQU ymm, m256`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [i32; 8]) -> I32x8 {
        select_impl_block! { scalar { I32x8(array) } avx2 { I32x8(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_loadu_si256 (array.as_ptr() as *const ::std::arch::x86_64::__m256i) }) } }
    }
}
impl From<I32x8> for [i32; 8] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_storeu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_storeu_si256)\n\n\n * `VMOVDQU m256, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I32x8) -> [i32; 8] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [i32; 8] = [0; 8]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_storeu_si256 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m256i, vector.0) } out } }
    }
}
impl From<I8x32> for I32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x32\nas little endian bits of I32x8."]
    #[inline(always)]
    fn from(x: I8x32) -> I32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x16> for I32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x16\nas little endian bits of I32x8."]
    #[inline(always)]
    fn from(x: I16x16) -> I32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x4> for I32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x4\nas little endian bits of I32x8."]
    #[inline(always)]
    fn from(x: I64x4) -> I32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x32> for I32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x32\nas little endian bits of I32x8."]
    #[inline(always)]
    fn from(x: U8x32) -> I32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x16> for I32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x16\nas little endian bits of I32x8."]
    #[inline(always)]
    fn from(x: U16x16) -> I32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x8> for I32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x8\nas little endian bits of I32x8."]
    #[inline(always)]
    fn from(x: U32x8) -> I32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x4> for I32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x4\nas little endian bits of I32x8."]
    #[inline(always)]
    fn from(x: U64x4) -> I32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x8> for I32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n         i32::from(vector.as_array()[0]),\n         i32::from(vector.as_array()[1]),\n         i32::from(vector.as_array()[2]),\n         i32::from(vector.as_array()[3]),\n         i32::from(vector.as_array()[4]),\n         i32::from(vector.as_array()[5]),\n         i32::from(vector.as_array()[6]),\n         i32::from(vector.as_array()[7]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi16_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi16_epi32)\n\n\n * `VPMOVSXWD ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I16x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ i32::from(vector.as_array()[0]), i32::from(vector.as_array()[1]), i32::from(vector.as_array()[2]), i32::from(vector.as_array()[3]), i32::from(vector.as_array()[4]), i32::from(vector.as_array()[5]), i32::from(vector.as_array()[6]), i32::from(vector.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_cvtepi16_epi32 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I8x16> for I32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I32x8\n # {\n I32x8::from([\n         i32::from(vector.as_array()[0]),\n         i32::from(vector.as_array()[1]),\n         i32::from(vector.as_array()[2]),\n         i32::from(vector.as_array()[3]),\n         i32::from(vector.as_array()[4]),\n         i32::from(vector.as_array()[5]),\n         i32::from(vector.as_array()[6]),\n         i32::from(vector.as_array()[7]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi8_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi8_epi32)\n\n\n * `VPMOVSXBD ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I8x16) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ i32::from(vector.as_array()[0]), i32::from(vector.as_array()[1]), i32::from(vector.as_array()[2]), i32::from(vector.as_array()[3]), i32::from(vector.as_array()[4]), i32::from(vector.as_array()[5]), i32::from(vector.as_array()[6]), i32::from(vector.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_cvtepi8_epi32 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I16x8> for I32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n         i32::from(vector.as_array()[0]),\n         i32::from(vector.as_array()[1]),\n         i32::from(vector.as_array()[2]),\n         i32::from(vector.as_array()[3]),\n         i32::from(vector.as_array()[4]),\n         i32::from(vector.as_array()[5]),\n         i32::from(vector.as_array()[6]),\n         i32::from(vector.as_array()[7]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi16_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi16_epi32)\n\n\n * `VPMOVSXWD ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I16x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ i32::from(vector.as_array()[0]), i32::from(vector.as_array()[1]), i32::from(vector.as_array()[2]), i32::from(vector.as_array()[3]), i32::from(vector.as_array()[4]), i32::from(vector.as_array()[5]), i32::from(vector.as_array()[6]), i32::from(vector.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_cvtepi16_epi32 (vector.0)) } }
    }
}
impl From<I32x4> for I32x8 {
    #[doc = " NOTE: this will _zero_ the upper bits of the destination. Other\n intrinsics are more effcient, but leave the upper bits undefined.\n At present, these more effcient intrinsics are not exposed.\n\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I32x4  ,\n # )  -> I32x8\n # {\n let mut out = [0; 8];\n out[0..4].copy_from_slice(&vector.as_array());\n I32x8::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_zextsi128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_zextsi128_si256)\n\n\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I32x4) -> I32x8 {
        select_impl_block! { scalar { let mut out = [0; 8]; out[0..4].copy_from_slice(&vector.as_array()); I32x8::from(out) } avx2 { Self( avx2::_mm256_zextsi128_si256 (vector.0)) } }
    }
}
impl From<[I32x4; 2]> for I32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vectors  : [I32x4; 2]  ,\n # )  -> I32x8\n # {\n let mut out = [0; 8];\n out[0..4].copy_from_slice(&vectors[0].as_array());\n out[4..].copy_from_slice(&vectors[1].as_array());\n I32x8::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_m128i`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_m128i)\n\n\n * `VINSERTF128 ymm, ymm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vectors: [I32x4; 2]) -> I32x8 {
        select_impl_block! { scalar { let mut out = [0; 8]; out[0..4].copy_from_slice(&vectors[0].as_array()); out[4..].copy_from_slice(&vectors[1].as_array()); I32x8::from(out) } avx2 { Self( avx2::_mm256_set_m128i (vectors[1].0, vectors[0].0)) } }
    }
} // TODO: this doesn't show up in the docs.
impl From<I32x8> for [I32x4; 2] {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I32x8  ,\n # )  -> [I32x4; 2]\n # {\n let mut lo = [0; 4];\n let mut hi = [0; 4];\n lo.copy_from_slice(&vector.as_array()[0..4]);\n hi.copy_from_slice(&vector.as_array()[4..]);\n [I32x4::from(lo), I32x4::from(hi)]\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extracti128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extracti128_si256)\n\n\n * `VEXTRACTI128 xmm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I32x8) -> [I32x4; 2] {
        select_impl_block! { scalar { let mut lo = [0; 4]; let mut hi = [0; 4]; lo.copy_from_slice(&vector.as_array()[0..4]); hi.copy_from_slice(&vector.as_array()[4..]); [I32x4::from(lo), I32x4::from(hi)] } avx2 { [ I32x4( avx2::_mm256_extracti128_si256 ::<0>(vector.0)), I32x4( avx2::_mm256_extracti128_si256 ::<1>(vector.0)), ] } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I32x8> for I32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i32  ,\n #         indices  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n     base.offset(indices.as_array()[4] as isize).read_unaligned(),\n     base.offset(indices.as_array()[5] as isize).read_unaligned(),\n     base.offset(indices.as_array()[6] as isize).read_unaligned(),\n     base.offset(indices.as_array()[7] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i32gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i32gather_epi32)\n\n\n * `VPGATHERDD ymm, vm32x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i32, indices: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), base.offset(indices.as_array()[4] as isize).read_unaligned(), base.offset(indices.as_array()[5] as isize).read_unaligned(), base.offset(indices.as_array()[6] as isize).read_unaligned(), base.offset(indices.as_array()[7] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i32gather_epi32 ::<4>(base as *const i32, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i32  ,\n #         indices  : I32x8  ,\n #         mask  : I32x8  ,\n #         src  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     if ((mask.as_array()[0] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if ((mask.as_array()[2] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if ((mask.as_array()[3] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n     if ((mask.as_array()[4] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[4] as isize).read_unaligned()\n     } else {\n         src.as_array()[4]\n     },\n     if ((mask.as_array()[5] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[5] as isize).read_unaligned()\n     } else {\n         src.as_array()[5]\n     },\n     if ((mask.as_array()[6] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[6] as isize).read_unaligned()\n     } else {\n         src.as_array()[6]\n     },\n     if ((mask.as_array()[7] as u32) >> 31) == 1 {\n         base.offset(indices.as_array()[7] as isize).read_unaligned()\n     } else {\n         src.as_array()[7]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i32gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i32gather_epi32)\n\n\n * `VPGATHERDD ymm, vm32x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i32, indices: I32x8, mask: I32x8, src: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ if ((mask.as_array()[0] as u32) >> 31) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u32) >> 31) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if ((mask.as_array()[2] as u32) >> 31) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if ((mask.as_array()[3] as u32) >> 31) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, if ((mask.as_array()[4] as u32) >> 31) == 1 { base.offset(indices.as_array()[4] as isize).read_unaligned() } else { src.as_array()[4] }, if ((mask.as_array()[5] as u32) >> 31) == 1 { base.offset(indices.as_array()[5] as isize).read_unaligned() } else { src.as_array()[5] }, if ((mask.as_array()[6] as u32) >> 31) == 1 { base.offset(indices.as_array()[6] as isize).read_unaligned() } else { src.as_array()[6] }, if ((mask.as_array()[7] as u32) >> 31) == 1 { base.offset(indices.as_array()[7] as isize).read_unaligned() } else { src.as_array()[7] }, ]) } avx2 { Self( avx2::_mm256_mask_i32gather_epi32 ::<4>( src.0, base as *const i32, indices.0, mask.0, )) } }
    }
} // Static shift
impl ShlAssign<u64> for I32x8 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I32x8\n # {\n if amount >= 32 {\n     I32x8::ZERO\n } else {\n     I32x8::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sll_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sll_epi32)\n\n\n * `VPSLLD ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> I32x8 {
        select_impl_block! { scalar { if amount >= 32 { I32x8::ZERO } else { I32x8::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_sll_epi32 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<I32x8> for I32x8 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: I32x8) {
        *self = (*self) << amount;
    }
}
impl Shl<I32x8> for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I32x8  ,\n # )  -> I32x8\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..32).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n I32x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sllv_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sllv_epi32)\n\n\n * `VPSLLVD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: I32x8) -> I32x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..32).contains(&amm) { *x << amm } else { 0 }; } I32x8::from(out) } avx2 { Self( avx2::_mm256_sllv_epi32 (self.0, amount.0)) } }
    }
} // Static shift
impl ShrAssign<u64> for I32x8 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I32x8\n # {\n if amount >= 32 {\n     let mut out = self.as_array();\n     for x in out.iter_mut() {\n         *x = if *x < 0 { -1 } else { 0 };\n     }\n     I32x8::from(out)\n } else {\n     I32x8::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sra_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sra_epi32)\n\n\n * `VPSRAD ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> I32x8 {
        select_impl_block! { scalar { if amount >= 32 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I32x8::from(out) } else { I32x8::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_sra_epi32 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<I32x8> for I32x8 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: I32x8) {
        *self = (*self) >> amount;
    }
}
impl Shr<I32x8> for I32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I32x8  ,\n # )  -> I32x8\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..32).contains(&amm) {\n         *x >> amm\n     } else if *x < 0 {\n         -1\n     }  else {\n         0\n     };\n }\n I32x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srav_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srav_epi32)\n\n\n * `VPSRAVD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: I32x8) -> I32x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..32).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I32x8::from(out) } avx2 { Self( avx2::_mm256_srav_epi32 (self.0, amount.0)) } }
    }
}
impl SimdBase for I32x8 {
    type Scalar = i32;
    type Array = [i32; 8];
    type Signed = I32x8;
    type Unsigned = U32x8;
    const LANES: usize = 8;
    const ZERO: Self = Self::from_array([0; 8]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_testz_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_testz_si256)\n\n\n * `VPTEST ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm256_testz_si256 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i32  ,\n # )  -> I32x8\n # {\n let mut out = [0; 8];\n out[0] = scalar;\n I32x8::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: i32) -> I32x8 {
        select_impl_block! { scalar { let mut out = [0; 8]; out[0] = scalar; I32x8::from(out) } avx2 { Self( avx2::_mm256_set_epi32 ( 0, 0, 0, 0, 0, 0, 0, scalar as i32, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i32\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i32\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extract_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extract_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> i32 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm256_extract_epi32 ::<I>(self.0) as i32 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i32  ,\n # )  -> I32x8\n # {\n I32x8::from([scalar; 8])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set1_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: i32) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([scalar; 8]) } avx2 { Self( avx2::_mm256_set1_epi32 (scalar as i32)) } }
    }
    type BroadcastLoInput = I32x4;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I32x4  ,\n # )  -> I32x8\n # {\n I32x8::from([vector.as_array()[0]; 8])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_broadcastd_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_broadcastd_epi32)\n\n\n * `VPBROADCASTD ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: I32x4) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([vector.as_array()[0]; 8]) } avx2 { Self( avx2::_mm256_broadcastd_epi32 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     if self.as_array()[0] == other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpeq_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpeq_epi32)\n\n\n * `VPCMPEQD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ if self.as_array()[0] == other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] == other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] == other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] == other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] == other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] == other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] == other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] == other.as_array()[7] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpeq_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_andnot_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_andnot_si256)\n\n\n * `VPANDN ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_andnot_si256 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     if self.as_array()[0] > other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  -1  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  -1  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  -1  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  -1  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpgt_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpgt_epi32)\n\n\n * `VPCMPGTD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_gt(&self, other: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ if self.as_array()[0] > other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] > other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] > other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] > other.as_array()[3] { -1 } else { 0 }, if self.as_array()[4] > other.as_array()[4] { -1 } else { 0 }, if self.as_array()[5] > other.as_array()[5] { -1 } else { 0 }, if self.as_array()[6] > other.as_array()[6] { -1 } else { 0 }, if self.as_array()[7] > other.as_array()[7] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpgt_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x8\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n I32x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_slli_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_slli_epi32)\n\n\n * `VPSLLD ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> I32x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I32x8::from(out) } avx2 { const USELESS_ARRAY: [u8; 32] = [0; 32]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_slli_epi32 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x8\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n I32x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srai_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srai_epi32)\n\n\n * `VPSRAD ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> I32x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I32x8::from(out) } avx2 { const USELESS_ARRAY: [u8; 32] = [0; 32]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_srai_epi32 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     // Lane# 1\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpacklo_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpacklo_epi32)\n\n\n * `VPUNPCKLDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], // Lane# 1
        self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], ]) } avx2 { Self( avx2::_mm256_unpacklo_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     // Lane# 0\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n     // Lane# 1\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpackhi_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpackhi_epi32)\n\n\n * `VPUNPCKHDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ // Lane# 0
        self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], // Lane# 1
        self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], ]) } avx2 { Self( avx2::_mm256_unpackhi_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_max_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_max_epi32)\n\n\n * `VPMAXSD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_max_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_min_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_min_epi32)\n\n\n * `VPMINSD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: I32x8) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_min_epi32 (self.0, other.0)) } }
    }
}
impl crate::SimdBase32 for I32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I32x8\n # {\n I32x8::from([\n     // 128-bit Lane #0\n     self.as_array()[I0 + 0 * 4],\n     self.as_array()[I1 + 0 * 4],\n     self.as_array()[I2 + 0 * 4],\n     self.as_array()[I3 + 0 * 4],\n     // 128-bit Lane #1\n     self.as_array()[I0 + 1 * 4],\n     self.as_array()[I1 + 1 * 4],\n     self.as_array()[I2 + 1 * 4],\n     self.as_array()[I3 + 1 * 4],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_shuffle_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shuffle_epi32)\n\n\n * `VPSHUFD ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(&self) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ // 128-bit Lane #0
        self.as_array()[I0 + 0 * 4], self.as_array()[I1 + 0 * 4], self.as_array()[I2 + 0 * 4], self.as_array()[I3 + 0 * 4], // 128-bit Lane #1
        self.as_array()[I0 + 1 * 4], self.as_array()[I1 + 1 * 4], self.as_array()[I2 + 1 * 4], self.as_array()[I3 + 1 * 4], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm256_shuffle_epi32 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
impl crate::SimdBase8x for I32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const B7: bool,\n #             const B6: bool,\n #             const B5: bool,\n #             const B4: bool,\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : I32x8  ,\n # )  -> I32x8\n # ;}\n # impl SomeTraitForDoc for I32x8 {\n # fn the_doc_function\n #     <\n #             const B7: bool,\n #             const B6: bool,\n #             const B5: bool,\n #             const B4: bool,\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : I32x8  ,\n # )  -> I32x8\n # {\n I32x8::from([\n         (if B0 { if_true } else { *self }).as_array()[0],\n         (if B1 { if_true } else { *self }).as_array()[1],\n         (if B2 { if_true } else { *self }).as_array()[2],\n         (if B3 { if_true } else { *self }).as_array()[3],\n         (if B4 { if_true } else { *self }).as_array()[4],\n         (if B5 { if_true } else { *self }).as_array()[5],\n         (if B6 { if_true } else { *self }).as_array()[6],\n         (if B7 { if_true } else { *self }).as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_blend_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_blend_epi32)\n\n\n * `VPBLENDD ymm, ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn blend<
        const B7: bool,
        const B6: bool,
        const B5: bool,
        const B4: bool,
        const B3: bool,
        const B2: bool,
        const B1: bool,
        const B0: bool,
    >(
        &self,
        if_true: I32x8,
    ) -> I32x8 {
        select_impl_block! { scalar { I32x8::from([ (if B0 { if_true } else { *self }).as_array()[0], (if B1 { if_true } else { *self }).as_array()[1], (if B2 { if_true } else { *self }).as_array()[2], (if B3 { if_true } else { *self }).as_array()[3], (if B4 { if_true } else { *self }).as_array()[4], (if B5 { if_true } else { *self }).as_array()[5], (if B6 { if_true } else { *self }).as_array()[6], (if B7 { if_true } else { *self }).as_array()[7], ]) } avx2 { Self( avx2::_mm256_blend_epi32 ::<B7, B6, B5, B4, B3, B2, B1, B0>(self.0, if_true.0)) } }
    }
}
select_impl! { scalar { type I64x2Internal = [i64 ; 2]; } avx2 { type I64x2Internal = ::std::arch::x86_64::__m128i; } }
#[doc = "`[i64; 2]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct I64x2(I64x2Internal);
unsafe impl bytemuck::Pod for I64x2 {}
unsafe impl bytemuck::Zeroable for I64x2 {}
impl PartialEq for I64x2 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for I64x2 {}
impl Default for I64x2 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for I64x2 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for I64x2 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "I64x2({:?})", <[i64; 2]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for I64x2 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for I64x2 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 2];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <i64 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<I64x2> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> I64x2 {
        let mut out = I64x2::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for I64x2 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[i64; 2]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for I64x2 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[i64; 2]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for I64x2 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_xor_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_si128)\n\n\n * `PXOR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], ]) } avx2 { Self( avx2::_mm_xor_si128 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for I64x2 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_or_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_or_si128)\n\n\n * `POR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], ]) } avx2 { Self( avx2::_mm_or_si128 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for I64x2 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_and_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_si128)\n\n\n * `PAND xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], ]) } avx2 { Self( avx2::_mm_and_si128 (self.0, rhs.0)) } }
    }
}
impl AddAssign for I64x2 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_add_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_epi64)\n\n\n * `PADDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), ]) } avx2 { Self( avx2::_mm_add_epi64 (self.0, rhs.0)) } }
    }
}
impl SubAssign for I64x2 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sub_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_epi64)\n\n\n * `PSUBQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), ]) } avx2 { Self( avx2::_mm_sub_epi64 (self.0, rhs.0)) } }
    }
}
impl I64x2 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: I64x2 =\n     I64x2::from_array([0, 1]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as i64, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [i64; 2]) -> I64x2 {
        select_impl_block! { scalar { I64x2(array) } avx2 { I64x2(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[i64; 2]> for I64x2 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_loadu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si128)\n\n\n * `MOVDQU xmm, m128`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [i64; 2]) -> I64x2 {
        select_impl_block! { scalar { I64x2(array) } avx2 { I64x2(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_loadu_si128 (array.as_ptr() as *const ::std::arch::x86_64::__m128i) }) } }
    }
}
impl From<I64x2> for [i64; 2] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_storeu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_si128)\n\n\n * `MOVDQU m128, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I64x2) -> [i64; 2] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [i64; 2] = [0; 2]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_storeu_si128 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m128i, vector.0) } out } }
    }
}
impl From<I8x16> for I64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x16\nas little endian bits of I64x2."]
    #[inline(always)]
    fn from(x: I8x16) -> I64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x8> for I64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x8\nas little endian bits of I64x2."]
    #[inline(always)]
    fn from(x: I16x8) -> I64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x4> for I64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x4\nas little endian bits of I64x2."]
    #[inline(always)]
    fn from(x: I32x4) -> I64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for I64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x16\nas little endian bits of I64x2."]
    #[inline(always)]
    fn from(x: U8x16) -> I64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x8> for I64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x8\nas little endian bits of I64x2."]
    #[inline(always)]
    fn from(x: U16x8) -> I64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x4> for I64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x4\nas little endian bits of I64x2."]
    #[inline(always)]
    fn from(x: U32x4) -> I64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x2> for I64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x2\nas little endian bits of I64x2."]
    #[inline(always)]
    fn from(x: U64x2) -> I64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl crate::ExtendingCast<I8x16> for I64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I64x2\n # {\n I64x2::from([\n         i64::from(vector.as_array()[0]),\n         i64::from(vector.as_array()[1]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepi8_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepi8_epi64)\n\n\n * `PMOVSXBQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I8x16) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ i64::from(vector.as_array()[0]), i64::from(vector.as_array()[1]), ]) } avx2 { Self( avx2::_mm_cvtepi8_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I16x8> for I64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x8  ,\n # )  -> I64x2\n # {\n I64x2::from([\n         i64::from(vector.as_array()[0]),\n         i64::from(vector.as_array()[1]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepi16_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepi16_epi64)\n\n\n * `PMOVSXWQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I16x8) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ i64::from(vector.as_array()[0]), i64::from(vector.as_array()[1]), ]) } avx2 { Self( avx2::_mm_cvtepi16_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I32x4> for I64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I32x4  ,\n # )  -> I64x2\n # {\n I64x2::from([\n         i64::from(vector.as_array()[0]),\n         i64::from(vector.as_array()[1]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepi32_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepi32_epi64)\n\n\n * `PMOVSXDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I32x4) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ i64::from(vector.as_array()[0]), i64::from(vector.as_array()[1]), ]) } avx2 { Self( avx2::_mm_cvtepi32_epi64 (vector.0)) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<U64x2> for I64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : U64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_i64gather_epi64)\n\n\n * `VPGATHERQQ xmm, vm64x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i64, indices: U64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm_i64gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : U64x2  ,\n #         mask  : I64x2  ,\n #         src  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     if ((mask.as_array()[0] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_mask_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mask_i64gather_epi64)\n\n\n * `VPGATHERQQ xmm, vm64x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i64, indices: U64x2, mask: I64x2, src: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ if ((mask.as_array()[0] as u64) >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u64) >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, ]) } avx2 { Self( avx2::_mm_mask_i64gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I64x2> for I64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_i64gather_epi64)\n\n\n * `VPGATHERQQ xmm, vm64x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i64, indices: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm_i64gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : I64x2  ,\n #         mask  : I64x2  ,\n #         src  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     if ((mask.as_array()[0] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_mask_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mask_i64gather_epi64)\n\n\n * `VPGATHERQQ xmm, vm64x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i64, indices: I64x2, mask: I64x2, src: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ if ((mask.as_array()[0] as u64) >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u64) >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, ]) } avx2 { Self( avx2::_mm_mask_i64gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
} // Static shift
impl ShlAssign<u64> for I64x2 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I64x2\n # {\n if amount >= 64 {\n     I64x2::ZERO\n } else {\n     I64x2::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: u64) -> I64x2 {
        select_impl_block! { scalar { if amount >= 64 { I64x2::ZERO } else { I64x2::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, ]) } } avx2 { if amount >= 64 { I64x2::ZERO } else { I64x2::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, ]) } } }
    }
} // Variable shift
impl ShlAssign<I64x2> for I64x2 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: I64x2) {
        *self = (*self) << amount;
    }
}
impl Shl<I64x2> for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I64x2  ,\n # )  -> I64x2\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..64).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n I64x2::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: I64x2) -> I64x2 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x << amm } else { 0 }; } I64x2::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x << amm } else { 0 }; } I64x2::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for I64x2 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I64x2\n # {\n if amount >= 64 {\n     let mut out = self.as_array();\n     for x in out.iter_mut() {\n         *x = if *x < 0 { -1 } else { 0 };\n     }\n     I64x2::from(out)\n } else {\n     I64x2::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: u64) -> I64x2 {
        select_impl_block! { scalar { if amount >= 64 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I64x2::from(out) } else { I64x2::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, ]) } } avx2 { if amount >= 64 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I64x2::from(out) } else { I64x2::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, ]) } } }
    }
} // Variable shift
impl ShrAssign<I64x2> for I64x2 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: I64x2) {
        *self = (*self) >> amount;
    }
}
impl Shr<I64x2> for I64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I64x2  ,\n # )  -> I64x2\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..64).contains(&amm) {\n         *x >> amm\n     } else if *x < 0 {\n         -1\n     }  else {\n         0\n     };\n }\n I64x2::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: I64x2) -> I64x2 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I64x2::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I64x2::from(out) } }
    }
}
impl SimdBase for I64x2 {
    type Scalar = i64;
    type Array = [i64; 2];
    type Signed = I64x2;
    type Unsigned = U64x2;
    const LANES: usize = 2;
    const ZERO: Self = Self::from_array([0; 2]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_testz_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128)\n\n\n * `PTEST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm_testz_si128 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i64  ,\n # )  -> I64x2\n # {\n let mut out = [0; 2];\n out[0] = scalar;\n I64x2::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: i64) -> I64x2 {
        select_impl_block! { scalar { let mut out = [0; 2]; out[0] = scalar; I64x2::from(out) } avx2 { Self( avx2::_mm_set_epi64x ( 0, scalar as i64, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i64\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i64\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_extract_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_extract_epi64)\n\n\n * `PEXTRQ r64, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> i64 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm_extract_epi64 ::<I>(self.0) as i64 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i64  ,\n # )  -> I64x2\n # {\n I64x2::from([scalar; 2])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set1_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: i64) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([scalar; 2]) } avx2 { Self( avx2::_mm_set1_epi64x (scalar as i64)) } }
    }
    type BroadcastLoInput = I64x2;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([vector.as_array()[0]; 2])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_broadcastq_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_broadcastq_epi64)\n\n\n * `VPBROADCASTQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([vector.as_array()[0]; 2]) } avx2 { Self( avx2::_mm_broadcastq_epi64 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     if self.as_array()[0] == other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpeq_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpeq_epi64)\n\n\n * `PCMPEQQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ if self.as_array()[0] == other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] == other.as_array()[1] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpeq_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_andnot_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_si128)\n\n\n * `PANDN xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), ]) } avx2 { Self( avx2::_mm_andnot_si128 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     if self.as_array()[0] > other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpgt_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpgt_epi64)\n\n\n * `PCMPGTQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_gt(&self, other: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ if self.as_array()[0] > other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] > other.as_array()[1] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpgt_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x2\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n I64x2::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_slli_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_slli_epi64)\n\n\n * `PSLLQ xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> I64x2 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I64x2::from(out) } avx2 { const USELESS_ARRAY: [u8; 64] = [0; 64]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_slli_epi64 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x2\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n I64x2::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> I64x2 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I64x2::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I64x2::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpacklo_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpacklo_epi64)\n\n\n * `PUNPCKLQDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], ]) } avx2 { Self( avx2::_mm_unpacklo_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     // Lane# 0\n     self.as_array()[1],\n     other.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpackhi_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpackhi_epi64)\n\n\n * `PUNPCKHQDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ // Lane# 0
        self.as_array()[1], other.as_array()[1], ]) } avx2 { Self( avx2::_mm_unpackhi_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn max(&self, other: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), ]) } avx2 { I64x2::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), ]) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn min(&self, other: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), ]) } avx2 { I64x2::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), ]) } }
    }
}
impl crate::SimdBase64 for I64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # ;}\n # impl SomeTraitForDoc for I64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x2  ,\n # )  -> I64x2\n # {\n I64x2::from([\n     ((self.as_array()[0] as i32) as i64) * ((other.as_array()[0] as i32) as i64),\n     ((self.as_array()[1] as i32) as i64) * ((other.as_array()[1] as i32) as i64),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_mul_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mul_epi32)\n\n\n * `PMULDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn mul_lo(&self, other: I64x2) -> I64x2 {
        select_impl_block! { scalar { I64x2::from([ ((self.as_array()[0] as i32) as i64) * ((other.as_array()[0] as i32) as i64), ((self.as_array()[1] as i32) as i64) * ((other.as_array()[1] as i32) as i64), ]) } avx2 { Self( avx2::_mm_mul_epi32 (self.0, other.0)) } }
    }
}
select_impl! { scalar { type I64x4Internal = [i64 ; 4]; } avx2 { type I64x4Internal = ::std::arch::x86_64::__m256i; } }
#[doc = "`[i64; 4]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct I64x4(I64x4Internal);
unsafe impl bytemuck::Pod for I64x4 {}
unsafe impl bytemuck::Zeroable for I64x4 {}
impl PartialEq for I64x4 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for I64x4 {}
impl Default for I64x4 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for I64x4 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for I64x4 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "I64x4({:?})", <[i64; 4]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for I64x4 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for I64x4 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 4];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <i64 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<I64x4> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> I64x4 {
        let mut out = I64x4::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for I64x4 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[i64; 4]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for I64x4 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[i64; 4]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for I64x4 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_xor_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_xor_si256)\n\n\n * `VPXOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm256_xor_si256 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for I64x4 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_or_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_or_si256)\n\n\n * `VPOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm256_or_si256 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for I64x4 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_and_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_and_si256)\n\n\n * `VPAND ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm256_and_si256 (self.0, rhs.0)) } }
    }
}
impl AddAssign for I64x4 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_add_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_add_epi64)\n\n\n * `VPADDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_add_epi64 (self.0, rhs.0)) } }
    }
}
impl SubAssign for I64x4 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sub_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sub_epi64)\n\n\n * `VPSUBQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_sub_epi64 (self.0, rhs.0)) } }
    }
}
impl I64x4 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: I64x4 =\n     I64x4::from_array([0, 1, 2, 3]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as i64, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [i64; 4]) -> I64x4 {
        select_impl_block! { scalar { I64x4(array) } avx2 { I64x4(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[i64; 4]> for I64x4 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_loadu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_loadu_si256)\n\n\n * `VMOVDQU ymm, m256`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [i64; 4]) -> I64x4 {
        select_impl_block! { scalar { I64x4(array) } avx2 { I64x4(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_loadu_si256 (array.as_ptr() as *const ::std::arch::x86_64::__m256i) }) } }
    }
}
impl From<I64x4> for [i64; 4] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_storeu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_storeu_si256)\n\n\n * `VMOVDQU m256, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I64x4) -> [i64; 4] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [i64; 4] = [0; 4]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_storeu_si256 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m256i, vector.0) } out } }
    }
}
impl From<I8x32> for I64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x32\nas little endian bits of I64x4."]
    #[inline(always)]
    fn from(x: I8x32) -> I64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x16> for I64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x16\nas little endian bits of I64x4."]
    #[inline(always)]
    fn from(x: I16x16) -> I64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x8> for I64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x8\nas little endian bits of I64x4."]
    #[inline(always)]
    fn from(x: I32x8) -> I64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x32> for I64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x32\nas little endian bits of I64x4."]
    #[inline(always)]
    fn from(x: U8x32) -> I64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x16> for I64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x16\nas little endian bits of I64x4."]
    #[inline(always)]
    fn from(x: U16x16) -> I64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x8> for I64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x8\nas little endian bits of I64x4."]
    #[inline(always)]
    fn from(x: U32x8) -> I64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x4> for I64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x4\nas little endian bits of I64x4."]
    #[inline(always)]
    fn from(x: U64x4) -> I64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x4> for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I32x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n         i64::from(vector.as_array()[0]),\n         i64::from(vector.as_array()[1]),\n         i64::from(vector.as_array()[2]),\n         i64::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi32_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi32_epi64)\n\n\n * `VPMOVSXDQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I32x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ i64::from(vector.as_array()[0]), i64::from(vector.as_array()[1]), i64::from(vector.as_array()[2]), i64::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_cvtepi32_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I8x16> for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I8x16  ,\n # )  -> I64x4\n # {\n I64x4::from([\n         i64::from(vector.as_array()[0]),\n         i64::from(vector.as_array()[1]),\n         i64::from(vector.as_array()[2]),\n         i64::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi8_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi8_epi64)\n\n\n * `VPMOVSXBQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I8x16) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ i64::from(vector.as_array()[0]), i64::from(vector.as_array()[1]), i64::from(vector.as_array()[2]), i64::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_cvtepi8_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I16x8> for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I16x8  ,\n # )  -> I64x4\n # {\n I64x4::from([\n         i64::from(vector.as_array()[0]),\n         i64::from(vector.as_array()[1]),\n         i64::from(vector.as_array()[2]),\n         i64::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi16_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi16_epi64)\n\n\n * `VPMOVSXWQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I16x8) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ i64::from(vector.as_array()[0]), i64::from(vector.as_array()[1]), i64::from(vector.as_array()[2]), i64::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_cvtepi16_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<I32x4> for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I32x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n         i64::from(vector.as_array()[0]),\n         i64::from(vector.as_array()[1]),\n         i64::from(vector.as_array()[2]),\n         i64::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepi32_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepi32_epi64)\n\n\n * `VPMOVSXDQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: I32x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ i64::from(vector.as_array()[0]), i64::from(vector.as_array()[1]), i64::from(vector.as_array()[2]), i64::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_cvtepi32_epi64 (vector.0)) } }
    }
}
impl From<I64x2> for I64x4 {
    #[doc = " NOTE: this will _zero_ the upper bits of the destination. Other\n intrinsics are more effcient, but leave the upper bits undefined.\n At present, these more effcient intrinsics are not exposed.\n\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I64x2  ,\n # )  -> I64x4\n # {\n let mut out = [0; 4];\n out[0..2].copy_from_slice(&vector.as_array());\n I64x4::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_zextsi128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_zextsi128_si256)\n\n\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I64x2) -> I64x4 {
        select_impl_block! { scalar { let mut out = [0; 4]; out[0..2].copy_from_slice(&vector.as_array()); I64x4::from(out) } avx2 { Self( avx2::_mm256_zextsi128_si256 (vector.0)) } }
    }
}
impl From<[I64x2; 2]> for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vectors  : [I64x2; 2]  ,\n # )  -> I64x4\n # {\n let mut out = [0; 4];\n out[0..2].copy_from_slice(&vectors[0].as_array());\n out[2..].copy_from_slice(&vectors[1].as_array());\n I64x4::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_m128i`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_m128i)\n\n\n * `VINSERTF128 ymm, ymm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vectors: [I64x2; 2]) -> I64x4 {
        select_impl_block! { scalar { let mut out = [0; 4]; out[0..2].copy_from_slice(&vectors[0].as_array()); out[2..].copy_from_slice(&vectors[1].as_array()); I64x4::from(out) } avx2 { Self( avx2::_mm256_set_m128i (vectors[1].0, vectors[0].0)) } }
    }
} // TODO: this doesn't show up in the docs.
impl From<I64x4> for [I64x2; 2] {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I64x4  ,\n # )  -> [I64x2; 2]\n # {\n let mut lo = [0; 2];\n let mut hi = [0; 2];\n lo.copy_from_slice(&vector.as_array()[0..2]);\n hi.copy_from_slice(&vector.as_array()[2..]);\n [I64x2::from(lo), I64x2::from(hi)]\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extracti128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extracti128_si256)\n\n\n * `VEXTRACTI128 xmm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: I64x4) -> [I64x2; 2] {
        select_impl_block! { scalar { let mut lo = [0; 2]; let mut hi = [0; 2]; lo.copy_from_slice(&vector.as_array()[0..2]); hi.copy_from_slice(&vector.as_array()[2..]); [I64x2::from(lo), I64x2::from(hi)] } avx2 { [ I64x2( avx2::_mm256_extracti128_si256 ::<0>(vector.0)), I64x2( avx2::_mm256_extracti128_si256 ::<1>(vector.0)), ] } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I32x4> for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : I32x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i32gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i32gather_epi64)\n\n\n * `VPGATHERDQ ymm, vm32x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i64, indices: I32x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i32gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : I32x4  ,\n #         mask  : I64x4  ,\n #         src  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     if ((mask.as_array()[0] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if ((mask.as_array()[2] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if ((mask.as_array()[3] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i32gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i32gather_epi64)\n\n\n * `VPGATHERDQ ymm, vm32x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i64, indices: I32x4, mask: I64x4, src: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ if ((mask.as_array()[0] as u64) >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u64) >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if ((mask.as_array()[2] as u64) >> 63) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if ((mask.as_array()[3] as u64) >> 63) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i32gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<U64x4> for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : U64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i64gather_epi64)\n\n\n * `VPGATHERQQ ymm, vm64x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i64, indices: U64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i64gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : U64x4  ,\n #         mask  : I64x4  ,\n #         src  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     if ((mask.as_array()[0] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if ((mask.as_array()[2] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if ((mask.as_array()[3] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i64gather_epi64)\n\n\n * `VPGATHERQQ ymm, vm64x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i64, indices: U64x4, mask: I64x4, src: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ if ((mask.as_array()[0] as u64) >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u64) >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if ((mask.as_array()[2] as u64) >> 63) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if ((mask.as_array()[3] as u64) >> 63) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i64gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I64x4> for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i64gather_epi64)\n\n\n * `VPGATHERQQ ymm, vm64x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const i64, indices: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i64gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const i64  ,\n #         indices  : I64x4  ,\n #         mask  : I64x4  ,\n #         src  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     if ((mask.as_array()[0] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if ((mask.as_array()[1] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if ((mask.as_array()[2] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if ((mask.as_array()[3] as u64) >> 63) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i64gather_epi64)\n\n\n * `VPGATHERQQ ymm, vm64x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const i64, indices: I64x4, mask: I64x4, src: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ if ((mask.as_array()[0] as u64) >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if ((mask.as_array()[1] as u64) >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if ((mask.as_array()[2] as u64) >> 63) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if ((mask.as_array()[3] as u64) >> 63) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i64gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
} // Static shift
impl ShlAssign<u64> for I64x4 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I64x4\n # {\n if amount >= 64 {\n     I64x4::ZERO\n } else {\n     I64x4::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: u64) -> I64x4 {
        select_impl_block! { scalar { if amount >= 64 { I64x4::ZERO } else { I64x4::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, ]) } } avx2 { if amount >= 64 { I64x4::ZERO } else { I64x4::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, ]) } } }
    }
} // Variable shift
impl ShlAssign<I64x4> for I64x4 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: I64x4) {
        *self = (*self) << amount;
    }
}
impl Shl<I64x4> for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I64x4  ,\n # )  -> I64x4\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..64).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n I64x4::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: I64x4) -> I64x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x << amm } else { 0 }; } I64x4::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x << amm } else { 0 }; } I64x4::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for I64x4 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> I64x4\n # {\n if amount >= 64 {\n     let mut out = self.as_array();\n     for x in out.iter_mut() {\n         *x = if *x < 0 { -1 } else { 0 };\n     }\n     I64x4::from(out)\n } else {\n     I64x4::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: u64) -> I64x4 {
        select_impl_block! { scalar { if amount >= 64 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I64x4::from(out) } else { I64x4::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, ]) } } avx2 { if amount >= 64 { let mut out = self.as_array(); for x in out.iter_mut() { *x = if *x < 0 { -1 } else { 0 }; } I64x4::from(out) } else { I64x4::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, ]) } } }
    }
} // Variable shift
impl ShrAssign<I64x4> for I64x4 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: I64x4) {
        *self = (*self) >> amount;
    }
}
impl Shr<I64x4> for I64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : I64x4  ,\n # )  -> I64x4\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..64).contains(&amm) {\n         *x >> amm\n     } else if *x < 0 {\n         -1\n     }  else {\n         0\n     };\n }\n I64x4::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: I64x4) -> I64x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I64x4::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x >> amm } else if *x < 0 { -1 } else { 0 }; } I64x4::from(out) } }
    }
}
impl SimdBase for I64x4 {
    type Scalar = i64;
    type Array = [i64; 4];
    type Signed = I64x4;
    type Unsigned = U64x4;
    const LANES: usize = 4;
    const ZERO: Self = Self::from_array([0; 4]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_testz_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_testz_si256)\n\n\n * `VPTEST ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm256_testz_si256 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i64  ,\n # )  -> I64x4\n # {\n let mut out = [0; 4];\n out[0] = scalar;\n I64x4::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: i64) -> I64x4 {
        select_impl_block! { scalar { let mut out = [0; 4]; out[0] = scalar; I64x4::from(out) } avx2 { Self( avx2::_mm256_set_epi64x ( 0, 0, 0, scalar as i64, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i64\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> i64\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extract_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extract_epi64)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> i64 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm256_extract_epi64 ::<I>(self.0) as i64 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : i64  ,\n # )  -> I64x4\n # {\n I64x4::from([scalar; 4])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set1_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: i64) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([scalar; 4]) } avx2 { Self( avx2::_mm256_set1_epi64x (scalar as i64)) } }
    }
    type BroadcastLoInput = I64x2;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : I64x2  ,\n # )  -> I64x4\n # {\n I64x4::from([vector.as_array()[0]; 4])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_broadcastq_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_broadcastq_epi64)\n\n\n * `VPBROADCASTQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: I64x2) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([vector.as_array()[0]; 4]) } avx2 { Self( avx2::_mm256_broadcastq_epi64 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     if self.as_array()[0] == other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpeq_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpeq_epi64)\n\n\n * `VPCMPEQQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ if self.as_array()[0] == other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] == other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] == other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] == other.as_array()[3] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpeq_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_andnot_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_andnot_si256)\n\n\n * `VPANDN ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_andnot_si256 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     if self.as_array()[0] > other.as_array()[0] {  -1  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  -1  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  -1  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  -1  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpgt_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpgt_epi64)\n\n\n * `VPCMPGTQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_gt(&self, other: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ if self.as_array()[0] > other.as_array()[0] { -1 } else { 0 }, if self.as_array()[1] > other.as_array()[1] { -1 } else { 0 }, if self.as_array()[2] > other.as_array()[2] { -1 } else { 0 }, if self.as_array()[3] > other.as_array()[3] { -1 } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpgt_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x4\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n I64x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_slli_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_slli_epi64)\n\n\n * `VPSLLQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> I64x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } I64x4::from(out) } avx2 { const USELESS_ARRAY: [u8; 64] = [0; 64]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_slli_epi64 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x4\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n I64x4::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> I64x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I64x4::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } I64x4::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     // Lane# 1\n     self.as_array()[2],\n     other.as_array()[2],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpacklo_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpacklo_epi64)\n\n\n * `VPUNPCKLQDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], // Lane# 1
        self.as_array()[2], other.as_array()[2], ]) } avx2 { Self( avx2::_mm256_unpacklo_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     // Lane# 0\n     self.as_array()[1],\n     other.as_array()[1],\n     // Lane# 1\n     self.as_array()[3],\n     other.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpackhi_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpackhi_epi64)\n\n\n * `VPUNPCKHQDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ // Lane# 0
        self.as_array()[1], other.as_array()[1], // Lane# 1
        self.as_array()[3], other.as_array()[3], ]) } avx2 { Self( avx2::_mm256_unpackhi_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn max(&self, other: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), ]) } avx2 { I64x4::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), ]) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn min(&self, other: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), ]) } avx2 { I64x4::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), ]) } }
    }
}
impl crate::SimdBase4x for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n #     <\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n         (if B0 { if_true } else { *self }).as_array()[0],\n         (if B1 { if_true } else { *self }).as_array()[1],\n         (if B2 { if_true } else { *self }).as_array()[2],\n         (if B3 { if_true } else { *self }).as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_blend_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_blend_epi32)\n\n\n * `VPBLENDD ymm, ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn blend<const B3: bool, const B2: bool, const B1: bool, const B0: bool>(
        &self,
        if_true: I64x4,
    ) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ (if B0 { if_true } else { *self }).as_array()[0], (if B1 { if_true } else { *self }).as_array()[1], (if B2 { if_true } else { *self }).as_array()[2], (if B3 { if_true } else { *self }).as_array()[3], ]) } avx2 { Self( avx2::_mm256_blend_epi32 ::< B3, B3, B2, B2, B1, B1, B0, B0 >(self.0, if_true.0)) } }
    }
}
impl crate::SimdBase64 for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : I64x4  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     ((self.as_array()[0] as i32) as i64) * ((other.as_array()[0] as i32) as i64),\n     ((self.as_array()[1] as i32) as i64) * ((other.as_array()[1] as i32) as i64),\n     ((self.as_array()[2] as i32) as i64) * ((other.as_array()[2] as i32) as i64),\n     ((self.as_array()[3] as i32) as i64) * ((other.as_array()[3] as i32) as i64),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mul_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mul_epi32)\n\n\n * `VPMULDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn mul_lo(&self, other: I64x4) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ ((self.as_array()[0] as i32) as i64) * ((other.as_array()[0] as i32) as i64), ((self.as_array()[1] as i32) as i64) * ((other.as_array()[1] as i32) as i64), ((self.as_array()[2] as i32) as i64) * ((other.as_array()[2] as i32) as i64), ((self.as_array()[3] as i32) as i64) * ((other.as_array()[3] as i32) as i64), ]) } avx2 { Self( avx2::_mm256_mul_epi32 (self.0, other.0)) } }
    }
}
impl crate::SimdBase4x64 for I64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x4\n # ;}\n # impl SomeTraitForDoc for I64x4 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> I64x4\n # {\n I64x4::from([\n     self.as_array()[I0],\n     self.as_array()[I1],\n     self.as_array()[I2],\n     self.as_array()[I3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_permute4x64_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_permute4x64_epi64)\n\n\n * `VPERMQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(&self) -> I64x4 {
        select_impl_block! { scalar { I64x4::from([ self.as_array()[I0], self.as_array()[I1], self.as_array()[I2], self.as_array()[I3], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm256_permute4x64_epi64 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
select_impl! { scalar { type U8x16Internal = [u8 ; 16]; } avx2 { type U8x16Internal = ::std::arch::x86_64::__m128i; } }
#[doc = "`[u8; 16]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct U8x16(U8x16Internal);
unsafe impl bytemuck::Pod for U8x16 {}
unsafe impl bytemuck::Zeroable for U8x16 {}
impl PartialEq for U8x16 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for U8x16 {}
impl Default for U8x16 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for U8x16 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for U8x16 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "U8x16({:?})", <[u8; 16]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for U8x16 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for U8x16 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 16];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <u8 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<U8x16> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> U8x16 {
        let mut out = U8x16::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for U8x16 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[u8; 16]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for U8x16 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[u8; 16]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for U8x16 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n     self.as_array()[8] ^ rhs.as_array()[8],\n     self.as_array()[9] ^ rhs.as_array()[9],\n     self.as_array()[10] ^ rhs.as_array()[10],\n     self.as_array()[11] ^ rhs.as_array()[11],\n     self.as_array()[12] ^ rhs.as_array()[12],\n     self.as_array()[13] ^ rhs.as_array()[13],\n     self.as_array()[14] ^ rhs.as_array()[14],\n     self.as_array()[15] ^ rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_xor_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_si128)\n\n\n * `PXOR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], self.as_array()[8] ^ rhs.as_array()[8], self.as_array()[9] ^ rhs.as_array()[9], self.as_array()[10] ^ rhs.as_array()[10], self.as_array()[11] ^ rhs.as_array()[11], self.as_array()[12] ^ rhs.as_array()[12], self.as_array()[13] ^ rhs.as_array()[13], self.as_array()[14] ^ rhs.as_array()[14], self.as_array()[15] ^ rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm_xor_si128 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for U8x16 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n     self.as_array()[8] | rhs.as_array()[8],\n     self.as_array()[9] | rhs.as_array()[9],\n     self.as_array()[10] | rhs.as_array()[10],\n     self.as_array()[11] | rhs.as_array()[11],\n     self.as_array()[12] | rhs.as_array()[12],\n     self.as_array()[13] | rhs.as_array()[13],\n     self.as_array()[14] | rhs.as_array()[14],\n     self.as_array()[15] | rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_or_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_or_si128)\n\n\n * `POR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], self.as_array()[8] | rhs.as_array()[8], self.as_array()[9] | rhs.as_array()[9], self.as_array()[10] | rhs.as_array()[10], self.as_array()[11] | rhs.as_array()[11], self.as_array()[12] | rhs.as_array()[12], self.as_array()[13] | rhs.as_array()[13], self.as_array()[14] | rhs.as_array()[14], self.as_array()[15] | rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm_or_si128 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for U8x16 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n     self.as_array()[8] & rhs.as_array()[8],\n     self.as_array()[9] & rhs.as_array()[9],\n     self.as_array()[10] & rhs.as_array()[10],\n     self.as_array()[11] & rhs.as_array()[11],\n     self.as_array()[12] & rhs.as_array()[12],\n     self.as_array()[13] & rhs.as_array()[13],\n     self.as_array()[14] & rhs.as_array()[14],\n     self.as_array()[15] & rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_and_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_si128)\n\n\n * `PAND xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], self.as_array()[8] & rhs.as_array()[8], self.as_array()[9] & rhs.as_array()[9], self.as_array()[10] & rhs.as_array()[10], self.as_array()[11] & rhs.as_array()[11], self.as_array()[12] & rhs.as_array()[12], self.as_array()[13] & rhs.as_array()[13], self.as_array()[14] & rhs.as_array()[14], self.as_array()[15] & rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm_and_si128 (self.0, rhs.0)) } }
    }
}
impl AddAssign for U8x16 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_add(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_add(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_add(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_add(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_add(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_add(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_add(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_add(rhs.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_add_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_epi8)\n\n\n * `PADDB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), self.as_array()[8].wrapping_add(rhs.as_array()[8]), self.as_array()[9].wrapping_add(rhs.as_array()[9]), self.as_array()[10].wrapping_add(rhs.as_array()[10]), self.as_array()[11].wrapping_add(rhs.as_array()[11]), self.as_array()[12].wrapping_add(rhs.as_array()[12]), self.as_array()[13].wrapping_add(rhs.as_array()[13]), self.as_array()[14].wrapping_add(rhs.as_array()[14]), self.as_array()[15].wrapping_add(rhs.as_array()[15]), ]) } avx2 { Self( avx2::_mm_add_epi8 (self.0, rhs.0)) } }
    }
}
impl SubAssign for U8x16 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_sub(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_sub(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_sub(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_sub(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_sub(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_sub(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_sub(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_sub(rhs.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sub_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_epi8)\n\n\n * `PSUBB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), self.as_array()[8].wrapping_sub(rhs.as_array()[8]), self.as_array()[9].wrapping_sub(rhs.as_array()[9]), self.as_array()[10].wrapping_sub(rhs.as_array()[10]), self.as_array()[11].wrapping_sub(rhs.as_array()[11]), self.as_array()[12].wrapping_sub(rhs.as_array()[12]), self.as_array()[13].wrapping_sub(rhs.as_array()[13]), self.as_array()[14].wrapping_sub(rhs.as_array()[14]), self.as_array()[15].wrapping_sub(rhs.as_array()[15]), ]) } avx2 { Self( avx2::_mm_sub_epi8 (self.0, rhs.0)) } }
    }
}
impl U8x16 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: U8x16 =\n     U8x16::from_array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as u8, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [u8; 16]) -> U8x16 {
        select_impl_block! { scalar { U8x16(array) } avx2 { U8x16(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[u8; 16]> for U8x16 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_loadu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si128)\n\n\n * `MOVDQU xmm, m128`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [u8; 16]) -> U8x16 {
        select_impl_block! { scalar { U8x16(array) } avx2 { U8x16(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_loadu_si128 (array.as_ptr() as *const ::std::arch::x86_64::__m128i) }) } }
    }
}
impl From<U8x16> for [u8; 16] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_storeu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_si128)\n\n\n * `MOVDQU m128, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U8x16) -> [u8; 16] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [u8; 16] = [0; 16]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_storeu_si128 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m128i, vector.0) } out } }
    }
}
impl From<I8x16> for U8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x16\nas little endian bits of U8x16."]
    #[inline(always)]
    fn from(x: I8x16) -> U8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x8> for U8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x8\nas little endian bits of U8x16."]
    #[inline(always)]
    fn from(x: I16x8) -> U8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x4> for U8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x4\nas little endian bits of U8x16."]
    #[inline(always)]
    fn from(x: I32x4) -> U8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x2> for U8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x2\nas little endian bits of U8x16."]
    #[inline(always)]
    fn from(x: I64x2) -> U8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x8> for U8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x8\nas little endian bits of U8x16."]
    #[inline(always)]
    fn from(x: U16x8) -> U8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x4> for U8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x4\nas little endian bits of U8x16."]
    #[inline(always)]
    fn from(x: U32x4) -> U8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x2> for U8x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x2\nas little endian bits of U8x16."]
    #[inline(always)]
    fn from(x: U64x2) -> U8x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl crate::SimdSaturatingArithmetic for U8x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_add(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_adds_epu8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_adds_epu8)\n\n\n * `PADDUSB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_add(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_add(*src); } Self::from(out) } avx2 { Self( avx2::_mm_adds_epu8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_sub(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_subs_epu8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_subs_epu8)\n\n\n * `PSUBUSB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_sub(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_sub(*src); } Self::from(out) } avx2 { Self( avx2::_mm_subs_epu8 (self.0, other.0)) } }
    }
} // Static shift
impl ShlAssign<u64> for U8x16 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U8x16\n # {\n if amount >= 8 {\n     U8x16::ZERO\n } else {\n     U8x16::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n         self.as_array()[8] << amount,\n         self.as_array()[9] << amount,\n         self.as_array()[10] << amount,\n         self.as_array()[11] << amount,\n         self.as_array()[12] << amount,\n         self.as_array()[13] << amount,\n         self.as_array()[14] << amount,\n         self.as_array()[15] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: u64) -> U8x16 {
        select_impl_block! { scalar { if amount >= 8 { U8x16::ZERO } else { U8x16::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, ]) } } avx2 { if amount >= 8 { U8x16::ZERO } else { U8x16::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, ]) } } }
    }
} // Variable shift
impl ShlAssign<U8x16> for U8x16 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: U8x16) {
        *self = (*self) << amount;
    }
}
impl Shl<U8x16> for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U8x16  ,\n # )  -> U8x16\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..8).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n U8x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: U8x16) -> U8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x << amm } else { 0 }; } U8x16::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x << amm } else { 0 }; } U8x16::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for U8x16 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U8x16\n # {\n if amount >= 8 {\n     U8x16::ZERO\n } else {\n     U8x16::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n         self.as_array()[8] >> amount,\n         self.as_array()[9] >> amount,\n         self.as_array()[10] >> amount,\n         self.as_array()[11] >> amount,\n         self.as_array()[12] >> amount,\n         self.as_array()[13] >> amount,\n         self.as_array()[14] >> amount,\n         self.as_array()[15] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: u64) -> U8x16 {
        select_impl_block! { scalar { if amount >= 8 { U8x16::ZERO } else { U8x16::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, ]) } } avx2 { if amount >= 8 { U8x16::ZERO } else { U8x16::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, ]) } } }
    }
} // Variable shift
impl ShrAssign<U8x16> for U8x16 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: U8x16) {
        *self = (*self) >> amount;
    }
}
impl Shr<U8x16> for U8x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U8x16  ,\n # )  -> U8x16\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..8).contains(&amm) {\n         *x >> amm\n     }  else {\n         0\n     };\n }\n U8x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: U8x16) -> U8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x >> amm } else { 0 }; } U8x16::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x >> amm } else { 0 }; } U8x16::from(out) } }
    }
}
impl SimdBase for U8x16 {
    type Scalar = u8;
    type Array = [u8; 16];
    type Signed = I8x16;
    type Unsigned = U8x16;
    const LANES: usize = 16;
    const ZERO: Self = Self::from_array([0; 16]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_testz_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128)\n\n\n * `PTEST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm_testz_si128 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u8  ,\n # )  -> U8x16\n # {\n let mut out = [0; 16];\n out[0] = scalar;\n U8x16::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: u8) -> U8x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; out[0] = scalar; U8x16::from(out) } avx2 { Self( avx2::_mm_set_epi8 ( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, scalar as i8, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u8\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u8\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_extract_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_extract_epi8)\n\n\n * `PEXTRB r32, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> u8 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm_extract_epi8 ::<I>(self.0) as u8 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u8  ,\n # )  -> U8x16\n # {\n U8x16::from([scalar; 16])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set1_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: u8) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([scalar; 16]) } avx2 { Self( avx2::_mm_set1_epi8 (scalar as i8)) } }
    }
    type BroadcastLoInput = U8x16;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([vector.as_array()[0]; 16])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_broadcastb_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_broadcastb_epi8)\n\n\n * `VPBROADCASTB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([vector.as_array()[0]; 16]) } avx2 { Self( avx2::_mm_broadcastb_epi8 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     if self.as_array()[0] == other.as_array()[0] {  u8::MAX  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  u8::MAX  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  u8::MAX  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  u8::MAX  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  u8::MAX  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  u8::MAX  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  u8::MAX  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  u8::MAX  } else { 0 },\n     if self.as_array()[8] == other.as_array()[8] {  u8::MAX  } else { 0 },\n     if self.as_array()[9] == other.as_array()[9] {  u8::MAX  } else { 0 },\n     if self.as_array()[10] == other.as_array()[10] {  u8::MAX  } else { 0 },\n     if self.as_array()[11] == other.as_array()[11] {  u8::MAX  } else { 0 },\n     if self.as_array()[12] == other.as_array()[12] {  u8::MAX  } else { 0 },\n     if self.as_array()[13] == other.as_array()[13] {  u8::MAX  } else { 0 },\n     if self.as_array()[14] == other.as_array()[14] {  u8::MAX  } else { 0 },\n     if self.as_array()[15] == other.as_array()[15] {  u8::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpeq_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpeq_epi8)\n\n\n * `PCMPEQB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ if self.as_array()[0] == other.as_array()[0] { u8::MAX } else { 0 }, if self.as_array()[1] == other.as_array()[1] { u8::MAX } else { 0 }, if self.as_array()[2] == other.as_array()[2] { u8::MAX } else { 0 }, if self.as_array()[3] == other.as_array()[3] { u8::MAX } else { 0 }, if self.as_array()[4] == other.as_array()[4] { u8::MAX } else { 0 }, if self.as_array()[5] == other.as_array()[5] { u8::MAX } else { 0 }, if self.as_array()[6] == other.as_array()[6] { u8::MAX } else { 0 }, if self.as_array()[7] == other.as_array()[7] { u8::MAX } else { 0 }, if self.as_array()[8] == other.as_array()[8] { u8::MAX } else { 0 }, if self.as_array()[9] == other.as_array()[9] { u8::MAX } else { 0 }, if self.as_array()[10] == other.as_array()[10] { u8::MAX } else { 0 }, if self.as_array()[11] == other.as_array()[11] { u8::MAX } else { 0 }, if self.as_array()[12] == other.as_array()[12] { u8::MAX } else { 0 }, if self.as_array()[13] == other.as_array()[13] { u8::MAX } else { 0 }, if self.as_array()[14] == other.as_array()[14] { u8::MAX } else { 0 }, if self.as_array()[15] == other.as_array()[15] { u8::MAX } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpeq_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n     self.as_array()[8] & (!other.as_array()[8]),\n     self.as_array()[9] & (!other.as_array()[9]),\n     self.as_array()[10] & (!other.as_array()[10]),\n     self.as_array()[11] & (!other.as_array()[11]),\n     self.as_array()[12] & (!other.as_array()[12]),\n     self.as_array()[13] & (!other.as_array()[13]),\n     self.as_array()[14] & (!other.as_array()[14]),\n     self.as_array()[15] & (!other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_andnot_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_si128)\n\n\n * `PANDN xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), self.as_array()[8] & (!other.as_array()[8]), self.as_array()[9] & (!other.as_array()[9]), self.as_array()[10] & (!other.as_array()[10]), self.as_array()[11] & (!other.as_array()[11]), self.as_array()[12] & (!other.as_array()[12]), self.as_array()[13] & (!other.as_array()[13]), self.as_array()[14] & (!other.as_array()[14]), self.as_array()[15] & (!other.as_array()[15]), ]) } avx2 { Self( avx2::_mm_andnot_si128 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     if self.as_array()[0] > other.as_array()[0] {  u8::MAX  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  u8::MAX  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  u8::MAX  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  u8::MAX  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  u8::MAX  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  u8::MAX  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  u8::MAX  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  u8::MAX  } else { 0 },\n     if self.as_array()[8] > other.as_array()[8] {  u8::MAX  } else { 0 },\n     if self.as_array()[9] > other.as_array()[9] {  u8::MAX  } else { 0 },\n     if self.as_array()[10] > other.as_array()[10] {  u8::MAX  } else { 0 },\n     if self.as_array()[11] > other.as_array()[11] {  u8::MAX  } else { 0 },\n     if self.as_array()[12] > other.as_array()[12] {  u8::MAX  } else { 0 },\n     if self.as_array()[13] > other.as_array()[13] {  u8::MAX  } else { 0 },\n     if self.as_array()[14] > other.as_array()[14] {  u8::MAX  } else { 0 },\n     if self.as_array()[15] > other.as_array()[15] {  u8::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n **NOTE:** this implementation uses an efficient vector polyfill, though this operation is not natively supported.\n ```ignore\n // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK\n let sign_bit = Self::broadcast(1 << 7);\n Self::from(I8x16::from(*self ^ sign_bit).cmp_gt(\n     I8x16::from(other ^ sign_bit)\n ))\n ```"]
    #[inline(always)]
    fn cmp_gt(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ if self.as_array()[0] > other.as_array()[0] { u8::MAX } else { 0 }, if self.as_array()[1] > other.as_array()[1] { u8::MAX } else { 0 }, if self.as_array()[2] > other.as_array()[2] { u8::MAX } else { 0 }, if self.as_array()[3] > other.as_array()[3] { u8::MAX } else { 0 }, if self.as_array()[4] > other.as_array()[4] { u8::MAX } else { 0 }, if self.as_array()[5] > other.as_array()[5] { u8::MAX } else { 0 }, if self.as_array()[6] > other.as_array()[6] { u8::MAX } else { 0 }, if self.as_array()[7] > other.as_array()[7] { u8::MAX } else { 0 }, if self.as_array()[8] > other.as_array()[8] { u8::MAX } else { 0 }, if self.as_array()[9] > other.as_array()[9] { u8::MAX } else { 0 }, if self.as_array()[10] > other.as_array()[10] { u8::MAX } else { 0 }, if self.as_array()[11] > other.as_array()[11] { u8::MAX } else { 0 }, if self.as_array()[12] > other.as_array()[12] { u8::MAX } else { 0 }, if self.as_array()[13] > other.as_array()[13] { u8::MAX } else { 0 }, if self.as_array()[14] > other.as_array()[14] { u8::MAX } else { 0 }, if self.as_array()[15] > other.as_array()[15] { u8::MAX } else { 0 }, ]) } avx2 { // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK
        let sign_bit = Self::broadcast(1 << 7); Self::from(I8x16::from(*self ^ sign_bit).cmp_gt( I8x16::from(other ^ sign_bit) )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x16\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n U8x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> U8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U8x16::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U8x16::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x16\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n U8x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> U8x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U8x16::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U8x16::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpacklo_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpacklo_epi8)\n\n\n * `PUNPCKLBW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], ]) } avx2 { Self( avx2::_mm_unpacklo_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     // Lane# 0\n     self.as_array()[8],\n     other.as_array()[8],\n     self.as_array()[9],\n     other.as_array()[9],\n     self.as_array()[10],\n     other.as_array()[10],\n     self.as_array()[11],\n     other.as_array()[11],\n     self.as_array()[12],\n     other.as_array()[12],\n     self.as_array()[13],\n     other.as_array()[13],\n     self.as_array()[14],\n     other.as_array()[14],\n     self.as_array()[15],\n     other.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpackhi_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpackhi_epi8)\n\n\n * `PUNPCKHBW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ // Lane# 0
        self.as_array()[8], other.as_array()[8], self.as_array()[9], other.as_array()[9], self.as_array()[10], other.as_array()[10], self.as_array()[11], other.as_array()[11], self.as_array()[12], other.as_array()[12], self.as_array()[13], other.as_array()[13], self.as_array()[14], other.as_array()[14], self.as_array()[15], other.as_array()[15], ]) } avx2 { Self( avx2::_mm_unpackhi_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n     self.as_array()[8].max(other.as_array()[8]),\n     self.as_array()[9].max(other.as_array()[9]),\n     self.as_array()[10].max(other.as_array()[10]),\n     self.as_array()[11].max(other.as_array()[11]),\n     self.as_array()[12].max(other.as_array()[12]),\n     self.as_array()[13].max(other.as_array()[13]),\n     self.as_array()[14].max(other.as_array()[14]),\n     self.as_array()[15].max(other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_max_epu8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epu8)\n\n\n * `PMAXUB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), self.as_array()[8].max(other.as_array()[8]), self.as_array()[9].max(other.as_array()[9]), self.as_array()[10].max(other.as_array()[10]), self.as_array()[11].max(other.as_array()[11]), self.as_array()[12].max(other.as_array()[12]), self.as_array()[13].max(other.as_array()[13]), self.as_array()[14].max(other.as_array()[14]), self.as_array()[15].max(other.as_array()[15]), ]) } avx2 { Self( avx2::_mm_max_epu8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x16  ,\n # )  -> U8x16\n # {\n U8x16::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n     self.as_array()[8].min(other.as_array()[8]),\n     self.as_array()[9].min(other.as_array()[9]),\n     self.as_array()[10].min(other.as_array()[10]),\n     self.as_array()[11].min(other.as_array()[11]),\n     self.as_array()[12].min(other.as_array()[12]),\n     self.as_array()[13].min(other.as_array()[13]),\n     self.as_array()[14].min(other.as_array()[14]),\n     self.as_array()[15].min(other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_min_epu8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_epu8)\n\n\n * `PMINUB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: U8x16) -> U8x16 {
        select_impl_block! { scalar { U8x16::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), self.as_array()[8].min(other.as_array()[8]), self.as_array()[9].min(other.as_array()[9]), self.as_array()[10].min(other.as_array()[10]), self.as_array()[11].min(other.as_array()[11]), self.as_array()[12].min(other.as_array()[12]), self.as_array()[13].min(other.as_array()[13]), self.as_array()[14].min(other.as_array()[14]), self.as_array()[15].min(other.as_array()[15]), ]) } avx2 { Self( avx2::_mm_min_epu8 (self.0, other.0)) } }
    }
}
impl crate::SimdBase8 for U8x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x16\n # {\n let mut out = [0; 16];\n for (out_lane, src_lane) in out\n     .chunks_exact_mut(16)\n     .zip(self.as_array().chunks_exact(16))\n {\n     out_lane[AMOUNT..].copy_from_slice(&src_lane[0..16 - AMOUNT]);\n }\n U8x16::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_slli_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_slli_si128)\n\n\n * `PSLLDQ xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_bytes_left<const AMOUNT: usize>(&self) -> U8x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; for (out_lane, src_lane) in out .chunks_exact_mut(16) .zip(self.as_array().chunks_exact(16)) { out_lane[AMOUNT..].copy_from_slice(&src_lane[0..16 - AMOUNT]); } U8x16::from(out) } avx2 { Self( avx2::_mm_slli_si128 ::<AMOUNT>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x16\n # {\n let mut out = [0; 16];\n for (out_lane, src_lane) in out\n     .chunks_exact_mut(16)\n     .zip(self.as_array().chunks_exact(16))\n {\n     out_lane[0..16 - AMOUNT].copy_from_slice(&src_lane[AMOUNT..]);\n }\n U8x16::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srli_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srli_si128)\n\n\n * `PSRLDQ xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_bytes_right<const AMOUNT: usize>(&self) -> U8x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; for (out_lane, src_lane) in out .chunks_exact_mut(16) .zip(self.as_array().chunks_exact(16)) { out_lane[0..16 - AMOUNT].copy_from_slice(&src_lane[AMOUNT..]); } U8x16::from(out) } avx2 { Self( avx2::_mm_srli_si128 ::<AMOUNT>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> u32\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> u32\n # {\n let mut out: u32 = 0;\n for (i, value) in self.as_array().iter().copied().enumerate() {\n     out |= u32::from((value as u8) >> 7) << i;\n }\n out\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_movemask_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_movemask_epi8)\n\n\n * `PMOVMSKB r32, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn most_significant_bits(&self) -> u32 {
        select_impl_block! { scalar { let mut out: u32 = 0; for (i, value) in self.as_array().iter().copied().enumerate() { out |= u32::from((value as u8) >> 7) << i; } out } avx2 { avx2::_mm_movemask_epi8 (self.0) as u32 } }
    }
}
impl U8x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         order  : U8x16  ,\n # )  -> U8x16\n # ;}\n # impl SomeTraitForDoc for U8x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         order  : U8x16  ,\n # )  -> U8x16\n # {\n let mut arr = [0; 16];\n for (lane_dst, (lane_src, order)) in\n     arr.chunks_exact_mut(16).zip(\n         self.as_array().chunks_exact(16)\n         .zip(order.as_array().chunks_exact(16))\n     )\n {\n     for (dst, idx) in lane_dst.iter_mut().zip(order) {\n         let idx = *idx;\n         *dst = if (idx >> 7) == 1 {\n             0\n         } else {\n             lane_src[(idx as usize) % 16]\n         };\n     }\n }\n arr.into()\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_shuffle_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shuffle_epi8)\n\n\n * `PSHUFB xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    pub fn shuffle(&self, order: U8x16) -> U8x16 {
        select_impl_block! { scalar { let mut arr = [0; 16]; for (lane_dst, (lane_src, order)) in arr.chunks_exact_mut(16).zip( self.as_array().chunks_exact(16) .zip(order.as_array().chunks_exact(16)) ) { for (dst, idx) in lane_dst.iter_mut().zip(order) { let idx = *idx; *dst = if (idx >> 7) == 1 { 0 } else { lane_src[(idx as usize) % 16] }; } } arr.into() } avx2 { Self( avx2::_mm_shuffle_epi8 (self.0, order.0)) } }
    }
}
select_impl! { scalar { type U8x32Internal = [u8 ; 32]; } avx2 { type U8x32Internal = ::std::arch::x86_64::__m256i; } }
#[doc = "`[u8; 32]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct U8x32(U8x32Internal);
unsafe impl bytemuck::Pod for U8x32 {}
unsafe impl bytemuck::Zeroable for U8x32 {}
impl PartialEq for U8x32 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for U8x32 {}
impl Default for U8x32 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for U8x32 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for U8x32 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "U8x32({:?})", <[u8; 32]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for U8x32 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for U8x32 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 32];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <u8 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<U8x32> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> U8x32 {
        let mut out = U8x32::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for U8x32 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[u8; 32]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for U8x32 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[u8; 32]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for U8x32 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n     self.as_array()[8] ^ rhs.as_array()[8],\n     self.as_array()[9] ^ rhs.as_array()[9],\n     self.as_array()[10] ^ rhs.as_array()[10],\n     self.as_array()[11] ^ rhs.as_array()[11],\n     self.as_array()[12] ^ rhs.as_array()[12],\n     self.as_array()[13] ^ rhs.as_array()[13],\n     self.as_array()[14] ^ rhs.as_array()[14],\n     self.as_array()[15] ^ rhs.as_array()[15],\n     self.as_array()[16] ^ rhs.as_array()[16],\n     self.as_array()[17] ^ rhs.as_array()[17],\n     self.as_array()[18] ^ rhs.as_array()[18],\n     self.as_array()[19] ^ rhs.as_array()[19],\n     self.as_array()[20] ^ rhs.as_array()[20],\n     self.as_array()[21] ^ rhs.as_array()[21],\n     self.as_array()[22] ^ rhs.as_array()[22],\n     self.as_array()[23] ^ rhs.as_array()[23],\n     self.as_array()[24] ^ rhs.as_array()[24],\n     self.as_array()[25] ^ rhs.as_array()[25],\n     self.as_array()[26] ^ rhs.as_array()[26],\n     self.as_array()[27] ^ rhs.as_array()[27],\n     self.as_array()[28] ^ rhs.as_array()[28],\n     self.as_array()[29] ^ rhs.as_array()[29],\n     self.as_array()[30] ^ rhs.as_array()[30],\n     self.as_array()[31] ^ rhs.as_array()[31],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_xor_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_xor_si256)\n\n\n * `VPXOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], self.as_array()[8] ^ rhs.as_array()[8], self.as_array()[9] ^ rhs.as_array()[9], self.as_array()[10] ^ rhs.as_array()[10], self.as_array()[11] ^ rhs.as_array()[11], self.as_array()[12] ^ rhs.as_array()[12], self.as_array()[13] ^ rhs.as_array()[13], self.as_array()[14] ^ rhs.as_array()[14], self.as_array()[15] ^ rhs.as_array()[15], self.as_array()[16] ^ rhs.as_array()[16], self.as_array()[17] ^ rhs.as_array()[17], self.as_array()[18] ^ rhs.as_array()[18], self.as_array()[19] ^ rhs.as_array()[19], self.as_array()[20] ^ rhs.as_array()[20], self.as_array()[21] ^ rhs.as_array()[21], self.as_array()[22] ^ rhs.as_array()[22], self.as_array()[23] ^ rhs.as_array()[23], self.as_array()[24] ^ rhs.as_array()[24], self.as_array()[25] ^ rhs.as_array()[25], self.as_array()[26] ^ rhs.as_array()[26], self.as_array()[27] ^ rhs.as_array()[27], self.as_array()[28] ^ rhs.as_array()[28], self.as_array()[29] ^ rhs.as_array()[29], self.as_array()[30] ^ rhs.as_array()[30], self.as_array()[31] ^ rhs.as_array()[31], ]) } avx2 { Self( avx2::_mm256_xor_si256 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for U8x32 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n     self.as_array()[8] | rhs.as_array()[8],\n     self.as_array()[9] | rhs.as_array()[9],\n     self.as_array()[10] | rhs.as_array()[10],\n     self.as_array()[11] | rhs.as_array()[11],\n     self.as_array()[12] | rhs.as_array()[12],\n     self.as_array()[13] | rhs.as_array()[13],\n     self.as_array()[14] | rhs.as_array()[14],\n     self.as_array()[15] | rhs.as_array()[15],\n     self.as_array()[16] | rhs.as_array()[16],\n     self.as_array()[17] | rhs.as_array()[17],\n     self.as_array()[18] | rhs.as_array()[18],\n     self.as_array()[19] | rhs.as_array()[19],\n     self.as_array()[20] | rhs.as_array()[20],\n     self.as_array()[21] | rhs.as_array()[21],\n     self.as_array()[22] | rhs.as_array()[22],\n     self.as_array()[23] | rhs.as_array()[23],\n     self.as_array()[24] | rhs.as_array()[24],\n     self.as_array()[25] | rhs.as_array()[25],\n     self.as_array()[26] | rhs.as_array()[26],\n     self.as_array()[27] | rhs.as_array()[27],\n     self.as_array()[28] | rhs.as_array()[28],\n     self.as_array()[29] | rhs.as_array()[29],\n     self.as_array()[30] | rhs.as_array()[30],\n     self.as_array()[31] | rhs.as_array()[31],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_or_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_or_si256)\n\n\n * `VPOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], self.as_array()[8] | rhs.as_array()[8], self.as_array()[9] | rhs.as_array()[9], self.as_array()[10] | rhs.as_array()[10], self.as_array()[11] | rhs.as_array()[11], self.as_array()[12] | rhs.as_array()[12], self.as_array()[13] | rhs.as_array()[13], self.as_array()[14] | rhs.as_array()[14], self.as_array()[15] | rhs.as_array()[15], self.as_array()[16] | rhs.as_array()[16], self.as_array()[17] | rhs.as_array()[17], self.as_array()[18] | rhs.as_array()[18], self.as_array()[19] | rhs.as_array()[19], self.as_array()[20] | rhs.as_array()[20], self.as_array()[21] | rhs.as_array()[21], self.as_array()[22] | rhs.as_array()[22], self.as_array()[23] | rhs.as_array()[23], self.as_array()[24] | rhs.as_array()[24], self.as_array()[25] | rhs.as_array()[25], self.as_array()[26] | rhs.as_array()[26], self.as_array()[27] | rhs.as_array()[27], self.as_array()[28] | rhs.as_array()[28], self.as_array()[29] | rhs.as_array()[29], self.as_array()[30] | rhs.as_array()[30], self.as_array()[31] | rhs.as_array()[31], ]) } avx2 { Self( avx2::_mm256_or_si256 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for U8x32 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n     self.as_array()[8] & rhs.as_array()[8],\n     self.as_array()[9] & rhs.as_array()[9],\n     self.as_array()[10] & rhs.as_array()[10],\n     self.as_array()[11] & rhs.as_array()[11],\n     self.as_array()[12] & rhs.as_array()[12],\n     self.as_array()[13] & rhs.as_array()[13],\n     self.as_array()[14] & rhs.as_array()[14],\n     self.as_array()[15] & rhs.as_array()[15],\n     self.as_array()[16] & rhs.as_array()[16],\n     self.as_array()[17] & rhs.as_array()[17],\n     self.as_array()[18] & rhs.as_array()[18],\n     self.as_array()[19] & rhs.as_array()[19],\n     self.as_array()[20] & rhs.as_array()[20],\n     self.as_array()[21] & rhs.as_array()[21],\n     self.as_array()[22] & rhs.as_array()[22],\n     self.as_array()[23] & rhs.as_array()[23],\n     self.as_array()[24] & rhs.as_array()[24],\n     self.as_array()[25] & rhs.as_array()[25],\n     self.as_array()[26] & rhs.as_array()[26],\n     self.as_array()[27] & rhs.as_array()[27],\n     self.as_array()[28] & rhs.as_array()[28],\n     self.as_array()[29] & rhs.as_array()[29],\n     self.as_array()[30] & rhs.as_array()[30],\n     self.as_array()[31] & rhs.as_array()[31],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_and_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_and_si256)\n\n\n * `VPAND ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], self.as_array()[8] & rhs.as_array()[8], self.as_array()[9] & rhs.as_array()[9], self.as_array()[10] & rhs.as_array()[10], self.as_array()[11] & rhs.as_array()[11], self.as_array()[12] & rhs.as_array()[12], self.as_array()[13] & rhs.as_array()[13], self.as_array()[14] & rhs.as_array()[14], self.as_array()[15] & rhs.as_array()[15], self.as_array()[16] & rhs.as_array()[16], self.as_array()[17] & rhs.as_array()[17], self.as_array()[18] & rhs.as_array()[18], self.as_array()[19] & rhs.as_array()[19], self.as_array()[20] & rhs.as_array()[20], self.as_array()[21] & rhs.as_array()[21], self.as_array()[22] & rhs.as_array()[22], self.as_array()[23] & rhs.as_array()[23], self.as_array()[24] & rhs.as_array()[24], self.as_array()[25] & rhs.as_array()[25], self.as_array()[26] & rhs.as_array()[26], self.as_array()[27] & rhs.as_array()[27], self.as_array()[28] & rhs.as_array()[28], self.as_array()[29] & rhs.as_array()[29], self.as_array()[30] & rhs.as_array()[30], self.as_array()[31] & rhs.as_array()[31], ]) } avx2 { Self( avx2::_mm256_and_si256 (self.0, rhs.0)) } }
    }
}
impl AddAssign for U8x32 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_add(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_add(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_add(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_add(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_add(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_add(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_add(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_add(rhs.as_array()[15]),\n     self.as_array()[16].wrapping_add(rhs.as_array()[16]),\n     self.as_array()[17].wrapping_add(rhs.as_array()[17]),\n     self.as_array()[18].wrapping_add(rhs.as_array()[18]),\n     self.as_array()[19].wrapping_add(rhs.as_array()[19]),\n     self.as_array()[20].wrapping_add(rhs.as_array()[20]),\n     self.as_array()[21].wrapping_add(rhs.as_array()[21]),\n     self.as_array()[22].wrapping_add(rhs.as_array()[22]),\n     self.as_array()[23].wrapping_add(rhs.as_array()[23]),\n     self.as_array()[24].wrapping_add(rhs.as_array()[24]),\n     self.as_array()[25].wrapping_add(rhs.as_array()[25]),\n     self.as_array()[26].wrapping_add(rhs.as_array()[26]),\n     self.as_array()[27].wrapping_add(rhs.as_array()[27]),\n     self.as_array()[28].wrapping_add(rhs.as_array()[28]),\n     self.as_array()[29].wrapping_add(rhs.as_array()[29]),\n     self.as_array()[30].wrapping_add(rhs.as_array()[30]),\n     self.as_array()[31].wrapping_add(rhs.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_add_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_add_epi8)\n\n\n * `VPADDB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), self.as_array()[8].wrapping_add(rhs.as_array()[8]), self.as_array()[9].wrapping_add(rhs.as_array()[9]), self.as_array()[10].wrapping_add(rhs.as_array()[10]), self.as_array()[11].wrapping_add(rhs.as_array()[11]), self.as_array()[12].wrapping_add(rhs.as_array()[12]), self.as_array()[13].wrapping_add(rhs.as_array()[13]), self.as_array()[14].wrapping_add(rhs.as_array()[14]), self.as_array()[15].wrapping_add(rhs.as_array()[15]), self.as_array()[16].wrapping_add(rhs.as_array()[16]), self.as_array()[17].wrapping_add(rhs.as_array()[17]), self.as_array()[18].wrapping_add(rhs.as_array()[18]), self.as_array()[19].wrapping_add(rhs.as_array()[19]), self.as_array()[20].wrapping_add(rhs.as_array()[20]), self.as_array()[21].wrapping_add(rhs.as_array()[21]), self.as_array()[22].wrapping_add(rhs.as_array()[22]), self.as_array()[23].wrapping_add(rhs.as_array()[23]), self.as_array()[24].wrapping_add(rhs.as_array()[24]), self.as_array()[25].wrapping_add(rhs.as_array()[25]), self.as_array()[26].wrapping_add(rhs.as_array()[26]), self.as_array()[27].wrapping_add(rhs.as_array()[27]), self.as_array()[28].wrapping_add(rhs.as_array()[28]), self.as_array()[29].wrapping_add(rhs.as_array()[29]), self.as_array()[30].wrapping_add(rhs.as_array()[30]), self.as_array()[31].wrapping_add(rhs.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_add_epi8 (self.0, rhs.0)) } }
    }
}
impl SubAssign for U8x32 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_sub(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_sub(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_sub(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_sub(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_sub(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_sub(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_sub(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_sub(rhs.as_array()[15]),\n     self.as_array()[16].wrapping_sub(rhs.as_array()[16]),\n     self.as_array()[17].wrapping_sub(rhs.as_array()[17]),\n     self.as_array()[18].wrapping_sub(rhs.as_array()[18]),\n     self.as_array()[19].wrapping_sub(rhs.as_array()[19]),\n     self.as_array()[20].wrapping_sub(rhs.as_array()[20]),\n     self.as_array()[21].wrapping_sub(rhs.as_array()[21]),\n     self.as_array()[22].wrapping_sub(rhs.as_array()[22]),\n     self.as_array()[23].wrapping_sub(rhs.as_array()[23]),\n     self.as_array()[24].wrapping_sub(rhs.as_array()[24]),\n     self.as_array()[25].wrapping_sub(rhs.as_array()[25]),\n     self.as_array()[26].wrapping_sub(rhs.as_array()[26]),\n     self.as_array()[27].wrapping_sub(rhs.as_array()[27]),\n     self.as_array()[28].wrapping_sub(rhs.as_array()[28]),\n     self.as_array()[29].wrapping_sub(rhs.as_array()[29]),\n     self.as_array()[30].wrapping_sub(rhs.as_array()[30]),\n     self.as_array()[31].wrapping_sub(rhs.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sub_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sub_epi8)\n\n\n * `VPSUBB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), self.as_array()[8].wrapping_sub(rhs.as_array()[8]), self.as_array()[9].wrapping_sub(rhs.as_array()[9]), self.as_array()[10].wrapping_sub(rhs.as_array()[10]), self.as_array()[11].wrapping_sub(rhs.as_array()[11]), self.as_array()[12].wrapping_sub(rhs.as_array()[12]), self.as_array()[13].wrapping_sub(rhs.as_array()[13]), self.as_array()[14].wrapping_sub(rhs.as_array()[14]), self.as_array()[15].wrapping_sub(rhs.as_array()[15]), self.as_array()[16].wrapping_sub(rhs.as_array()[16]), self.as_array()[17].wrapping_sub(rhs.as_array()[17]), self.as_array()[18].wrapping_sub(rhs.as_array()[18]), self.as_array()[19].wrapping_sub(rhs.as_array()[19]), self.as_array()[20].wrapping_sub(rhs.as_array()[20]), self.as_array()[21].wrapping_sub(rhs.as_array()[21]), self.as_array()[22].wrapping_sub(rhs.as_array()[22]), self.as_array()[23].wrapping_sub(rhs.as_array()[23]), self.as_array()[24].wrapping_sub(rhs.as_array()[24]), self.as_array()[25].wrapping_sub(rhs.as_array()[25]), self.as_array()[26].wrapping_sub(rhs.as_array()[26]), self.as_array()[27].wrapping_sub(rhs.as_array()[27]), self.as_array()[28].wrapping_sub(rhs.as_array()[28]), self.as_array()[29].wrapping_sub(rhs.as_array()[29]), self.as_array()[30].wrapping_sub(rhs.as_array()[30]), self.as_array()[31].wrapping_sub(rhs.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_sub_epi8 (self.0, rhs.0)) } }
    }
}
impl U8x32 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: U8x32 =\n     U8x32::from_array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as u8, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [u8; 32]) -> U8x32 {
        select_impl_block! { scalar { U8x32(array) } avx2 { U8x32(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[u8; 32]> for U8x32 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_loadu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_loadu_si256)\n\n\n * `VMOVDQU ymm, m256`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [u8; 32]) -> U8x32 {
        select_impl_block! { scalar { U8x32(array) } avx2 { U8x32(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_loadu_si256 (array.as_ptr() as *const ::std::arch::x86_64::__m256i) }) } }
    }
}
impl From<U8x32> for [u8; 32] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_storeu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_storeu_si256)\n\n\n * `VMOVDQU m256, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U8x32) -> [u8; 32] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [u8; 32] = [0; 32]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_storeu_si256 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m256i, vector.0) } out } }
    }
}
impl From<I8x32> for U8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x32\nas little endian bits of U8x32."]
    #[inline(always)]
    fn from(x: I8x32) -> U8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x16> for U8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x16\nas little endian bits of U8x32."]
    #[inline(always)]
    fn from(x: I16x16) -> U8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x8> for U8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x8\nas little endian bits of U8x32."]
    #[inline(always)]
    fn from(x: I32x8) -> U8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x4> for U8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x4\nas little endian bits of U8x32."]
    #[inline(always)]
    fn from(x: I64x4) -> U8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x16> for U8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x16\nas little endian bits of U8x32."]
    #[inline(always)]
    fn from(x: U16x16) -> U8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x8> for U8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x8\nas little endian bits of U8x32."]
    #[inline(always)]
    fn from(x: U32x8) -> U8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x4> for U8x32 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x4\nas little endian bits of U8x32."]
    #[inline(always)]
    fn from(x: U64x4) -> U8x32 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for U8x32 {
    #[doc = " NOTE: this will _zero_ the upper bits of the destination. Other\n intrinsics are more effcient, but leave the upper bits undefined.\n At present, these more effcient intrinsics are not exposed.\n\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U8x32\n # {\n let mut out = [0; 32];\n out[0..16].copy_from_slice(&vector.as_array());\n U8x32::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_zextsi128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_zextsi128_si256)\n\n\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U8x16) -> U8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; out[0..16].copy_from_slice(&vector.as_array()); U8x32::from(out) } avx2 { Self( avx2::_mm256_zextsi128_si256 (vector.0)) } }
    }
}
impl From<[U8x16; 2]> for U8x32 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vectors  : [U8x16; 2]  ,\n # )  -> U8x32\n # {\n let mut out = [0; 32];\n out[0..16].copy_from_slice(&vectors[0].as_array());\n out[16..].copy_from_slice(&vectors[1].as_array());\n U8x32::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_m128i`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_m128i)\n\n\n * `VINSERTF128 ymm, ymm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vectors: [U8x16; 2]) -> U8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; out[0..16].copy_from_slice(&vectors[0].as_array()); out[16..].copy_from_slice(&vectors[1].as_array()); U8x32::from(out) } avx2 { Self( avx2::_mm256_set_m128i (vectors[1].0, vectors[0].0)) } }
    }
} // TODO: this doesn't show up in the docs.
impl From<U8x32> for [U8x16; 2] {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x32  ,\n # )  -> [U8x16; 2]\n # {\n let mut lo = [0; 16];\n let mut hi = [0; 16];\n lo.copy_from_slice(&vector.as_array()[0..16]);\n hi.copy_from_slice(&vector.as_array()[16..]);\n [U8x16::from(lo), U8x16::from(hi)]\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extracti128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extracti128_si256)\n\n\n * `VEXTRACTI128 xmm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U8x32) -> [U8x16; 2] {
        select_impl_block! { scalar { let mut lo = [0; 16]; let mut hi = [0; 16]; lo.copy_from_slice(&vector.as_array()[0..16]); hi.copy_from_slice(&vector.as_array()[16..]); [U8x16::from(lo), U8x16::from(hi)] } avx2 { [ U8x16( avx2::_mm256_extracti128_si256 ::<0>(vector.0)), U8x16( avx2::_mm256_extracti128_si256 ::<1>(vector.0)), ] } }
    }
}
impl crate::SimdSaturatingArithmetic for U8x32 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_add(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_adds_epu8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_adds_epu8)\n\n\n * `VPADDUSB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_add(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_add(*src); } Self::from(out) } avx2 { Self( avx2::_mm256_adds_epu8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_sub(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_subs_epu8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_subs_epu8)\n\n\n * `VPSUBUSB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_sub(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_sub(*src); } Self::from(out) } avx2 { Self( avx2::_mm256_subs_epu8 (self.0, other.0)) } }
    }
} // Static shift
impl ShlAssign<u64> for U8x32 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U8x32\n # {\n if amount >= 8 {\n     U8x32::ZERO\n } else {\n     U8x32::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n         self.as_array()[8] << amount,\n         self.as_array()[9] << amount,\n         self.as_array()[10] << amount,\n         self.as_array()[11] << amount,\n         self.as_array()[12] << amount,\n         self.as_array()[13] << amount,\n         self.as_array()[14] << amount,\n         self.as_array()[15] << amount,\n         self.as_array()[16] << amount,\n         self.as_array()[17] << amount,\n         self.as_array()[18] << amount,\n         self.as_array()[19] << amount,\n         self.as_array()[20] << amount,\n         self.as_array()[21] << amount,\n         self.as_array()[22] << amount,\n         self.as_array()[23] << amount,\n         self.as_array()[24] << amount,\n         self.as_array()[25] << amount,\n         self.as_array()[26] << amount,\n         self.as_array()[27] << amount,\n         self.as_array()[28] << amount,\n         self.as_array()[29] << amount,\n         self.as_array()[30] << amount,\n         self.as_array()[31] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: u64) -> U8x32 {
        select_impl_block! { scalar { if amount >= 8 { U8x32::ZERO } else { U8x32::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, self.as_array()[16] << amount, self.as_array()[17] << amount, self.as_array()[18] << amount, self.as_array()[19] << amount, self.as_array()[20] << amount, self.as_array()[21] << amount, self.as_array()[22] << amount, self.as_array()[23] << amount, self.as_array()[24] << amount, self.as_array()[25] << amount, self.as_array()[26] << amount, self.as_array()[27] << amount, self.as_array()[28] << amount, self.as_array()[29] << amount, self.as_array()[30] << amount, self.as_array()[31] << amount, ]) } } avx2 { if amount >= 8 { U8x32::ZERO } else { U8x32::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, self.as_array()[16] << amount, self.as_array()[17] << amount, self.as_array()[18] << amount, self.as_array()[19] << amount, self.as_array()[20] << amount, self.as_array()[21] << amount, self.as_array()[22] << amount, self.as_array()[23] << amount, self.as_array()[24] << amount, self.as_array()[25] << amount, self.as_array()[26] << amount, self.as_array()[27] << amount, self.as_array()[28] << amount, self.as_array()[29] << amount, self.as_array()[30] << amount, self.as_array()[31] << amount, ]) } } }
    }
} // Variable shift
impl ShlAssign<U8x32> for U8x32 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: U8x32) {
        *self = (*self) << amount;
    }
}
impl Shl<U8x32> for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U8x32  ,\n # )  -> U8x32\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..8).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n U8x32::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: U8x32) -> U8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x << amm } else { 0 }; } U8x32::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x << amm } else { 0 }; } U8x32::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for U8x32 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U8x32\n # {\n if amount >= 8 {\n     U8x32::ZERO\n } else {\n     U8x32::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n         self.as_array()[8] >> amount,\n         self.as_array()[9] >> amount,\n         self.as_array()[10] >> amount,\n         self.as_array()[11] >> amount,\n         self.as_array()[12] >> amount,\n         self.as_array()[13] >> amount,\n         self.as_array()[14] >> amount,\n         self.as_array()[15] >> amount,\n         self.as_array()[16] >> amount,\n         self.as_array()[17] >> amount,\n         self.as_array()[18] >> amount,\n         self.as_array()[19] >> amount,\n         self.as_array()[20] >> amount,\n         self.as_array()[21] >> amount,\n         self.as_array()[22] >> amount,\n         self.as_array()[23] >> amount,\n         self.as_array()[24] >> amount,\n         self.as_array()[25] >> amount,\n         self.as_array()[26] >> amount,\n         self.as_array()[27] >> amount,\n         self.as_array()[28] >> amount,\n         self.as_array()[29] >> amount,\n         self.as_array()[30] >> amount,\n         self.as_array()[31] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: u64) -> U8x32 {
        select_impl_block! { scalar { if amount >= 8 { U8x32::ZERO } else { U8x32::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, self.as_array()[16] >> amount, self.as_array()[17] >> amount, self.as_array()[18] >> amount, self.as_array()[19] >> amount, self.as_array()[20] >> amount, self.as_array()[21] >> amount, self.as_array()[22] >> amount, self.as_array()[23] >> amount, self.as_array()[24] >> amount, self.as_array()[25] >> amount, self.as_array()[26] >> amount, self.as_array()[27] >> amount, self.as_array()[28] >> amount, self.as_array()[29] >> amount, self.as_array()[30] >> amount, self.as_array()[31] >> amount, ]) } } avx2 { if amount >= 8 { U8x32::ZERO } else { U8x32::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, self.as_array()[16] >> amount, self.as_array()[17] >> amount, self.as_array()[18] >> amount, self.as_array()[19] >> amount, self.as_array()[20] >> amount, self.as_array()[21] >> amount, self.as_array()[22] >> amount, self.as_array()[23] >> amount, self.as_array()[24] >> amount, self.as_array()[25] >> amount, self.as_array()[26] >> amount, self.as_array()[27] >> amount, self.as_array()[28] >> amount, self.as_array()[29] >> amount, self.as_array()[30] >> amount, self.as_array()[31] >> amount, ]) } } }
    }
} // Variable shift
impl ShrAssign<U8x32> for U8x32 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: U8x32) {
        *self = (*self) >> amount;
    }
}
impl Shr<U8x32> for U8x32 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U8x32  ,\n # )  -> U8x32\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..8).contains(&amm) {\n         *x >> amm\n     }  else {\n         0\n     };\n }\n U8x32::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: U8x32) -> U8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x >> amm } else { 0 }; } U8x32::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..8).contains(&amm) { *x >> amm } else { 0 }; } U8x32::from(out) } }
    }
}
impl SimdBase for U8x32 {
    type Scalar = u8;
    type Array = [u8; 32];
    type Signed = I8x32;
    type Unsigned = U8x32;
    const LANES: usize = 32;
    const ZERO: Self = Self::from_array([0; 32]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_testz_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_testz_si256)\n\n\n * `VPTEST ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm256_testz_si256 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u8  ,\n # )  -> U8x32\n # {\n let mut out = [0; 32];\n out[0] = scalar;\n U8x32::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: u8) -> U8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; out[0] = scalar; U8x32::from(out) } avx2 { Self( avx2::_mm256_set_epi8 ( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, scalar as i8, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u8\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u8\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extract_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extract_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> u8 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm256_extract_epi8 ::<I>(self.0) as u8 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u8  ,\n # )  -> U8x32\n # {\n U8x32::from([scalar; 32])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set1_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi8)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: u8) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([scalar; 32]) } avx2 { Self( avx2::_mm256_set1_epi8 (scalar as i8)) } }
    }
    type BroadcastLoInput = U8x16;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U8x32\n # {\n U8x32::from([vector.as_array()[0]; 32])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_broadcastb_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_broadcastb_epi8)\n\n\n * `VPBROADCASTB ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: U8x16) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([vector.as_array()[0]; 32]) } avx2 { Self( avx2::_mm256_broadcastb_epi8 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     if self.as_array()[0] == other.as_array()[0] {  u8::MAX  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  u8::MAX  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  u8::MAX  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  u8::MAX  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  u8::MAX  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  u8::MAX  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  u8::MAX  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  u8::MAX  } else { 0 },\n     if self.as_array()[8] == other.as_array()[8] {  u8::MAX  } else { 0 },\n     if self.as_array()[9] == other.as_array()[9] {  u8::MAX  } else { 0 },\n     if self.as_array()[10] == other.as_array()[10] {  u8::MAX  } else { 0 },\n     if self.as_array()[11] == other.as_array()[11] {  u8::MAX  } else { 0 },\n     if self.as_array()[12] == other.as_array()[12] {  u8::MAX  } else { 0 },\n     if self.as_array()[13] == other.as_array()[13] {  u8::MAX  } else { 0 },\n     if self.as_array()[14] == other.as_array()[14] {  u8::MAX  } else { 0 },\n     if self.as_array()[15] == other.as_array()[15] {  u8::MAX  } else { 0 },\n     if self.as_array()[16] == other.as_array()[16] {  u8::MAX  } else { 0 },\n     if self.as_array()[17] == other.as_array()[17] {  u8::MAX  } else { 0 },\n     if self.as_array()[18] == other.as_array()[18] {  u8::MAX  } else { 0 },\n     if self.as_array()[19] == other.as_array()[19] {  u8::MAX  } else { 0 },\n     if self.as_array()[20] == other.as_array()[20] {  u8::MAX  } else { 0 },\n     if self.as_array()[21] == other.as_array()[21] {  u8::MAX  } else { 0 },\n     if self.as_array()[22] == other.as_array()[22] {  u8::MAX  } else { 0 },\n     if self.as_array()[23] == other.as_array()[23] {  u8::MAX  } else { 0 },\n     if self.as_array()[24] == other.as_array()[24] {  u8::MAX  } else { 0 },\n     if self.as_array()[25] == other.as_array()[25] {  u8::MAX  } else { 0 },\n     if self.as_array()[26] == other.as_array()[26] {  u8::MAX  } else { 0 },\n     if self.as_array()[27] == other.as_array()[27] {  u8::MAX  } else { 0 },\n     if self.as_array()[28] == other.as_array()[28] {  u8::MAX  } else { 0 },\n     if self.as_array()[29] == other.as_array()[29] {  u8::MAX  } else { 0 },\n     if self.as_array()[30] == other.as_array()[30] {  u8::MAX  } else { 0 },\n     if self.as_array()[31] == other.as_array()[31] {  u8::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpeq_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpeq_epi8)\n\n\n * `VPCMPEQB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ if self.as_array()[0] == other.as_array()[0] { u8::MAX } else { 0 }, if self.as_array()[1] == other.as_array()[1] { u8::MAX } else { 0 }, if self.as_array()[2] == other.as_array()[2] { u8::MAX } else { 0 }, if self.as_array()[3] == other.as_array()[3] { u8::MAX } else { 0 }, if self.as_array()[4] == other.as_array()[4] { u8::MAX } else { 0 }, if self.as_array()[5] == other.as_array()[5] { u8::MAX } else { 0 }, if self.as_array()[6] == other.as_array()[6] { u8::MAX } else { 0 }, if self.as_array()[7] == other.as_array()[7] { u8::MAX } else { 0 }, if self.as_array()[8] == other.as_array()[8] { u8::MAX } else { 0 }, if self.as_array()[9] == other.as_array()[9] { u8::MAX } else { 0 }, if self.as_array()[10] == other.as_array()[10] { u8::MAX } else { 0 }, if self.as_array()[11] == other.as_array()[11] { u8::MAX } else { 0 }, if self.as_array()[12] == other.as_array()[12] { u8::MAX } else { 0 }, if self.as_array()[13] == other.as_array()[13] { u8::MAX } else { 0 }, if self.as_array()[14] == other.as_array()[14] { u8::MAX } else { 0 }, if self.as_array()[15] == other.as_array()[15] { u8::MAX } else { 0 }, if self.as_array()[16] == other.as_array()[16] { u8::MAX } else { 0 }, if self.as_array()[17] == other.as_array()[17] { u8::MAX } else { 0 }, if self.as_array()[18] == other.as_array()[18] { u8::MAX } else { 0 }, if self.as_array()[19] == other.as_array()[19] { u8::MAX } else { 0 }, if self.as_array()[20] == other.as_array()[20] { u8::MAX } else { 0 }, if self.as_array()[21] == other.as_array()[21] { u8::MAX } else { 0 }, if self.as_array()[22] == other.as_array()[22] { u8::MAX } else { 0 }, if self.as_array()[23] == other.as_array()[23] { u8::MAX } else { 0 }, if self.as_array()[24] == other.as_array()[24] { u8::MAX } else { 0 }, if self.as_array()[25] == other.as_array()[25] { u8::MAX } else { 0 }, if self.as_array()[26] == other.as_array()[26] { u8::MAX } else { 0 }, if self.as_array()[27] == other.as_array()[27] { u8::MAX } else { 0 }, if self.as_array()[28] == other.as_array()[28] { u8::MAX } else { 0 }, if self.as_array()[29] == other.as_array()[29] { u8::MAX } else { 0 }, if self.as_array()[30] == other.as_array()[30] { u8::MAX } else { 0 }, if self.as_array()[31] == other.as_array()[31] { u8::MAX } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpeq_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n     self.as_array()[8] & (!other.as_array()[8]),\n     self.as_array()[9] & (!other.as_array()[9]),\n     self.as_array()[10] & (!other.as_array()[10]),\n     self.as_array()[11] & (!other.as_array()[11]),\n     self.as_array()[12] & (!other.as_array()[12]),\n     self.as_array()[13] & (!other.as_array()[13]),\n     self.as_array()[14] & (!other.as_array()[14]),\n     self.as_array()[15] & (!other.as_array()[15]),\n     self.as_array()[16] & (!other.as_array()[16]),\n     self.as_array()[17] & (!other.as_array()[17]),\n     self.as_array()[18] & (!other.as_array()[18]),\n     self.as_array()[19] & (!other.as_array()[19]),\n     self.as_array()[20] & (!other.as_array()[20]),\n     self.as_array()[21] & (!other.as_array()[21]),\n     self.as_array()[22] & (!other.as_array()[22]),\n     self.as_array()[23] & (!other.as_array()[23]),\n     self.as_array()[24] & (!other.as_array()[24]),\n     self.as_array()[25] & (!other.as_array()[25]),\n     self.as_array()[26] & (!other.as_array()[26]),\n     self.as_array()[27] & (!other.as_array()[27]),\n     self.as_array()[28] & (!other.as_array()[28]),\n     self.as_array()[29] & (!other.as_array()[29]),\n     self.as_array()[30] & (!other.as_array()[30]),\n     self.as_array()[31] & (!other.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_andnot_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_andnot_si256)\n\n\n * `VPANDN ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), self.as_array()[8] & (!other.as_array()[8]), self.as_array()[9] & (!other.as_array()[9]), self.as_array()[10] & (!other.as_array()[10]), self.as_array()[11] & (!other.as_array()[11]), self.as_array()[12] & (!other.as_array()[12]), self.as_array()[13] & (!other.as_array()[13]), self.as_array()[14] & (!other.as_array()[14]), self.as_array()[15] & (!other.as_array()[15]), self.as_array()[16] & (!other.as_array()[16]), self.as_array()[17] & (!other.as_array()[17]), self.as_array()[18] & (!other.as_array()[18]), self.as_array()[19] & (!other.as_array()[19]), self.as_array()[20] & (!other.as_array()[20]), self.as_array()[21] & (!other.as_array()[21]), self.as_array()[22] & (!other.as_array()[22]), self.as_array()[23] & (!other.as_array()[23]), self.as_array()[24] & (!other.as_array()[24]), self.as_array()[25] & (!other.as_array()[25]), self.as_array()[26] & (!other.as_array()[26]), self.as_array()[27] & (!other.as_array()[27]), self.as_array()[28] & (!other.as_array()[28]), self.as_array()[29] & (!other.as_array()[29]), self.as_array()[30] & (!other.as_array()[30]), self.as_array()[31] & (!other.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_andnot_si256 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     if self.as_array()[0] > other.as_array()[0] {  u8::MAX  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  u8::MAX  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  u8::MAX  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  u8::MAX  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  u8::MAX  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  u8::MAX  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  u8::MAX  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  u8::MAX  } else { 0 },\n     if self.as_array()[8] > other.as_array()[8] {  u8::MAX  } else { 0 },\n     if self.as_array()[9] > other.as_array()[9] {  u8::MAX  } else { 0 },\n     if self.as_array()[10] > other.as_array()[10] {  u8::MAX  } else { 0 },\n     if self.as_array()[11] > other.as_array()[11] {  u8::MAX  } else { 0 },\n     if self.as_array()[12] > other.as_array()[12] {  u8::MAX  } else { 0 },\n     if self.as_array()[13] > other.as_array()[13] {  u8::MAX  } else { 0 },\n     if self.as_array()[14] > other.as_array()[14] {  u8::MAX  } else { 0 },\n     if self.as_array()[15] > other.as_array()[15] {  u8::MAX  } else { 0 },\n     if self.as_array()[16] > other.as_array()[16] {  u8::MAX  } else { 0 },\n     if self.as_array()[17] > other.as_array()[17] {  u8::MAX  } else { 0 },\n     if self.as_array()[18] > other.as_array()[18] {  u8::MAX  } else { 0 },\n     if self.as_array()[19] > other.as_array()[19] {  u8::MAX  } else { 0 },\n     if self.as_array()[20] > other.as_array()[20] {  u8::MAX  } else { 0 },\n     if self.as_array()[21] > other.as_array()[21] {  u8::MAX  } else { 0 },\n     if self.as_array()[22] > other.as_array()[22] {  u8::MAX  } else { 0 },\n     if self.as_array()[23] > other.as_array()[23] {  u8::MAX  } else { 0 },\n     if self.as_array()[24] > other.as_array()[24] {  u8::MAX  } else { 0 },\n     if self.as_array()[25] > other.as_array()[25] {  u8::MAX  } else { 0 },\n     if self.as_array()[26] > other.as_array()[26] {  u8::MAX  } else { 0 },\n     if self.as_array()[27] > other.as_array()[27] {  u8::MAX  } else { 0 },\n     if self.as_array()[28] > other.as_array()[28] {  u8::MAX  } else { 0 },\n     if self.as_array()[29] > other.as_array()[29] {  u8::MAX  } else { 0 },\n     if self.as_array()[30] > other.as_array()[30] {  u8::MAX  } else { 0 },\n     if self.as_array()[31] > other.as_array()[31] {  u8::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n **NOTE:** this implementation uses an efficient vector polyfill, though this operation is not natively supported.\n ```ignore\n // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK\n let sign_bit = Self::broadcast(1 << 7);\n Self::from(I8x32::from(*self ^ sign_bit).cmp_gt(\n     I8x32::from(other ^ sign_bit)\n ))\n ```"]
    #[inline(always)]
    fn cmp_gt(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ if self.as_array()[0] > other.as_array()[0] { u8::MAX } else { 0 }, if self.as_array()[1] > other.as_array()[1] { u8::MAX } else { 0 }, if self.as_array()[2] > other.as_array()[2] { u8::MAX } else { 0 }, if self.as_array()[3] > other.as_array()[3] { u8::MAX } else { 0 }, if self.as_array()[4] > other.as_array()[4] { u8::MAX } else { 0 }, if self.as_array()[5] > other.as_array()[5] { u8::MAX } else { 0 }, if self.as_array()[6] > other.as_array()[6] { u8::MAX } else { 0 }, if self.as_array()[7] > other.as_array()[7] { u8::MAX } else { 0 }, if self.as_array()[8] > other.as_array()[8] { u8::MAX } else { 0 }, if self.as_array()[9] > other.as_array()[9] { u8::MAX } else { 0 }, if self.as_array()[10] > other.as_array()[10] { u8::MAX } else { 0 }, if self.as_array()[11] > other.as_array()[11] { u8::MAX } else { 0 }, if self.as_array()[12] > other.as_array()[12] { u8::MAX } else { 0 }, if self.as_array()[13] > other.as_array()[13] { u8::MAX } else { 0 }, if self.as_array()[14] > other.as_array()[14] { u8::MAX } else { 0 }, if self.as_array()[15] > other.as_array()[15] { u8::MAX } else { 0 }, if self.as_array()[16] > other.as_array()[16] { u8::MAX } else { 0 }, if self.as_array()[17] > other.as_array()[17] { u8::MAX } else { 0 }, if self.as_array()[18] > other.as_array()[18] { u8::MAX } else { 0 }, if self.as_array()[19] > other.as_array()[19] { u8::MAX } else { 0 }, if self.as_array()[20] > other.as_array()[20] { u8::MAX } else { 0 }, if self.as_array()[21] > other.as_array()[21] { u8::MAX } else { 0 }, if self.as_array()[22] > other.as_array()[22] { u8::MAX } else { 0 }, if self.as_array()[23] > other.as_array()[23] { u8::MAX } else { 0 }, if self.as_array()[24] > other.as_array()[24] { u8::MAX } else { 0 }, if self.as_array()[25] > other.as_array()[25] { u8::MAX } else { 0 }, if self.as_array()[26] > other.as_array()[26] { u8::MAX } else { 0 }, if self.as_array()[27] > other.as_array()[27] { u8::MAX } else { 0 }, if self.as_array()[28] > other.as_array()[28] { u8::MAX } else { 0 }, if self.as_array()[29] > other.as_array()[29] { u8::MAX } else { 0 }, if self.as_array()[30] > other.as_array()[30] { u8::MAX } else { 0 }, if self.as_array()[31] > other.as_array()[31] { u8::MAX } else { 0 }, ]) } avx2 { // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK
        let sign_bit = Self::broadcast(1 << 7); Self::from(I8x32::from(*self ^ sign_bit).cmp_gt( I8x32::from(other ^ sign_bit) )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x32\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n U8x32::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> U8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U8x32::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U8x32::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x32\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n U8x32::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> U8x32 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U8x32::from(out) } avx2 { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U8x32::from(out) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n     // Lane# 1\n     self.as_array()[16],\n     other.as_array()[16],\n     self.as_array()[17],\n     other.as_array()[17],\n     self.as_array()[18],\n     other.as_array()[18],\n     self.as_array()[19],\n     other.as_array()[19],\n     self.as_array()[20],\n     other.as_array()[20],\n     self.as_array()[21],\n     other.as_array()[21],\n     self.as_array()[22],\n     other.as_array()[22],\n     self.as_array()[23],\n     other.as_array()[23],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpacklo_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpacklo_epi8)\n\n\n * `VPUNPCKLBW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], // Lane# 1
        self.as_array()[16], other.as_array()[16], self.as_array()[17], other.as_array()[17], self.as_array()[18], other.as_array()[18], self.as_array()[19], other.as_array()[19], self.as_array()[20], other.as_array()[20], self.as_array()[21], other.as_array()[21], self.as_array()[22], other.as_array()[22], self.as_array()[23], other.as_array()[23], ]) } avx2 { Self( avx2::_mm256_unpacklo_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     // Lane# 0\n     self.as_array()[8],\n     other.as_array()[8],\n     self.as_array()[9],\n     other.as_array()[9],\n     self.as_array()[10],\n     other.as_array()[10],\n     self.as_array()[11],\n     other.as_array()[11],\n     self.as_array()[12],\n     other.as_array()[12],\n     self.as_array()[13],\n     other.as_array()[13],\n     self.as_array()[14],\n     other.as_array()[14],\n     self.as_array()[15],\n     other.as_array()[15],\n     // Lane# 1\n     self.as_array()[24],\n     other.as_array()[24],\n     self.as_array()[25],\n     other.as_array()[25],\n     self.as_array()[26],\n     other.as_array()[26],\n     self.as_array()[27],\n     other.as_array()[27],\n     self.as_array()[28],\n     other.as_array()[28],\n     self.as_array()[29],\n     other.as_array()[29],\n     self.as_array()[30],\n     other.as_array()[30],\n     self.as_array()[31],\n     other.as_array()[31],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpackhi_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpackhi_epi8)\n\n\n * `VPUNPCKHBW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ // Lane# 0
        self.as_array()[8], other.as_array()[8], self.as_array()[9], other.as_array()[9], self.as_array()[10], other.as_array()[10], self.as_array()[11], other.as_array()[11], self.as_array()[12], other.as_array()[12], self.as_array()[13], other.as_array()[13], self.as_array()[14], other.as_array()[14], self.as_array()[15], other.as_array()[15], // Lane# 1
        self.as_array()[24], other.as_array()[24], self.as_array()[25], other.as_array()[25], self.as_array()[26], other.as_array()[26], self.as_array()[27], other.as_array()[27], self.as_array()[28], other.as_array()[28], self.as_array()[29], other.as_array()[29], self.as_array()[30], other.as_array()[30], self.as_array()[31], other.as_array()[31], ]) } avx2 { Self( avx2::_mm256_unpackhi_epi8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n     self.as_array()[8].max(other.as_array()[8]),\n     self.as_array()[9].max(other.as_array()[9]),\n     self.as_array()[10].max(other.as_array()[10]),\n     self.as_array()[11].max(other.as_array()[11]),\n     self.as_array()[12].max(other.as_array()[12]),\n     self.as_array()[13].max(other.as_array()[13]),\n     self.as_array()[14].max(other.as_array()[14]),\n     self.as_array()[15].max(other.as_array()[15]),\n     self.as_array()[16].max(other.as_array()[16]),\n     self.as_array()[17].max(other.as_array()[17]),\n     self.as_array()[18].max(other.as_array()[18]),\n     self.as_array()[19].max(other.as_array()[19]),\n     self.as_array()[20].max(other.as_array()[20]),\n     self.as_array()[21].max(other.as_array()[21]),\n     self.as_array()[22].max(other.as_array()[22]),\n     self.as_array()[23].max(other.as_array()[23]),\n     self.as_array()[24].max(other.as_array()[24]),\n     self.as_array()[25].max(other.as_array()[25]),\n     self.as_array()[26].max(other.as_array()[26]),\n     self.as_array()[27].max(other.as_array()[27]),\n     self.as_array()[28].max(other.as_array()[28]),\n     self.as_array()[29].max(other.as_array()[29]),\n     self.as_array()[30].max(other.as_array()[30]),\n     self.as_array()[31].max(other.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_max_epu8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_max_epu8)\n\n\n * `VPMAXUB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), self.as_array()[8].max(other.as_array()[8]), self.as_array()[9].max(other.as_array()[9]), self.as_array()[10].max(other.as_array()[10]), self.as_array()[11].max(other.as_array()[11]), self.as_array()[12].max(other.as_array()[12]), self.as_array()[13].max(other.as_array()[13]), self.as_array()[14].max(other.as_array()[14]), self.as_array()[15].max(other.as_array()[15]), self.as_array()[16].max(other.as_array()[16]), self.as_array()[17].max(other.as_array()[17]), self.as_array()[18].max(other.as_array()[18]), self.as_array()[19].max(other.as_array()[19]), self.as_array()[20].max(other.as_array()[20]), self.as_array()[21].max(other.as_array()[21]), self.as_array()[22].max(other.as_array()[22]), self.as_array()[23].max(other.as_array()[23]), self.as_array()[24].max(other.as_array()[24]), self.as_array()[25].max(other.as_array()[25]), self.as_array()[26].max(other.as_array()[26]), self.as_array()[27].max(other.as_array()[27]), self.as_array()[28].max(other.as_array()[28]), self.as_array()[29].max(other.as_array()[29]), self.as_array()[30].max(other.as_array()[30]), self.as_array()[31].max(other.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_max_epu8 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U8x32  ,\n # )  -> U8x32\n # {\n U8x32::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n     self.as_array()[8].min(other.as_array()[8]),\n     self.as_array()[9].min(other.as_array()[9]),\n     self.as_array()[10].min(other.as_array()[10]),\n     self.as_array()[11].min(other.as_array()[11]),\n     self.as_array()[12].min(other.as_array()[12]),\n     self.as_array()[13].min(other.as_array()[13]),\n     self.as_array()[14].min(other.as_array()[14]),\n     self.as_array()[15].min(other.as_array()[15]),\n     self.as_array()[16].min(other.as_array()[16]),\n     self.as_array()[17].min(other.as_array()[17]),\n     self.as_array()[18].min(other.as_array()[18]),\n     self.as_array()[19].min(other.as_array()[19]),\n     self.as_array()[20].min(other.as_array()[20]),\n     self.as_array()[21].min(other.as_array()[21]),\n     self.as_array()[22].min(other.as_array()[22]),\n     self.as_array()[23].min(other.as_array()[23]),\n     self.as_array()[24].min(other.as_array()[24]),\n     self.as_array()[25].min(other.as_array()[25]),\n     self.as_array()[26].min(other.as_array()[26]),\n     self.as_array()[27].min(other.as_array()[27]),\n     self.as_array()[28].min(other.as_array()[28]),\n     self.as_array()[29].min(other.as_array()[29]),\n     self.as_array()[30].min(other.as_array()[30]),\n     self.as_array()[31].min(other.as_array()[31]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_min_epu8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_min_epu8)\n\n\n * `VPMINUB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: U8x32) -> U8x32 {
        select_impl_block! { scalar { U8x32::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), self.as_array()[8].min(other.as_array()[8]), self.as_array()[9].min(other.as_array()[9]), self.as_array()[10].min(other.as_array()[10]), self.as_array()[11].min(other.as_array()[11]), self.as_array()[12].min(other.as_array()[12]), self.as_array()[13].min(other.as_array()[13]), self.as_array()[14].min(other.as_array()[14]), self.as_array()[15].min(other.as_array()[15]), self.as_array()[16].min(other.as_array()[16]), self.as_array()[17].min(other.as_array()[17]), self.as_array()[18].min(other.as_array()[18]), self.as_array()[19].min(other.as_array()[19]), self.as_array()[20].min(other.as_array()[20]), self.as_array()[21].min(other.as_array()[21]), self.as_array()[22].min(other.as_array()[22]), self.as_array()[23].min(other.as_array()[23]), self.as_array()[24].min(other.as_array()[24]), self.as_array()[25].min(other.as_array()[25]), self.as_array()[26].min(other.as_array()[26]), self.as_array()[27].min(other.as_array()[27]), self.as_array()[28].min(other.as_array()[28]), self.as_array()[29].min(other.as_array()[29]), self.as_array()[30].min(other.as_array()[30]), self.as_array()[31].min(other.as_array()[31]), ]) } avx2 { Self( avx2::_mm256_min_epu8 (self.0, other.0)) } }
    }
}
impl crate::SimdBase8 for U8x32 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x32\n # {\n let mut out = [0; 32];\n for (out_lane, src_lane) in out\n     .chunks_exact_mut(16)\n     .zip(self.as_array().chunks_exact(16))\n {\n     out_lane[AMOUNT..].copy_from_slice(&src_lane[0..16 - AMOUNT]);\n }\n U8x32::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_slli_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_slli_si256)\n\n\n * `VPSLLDQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_bytes_left<const AMOUNT: usize>(&self) -> U8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; for (out_lane, src_lane) in out .chunks_exact_mut(16) .zip(self.as_array().chunks_exact(16)) { out_lane[AMOUNT..].copy_from_slice(&src_lane[0..16 - AMOUNT]); } U8x32::from(out) } avx2 { Self( avx2::_mm256_slli_si256 ::<AMOUNT>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n #     <\n #             const AMOUNT: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U8x32\n # {\n let mut out = [0; 32];\n for (out_lane, src_lane) in out\n     .chunks_exact_mut(16)\n     .zip(self.as_array().chunks_exact(16))\n {\n     out_lane[0..16 - AMOUNT].copy_from_slice(&src_lane[AMOUNT..]);\n }\n U8x32::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srli_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srli_si256)\n\n\n * `VPSRLDQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_bytes_right<const AMOUNT: usize>(&self) -> U8x32 {
        select_impl_block! { scalar { let mut out = [0; 32]; for (out_lane, src_lane) in out .chunks_exact_mut(16) .zip(self.as_array().chunks_exact(16)) { out_lane[0..16 - AMOUNT].copy_from_slice(&src_lane[AMOUNT..]); } U8x32::from(out) } avx2 { Self( avx2::_mm256_srli_si256 ::<AMOUNT>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> u32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> u32\n # {\n let mut out: u32 = 0;\n for (i, value) in self.as_array().iter().copied().enumerate() {\n     out |= u32::from((value as u8) >> 7) << i;\n }\n out\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_movemask_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_movemask_epi8)\n\n\n * `VPMOVMSKB r32, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn most_significant_bits(&self) -> u32 {
        select_impl_block! { scalar { let mut out: u32 = 0; for (i, value) in self.as_array().iter().copied().enumerate() { out |= u32::from((value as u8) >> 7) << i; } out } avx2 { avx2::_mm256_movemask_epi8 (self.0) as u32 } }
    }
}
impl U8x32 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         order  : U8x32  ,\n # )  -> U8x32\n # ;}\n # impl SomeTraitForDoc for U8x32 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         order  : U8x32  ,\n # )  -> U8x32\n # {\n let mut arr = [0; 32];\n for (lane_dst, (lane_src, order)) in\n     arr.chunks_exact_mut(16).zip(\n         self.as_array().chunks_exact(16)\n         .zip(order.as_array().chunks_exact(16))\n     )\n {\n     for (dst, idx) in lane_dst.iter_mut().zip(order) {\n         let idx = *idx;\n         *dst = if (idx >> 7) == 1 {\n             0\n         } else {\n             lane_src[(idx as usize) % 16]\n         };\n     }\n }\n arr.into()\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_shuffle_epi8`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shuffle_epi8)\n\n\n * `VPSHUFB ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    pub fn shuffle(&self, order: U8x32) -> U8x32 {
        select_impl_block! { scalar { let mut arr = [0; 32]; for (lane_dst, (lane_src, order)) in arr.chunks_exact_mut(16).zip( self.as_array().chunks_exact(16) .zip(order.as_array().chunks_exact(16)) ) { for (dst, idx) in lane_dst.iter_mut().zip(order) { let idx = *idx; *dst = if (idx >> 7) == 1 { 0 } else { lane_src[(idx as usize) % 16] }; } } arr.into() } avx2 { Self( avx2::_mm256_shuffle_epi8 (self.0, order.0)) } }
    }
}
select_impl! { scalar { type U16x8Internal = [u16 ; 8]; } avx2 { type U16x8Internal = ::std::arch::x86_64::__m128i; } }
#[doc = "`[u16; 8]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct U16x8(U16x8Internal);
unsafe impl bytemuck::Pod for U16x8 {}
unsafe impl bytemuck::Zeroable for U16x8 {}
impl PartialEq for U16x8 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for U16x8 {}
impl Default for U16x8 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for U16x8 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for U16x8 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "U16x8({:?})", <[u16; 8]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for U16x8 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for U16x8 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 8];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <u16 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<U16x8> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> U16x8 {
        let mut out = U16x8::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for U16x8 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[u16; 8]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for U16x8 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[u16; 8]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for U16x8 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_xor_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_si128)\n\n\n * `PXOR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm_xor_si128 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for U16x8 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_or_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_or_si128)\n\n\n * `POR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm_or_si128 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for U16x8 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_and_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_si128)\n\n\n * `PAND xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm_and_si128 (self.0, rhs.0)) } }
    }
}
impl AddAssign for U16x8 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_add_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_epi16)\n\n\n * `PADDW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), ]) } avx2 { Self( avx2::_mm_add_epi16 (self.0, rhs.0)) } }
    }
}
impl SubAssign for U16x8 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sub_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_epi16)\n\n\n * `PSUBW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), ]) } avx2 { Self( avx2::_mm_sub_epi16 (self.0, rhs.0)) } }
    }
}
impl U16x8 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: U16x8 =\n     U16x8::from_array([0, 1, 2, 3, 4, 5, 6, 7]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as u16, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [u16; 8]) -> U16x8 {
        select_impl_block! { scalar { U16x8(array) } avx2 { U16x8(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[u16; 8]> for U16x8 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_loadu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si128)\n\n\n * `MOVDQU xmm, m128`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [u16; 8]) -> U16x8 {
        select_impl_block! { scalar { U16x8(array) } avx2 { U16x8(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_loadu_si128 (array.as_ptr() as *const ::std::arch::x86_64::__m128i) }) } }
    }
}
impl From<U16x8> for [u16; 8] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_storeu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_si128)\n\n\n * `MOVDQU m128, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U16x8) -> [u16; 8] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [u16; 8] = [0; 8]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_storeu_si128 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m128i, vector.0) } out } }
    }
}
impl From<I8x16> for U16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x16\nas little endian bits of U16x8."]
    #[inline(always)]
    fn from(x: I8x16) -> U16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x8> for U16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x8\nas little endian bits of U16x8."]
    #[inline(always)]
    fn from(x: I16x8) -> U16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x4> for U16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x4\nas little endian bits of U16x8."]
    #[inline(always)]
    fn from(x: I32x4) -> U16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x2> for U16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x2\nas little endian bits of U16x8."]
    #[inline(always)]
    fn from(x: I64x2) -> U16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for U16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x16\nas little endian bits of U16x8."]
    #[inline(always)]
    fn from(x: U8x16) -> U16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x4> for U16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x4\nas little endian bits of U16x8."]
    #[inline(always)]
    fn from(x: U32x4) -> U16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x2> for U16x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x2\nas little endian bits of U16x8."]
    #[inline(always)]
    fn from(x: U64x2) -> U16x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl crate::ExtendingCast<U8x16> for U16x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U16x8\n # {\n U16x8::from([\n         u16::from(vector.as_array()[0]),\n         u16::from(vector.as_array()[1]),\n         u16::from(vector.as_array()[2]),\n         u16::from(vector.as_array()[3]),\n         u16::from(vector.as_array()[4]),\n         u16::from(vector.as_array()[5]),\n         u16::from(vector.as_array()[6]),\n         u16::from(vector.as_array()[7]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepu8_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepu8_epi16)\n\n\n * `PMOVZXBW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U8x16) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ u16::from(vector.as_array()[0]), u16::from(vector.as_array()[1]), u16::from(vector.as_array()[2]), u16::from(vector.as_array()[3]), u16::from(vector.as_array()[4]), u16::from(vector.as_array()[5]), u16::from(vector.as_array()[6]), u16::from(vector.as_array()[7]), ]) } avx2 { Self( avx2::_mm_cvtepu8_epi16 (vector.0)) } }
    }
}
impl crate::SimdSaturatingArithmetic for U16x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_add(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_adds_epu16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_adds_epu16)\n\n\n * `PADDUSW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_add(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_add(*src); } Self::from(out) } avx2 { Self( avx2::_mm_adds_epu16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_sub(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_subs_epu16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_subs_epu16)\n\n\n * `PSUBUSW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_sub(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_sub(*src); } Self::from(out) } avx2 { Self( avx2::_mm_subs_epu16 (self.0, other.0)) } }
    }
} // Static shift
impl ShlAssign<u64> for U16x8 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U16x8\n # {\n if amount >= 16 {\n     U16x8::ZERO\n } else {\n     U16x8::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_sll_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sll_epi16)\n\n\n * `PSLLW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> U16x8 {
        select_impl_block! { scalar { if amount >= 16 { U16x8::ZERO } else { U16x8::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_sll_epi16 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<U16x8> for U16x8 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: U16x8) {
        *self = (*self) << amount;
    }
}
impl Shl<U16x8> for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U16x8  ,\n # )  -> U16x8\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..16).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n U16x8::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: U16x8) -> U16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x << amm } else { 0 }; } U16x8::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x << amm } else { 0 }; } U16x8::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for U16x8 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U16x8\n # {\n if amount >= 16 {\n     U16x8::ZERO\n } else {\n     U16x8::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_srl_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srl_epi16)\n\n\n * `PSRLW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> U16x8 {
        select_impl_block! { scalar { if amount >= 16 { U16x8::ZERO } else { U16x8::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_srl_epi16 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<U16x8> for U16x8 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: U16x8) {
        *self = (*self) >> amount;
    }
}
impl Shr<U16x8> for U16x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U16x8  ,\n # )  -> U16x8\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..16).contains(&amm) {\n         *x >> amm\n     }  else {\n         0\n     };\n }\n U16x8::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: U16x8) -> U16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x >> amm } else { 0 }; } U16x8::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x >> amm } else { 0 }; } U16x8::from(out) } }
    }
}
impl SimdBase for U16x8 {
    type Scalar = u16;
    type Array = [u16; 8];
    type Signed = I16x8;
    type Unsigned = U16x8;
    const LANES: usize = 8;
    const ZERO: Self = Self::from_array([0; 8]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_testz_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128)\n\n\n * `PTEST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm_testz_si128 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u16  ,\n # )  -> U16x8\n # {\n let mut out = [0; 8];\n out[0] = scalar;\n U16x8::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: u16) -> U16x8 {
        select_impl_block! { scalar { let mut out = [0; 8]; out[0] = scalar; U16x8::from(out) } avx2 { Self( avx2::_mm_set_epi16 ( 0, 0, 0, 0, 0, 0, 0, scalar as i16, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u16\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u16\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_extract_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_extract_epi16)\n\n\n * `PEXTRW r32, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> u16 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm_extract_epi16 ::<I>(self.0) as u16 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u16  ,\n # )  -> U16x8\n # {\n U16x8::from([scalar; 8])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set1_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: u16) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([scalar; 8]) } avx2 { Self( avx2::_mm_set1_epi16 (scalar as i16)) } }
    }
    type BroadcastLoInput = U16x8;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([vector.as_array()[0]; 8])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_broadcastw_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_broadcastw_epi16)\n\n\n * `VPBROADCASTW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([vector.as_array()[0]; 8]) } avx2 { Self( avx2::_mm_broadcastw_epi16 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     if self.as_array()[0] == other.as_array()[0] {  u16::MAX  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  u16::MAX  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  u16::MAX  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  u16::MAX  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  u16::MAX  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  u16::MAX  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  u16::MAX  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  u16::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpeq_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpeq_epi16)\n\n\n * `PCMPEQW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ if self.as_array()[0] == other.as_array()[0] { u16::MAX } else { 0 }, if self.as_array()[1] == other.as_array()[1] { u16::MAX } else { 0 }, if self.as_array()[2] == other.as_array()[2] { u16::MAX } else { 0 }, if self.as_array()[3] == other.as_array()[3] { u16::MAX } else { 0 }, if self.as_array()[4] == other.as_array()[4] { u16::MAX } else { 0 }, if self.as_array()[5] == other.as_array()[5] { u16::MAX } else { 0 }, if self.as_array()[6] == other.as_array()[6] { u16::MAX } else { 0 }, if self.as_array()[7] == other.as_array()[7] { u16::MAX } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpeq_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_andnot_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_si128)\n\n\n * `PANDN xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), ]) } avx2 { Self( avx2::_mm_andnot_si128 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     if self.as_array()[0] > other.as_array()[0] {  u16::MAX  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  u16::MAX  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  u16::MAX  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  u16::MAX  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  u16::MAX  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  u16::MAX  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  u16::MAX  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  u16::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n **NOTE:** this implementation uses an efficient vector polyfill, though this operation is not natively supported.\n ```ignore\n // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK\n let sign_bit = Self::broadcast(1 << 15);\n Self::from(I16x8::from(*self ^ sign_bit).cmp_gt(\n     I16x8::from(other ^ sign_bit)\n ))\n ```"]
    #[inline(always)]
    fn cmp_gt(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ if self.as_array()[0] > other.as_array()[0] { u16::MAX } else { 0 }, if self.as_array()[1] > other.as_array()[1] { u16::MAX } else { 0 }, if self.as_array()[2] > other.as_array()[2] { u16::MAX } else { 0 }, if self.as_array()[3] > other.as_array()[3] { u16::MAX } else { 0 }, if self.as_array()[4] > other.as_array()[4] { u16::MAX } else { 0 }, if self.as_array()[5] > other.as_array()[5] { u16::MAX } else { 0 }, if self.as_array()[6] > other.as_array()[6] { u16::MAX } else { 0 }, if self.as_array()[7] > other.as_array()[7] { u16::MAX } else { 0 }, ]) } avx2 { // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK
        let sign_bit = Self::broadcast(1 << 15); Self::from(I16x8::from(*self ^ sign_bit).cmp_gt( I16x8::from(other ^ sign_bit) )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x8\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n U16x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_slli_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_slli_epi16)\n\n\n * `PSLLW xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> U16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U16x8::from(out) } avx2 { const USELESS_ARRAY: [u8; 16] = [0; 16]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_slli_epi16 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x8\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n U16x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srli_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srli_epi16)\n\n\n * `PSRLW xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> U16x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U16x8::from(out) } avx2 { const USELESS_ARRAY: [u8; 16] = [0; 16]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_srli_epi16 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpacklo_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpacklo_epi16)\n\n\n * `PUNPCKLWD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], ]) } avx2 { Self( avx2::_mm_unpacklo_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     // Lane# 0\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpackhi_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpackhi_epi16)\n\n\n * `PUNPCKHWD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ // Lane# 0
        self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], ]) } avx2 { Self( avx2::_mm_unpackhi_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_max_epu16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epu16)\n\n\n * `PMAXUW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), ]) } avx2 { Self( avx2::_mm_max_epu16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_min_epu16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_epu16)\n\n\n * `PMINUW xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: U16x8) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), ]) } avx2 { Self( avx2::_mm_min_epu16 (self.0, other.0)) } }
    }
}
impl crate::SimdBase16 for U16x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     // 128-bit Lane #0\n     self.as_array()[I0 + 0 * 8],\n     self.as_array()[I1 + 0 * 8],\n     self.as_array()[I2 + 0 * 8],\n     self.as_array()[I3 + 0 * 8],\n     self.as_array()[4 + 0 * 8],\n     self.as_array()[5 + 0 * 8],\n     self.as_array()[6 + 0 * 8],\n     self.as_array()[7 + 0 * 8],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_shufflelo_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shufflelo_epi16)\n\n\n * `PSHUFLW xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle_lo<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(
        &self,
    ) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ // 128-bit Lane #0
        self.as_array()[I0 + 0 * 8], self.as_array()[I1 + 0 * 8], self.as_array()[I2 + 0 * 8], self.as_array()[I3 + 0 * 8], self.as_array()[4 + 0 * 8], self.as_array()[5 + 0 * 8], self.as_array()[6 + 0 * 8], self.as_array()[7 + 0 * 8], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm_shufflelo_epi16 ::<I3, I2, I1, I0>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x8\n # {\n U16x8::from([\n     // 128-bit Lane #0\n     self.as_array()[0 + 0 * 8],\n     self.as_array()[1 + 0 * 8],\n     self.as_array()[2 + 0 * 8],\n     self.as_array()[3 + 0 * 8],\n     self.as_array()[I0 + 4 + 0 * 8],\n     self.as_array()[I1 + 4 + 0 * 8],\n     self.as_array()[I2 + 4 + 0 * 8],\n     self.as_array()[I3 + 4 + 0 * 8],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_shufflehi_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shufflehi_epi16)\n\n\n * `PSHUFHW xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle_hi<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(
        &self,
    ) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ // 128-bit Lane #0
        self.as_array()[0 + 0 * 8], self.as_array()[1 + 0 * 8], self.as_array()[2 + 0 * 8], self.as_array()[3 + 0 * 8], self.as_array()[I0 + 4 + 0 * 8], self.as_array()[I1 + 4 + 0 * 8], self.as_array()[I2 + 4 + 0 * 8], self.as_array()[I3 + 4 + 0 * 8], ]) } avx2 { if I0 > 4 { panic!("I0 ({}) > 4", I0); } if I1 > 4 { panic!("I1 ({}) > 4", I1); } if I2 > 4 { panic!("I2 ({}) > 4", I2); } if I3 > 4 { panic!("I3 ({}) > 4", I3); } Self( avx2::_mm_shufflehi_epi16 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
impl crate::SimdBase8x for U16x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const B7: bool,\n #             const B6: bool,\n #             const B5: bool,\n #             const B4: bool,\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : U16x8  ,\n # )  -> U16x8\n # ;}\n # impl SomeTraitForDoc for U16x8 {\n # fn the_doc_function\n #     <\n #             const B7: bool,\n #             const B6: bool,\n #             const B5: bool,\n #             const B4: bool,\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : U16x8  ,\n # )  -> U16x8\n # {\n U16x8::from([\n         (if B0 { if_true } else { *self }).as_array()[0],\n         (if B1 { if_true } else { *self }).as_array()[1],\n         (if B2 { if_true } else { *self }).as_array()[2],\n         (if B3 { if_true } else { *self }).as_array()[3],\n         (if B4 { if_true } else { *self }).as_array()[4],\n         (if B5 { if_true } else { *self }).as_array()[5],\n         (if B6 { if_true } else { *self }).as_array()[6],\n         (if B7 { if_true } else { *self }).as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_blend_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_blend_epi16)\n\n\n * `PBLENDW xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn blend<
        const B7: bool,
        const B6: bool,
        const B5: bool,
        const B4: bool,
        const B3: bool,
        const B2: bool,
        const B1: bool,
        const B0: bool,
    >(
        &self,
        if_true: U16x8,
    ) -> U16x8 {
        select_impl_block! { scalar { U16x8::from([ (if B0 { if_true } else { *self }).as_array()[0], (if B1 { if_true } else { *self }).as_array()[1], (if B2 { if_true } else { *self }).as_array()[2], (if B3 { if_true } else { *self }).as_array()[3], (if B4 { if_true } else { *self }).as_array()[4], (if B5 { if_true } else { *self }).as_array()[5], (if B6 { if_true } else { *self }).as_array()[6], (if B7 { if_true } else { *self }).as_array()[7], ]) } avx2 { Self( avx2::_mm_blend_epi16 ::<B7, B6, B5, B4, B3, B2, B1, B0>(self.0, if_true.0)) } }
    }
}
select_impl! { scalar { type U16x16Internal = [u16 ; 16]; } avx2 { type U16x16Internal = ::std::arch::x86_64::__m256i; } }
#[doc = "`[u16; 16]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct U16x16(U16x16Internal);
unsafe impl bytemuck::Pod for U16x16 {}
unsafe impl bytemuck::Zeroable for U16x16 {}
impl PartialEq for U16x16 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for U16x16 {}
impl Default for U16x16 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for U16x16 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for U16x16 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "U16x16({:?})", <[u16; 16]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for U16x16 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for U16x16 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 16];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <u16 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<U16x16> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> U16x16 {
        let mut out = U16x16::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for U16x16 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[u16; 16]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for U16x16 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[u16; 16]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for U16x16 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n     self.as_array()[8] ^ rhs.as_array()[8],\n     self.as_array()[9] ^ rhs.as_array()[9],\n     self.as_array()[10] ^ rhs.as_array()[10],\n     self.as_array()[11] ^ rhs.as_array()[11],\n     self.as_array()[12] ^ rhs.as_array()[12],\n     self.as_array()[13] ^ rhs.as_array()[13],\n     self.as_array()[14] ^ rhs.as_array()[14],\n     self.as_array()[15] ^ rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_xor_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_xor_si256)\n\n\n * `VPXOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], self.as_array()[8] ^ rhs.as_array()[8], self.as_array()[9] ^ rhs.as_array()[9], self.as_array()[10] ^ rhs.as_array()[10], self.as_array()[11] ^ rhs.as_array()[11], self.as_array()[12] ^ rhs.as_array()[12], self.as_array()[13] ^ rhs.as_array()[13], self.as_array()[14] ^ rhs.as_array()[14], self.as_array()[15] ^ rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm256_xor_si256 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for U16x16 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n     self.as_array()[8] | rhs.as_array()[8],\n     self.as_array()[9] | rhs.as_array()[9],\n     self.as_array()[10] | rhs.as_array()[10],\n     self.as_array()[11] | rhs.as_array()[11],\n     self.as_array()[12] | rhs.as_array()[12],\n     self.as_array()[13] | rhs.as_array()[13],\n     self.as_array()[14] | rhs.as_array()[14],\n     self.as_array()[15] | rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_or_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_or_si256)\n\n\n * `VPOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], self.as_array()[8] | rhs.as_array()[8], self.as_array()[9] | rhs.as_array()[9], self.as_array()[10] | rhs.as_array()[10], self.as_array()[11] | rhs.as_array()[11], self.as_array()[12] | rhs.as_array()[12], self.as_array()[13] | rhs.as_array()[13], self.as_array()[14] | rhs.as_array()[14], self.as_array()[15] | rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm256_or_si256 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for U16x16 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n     self.as_array()[8] & rhs.as_array()[8],\n     self.as_array()[9] & rhs.as_array()[9],\n     self.as_array()[10] & rhs.as_array()[10],\n     self.as_array()[11] & rhs.as_array()[11],\n     self.as_array()[12] & rhs.as_array()[12],\n     self.as_array()[13] & rhs.as_array()[13],\n     self.as_array()[14] & rhs.as_array()[14],\n     self.as_array()[15] & rhs.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_and_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_and_si256)\n\n\n * `VPAND ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], self.as_array()[8] & rhs.as_array()[8], self.as_array()[9] & rhs.as_array()[9], self.as_array()[10] & rhs.as_array()[10], self.as_array()[11] & rhs.as_array()[11], self.as_array()[12] & rhs.as_array()[12], self.as_array()[13] & rhs.as_array()[13], self.as_array()[14] & rhs.as_array()[14], self.as_array()[15] & rhs.as_array()[15], ]) } avx2 { Self( avx2::_mm256_and_si256 (self.0, rhs.0)) } }
    }
}
impl AddAssign for U16x16 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_add(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_add(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_add(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_add(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_add(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_add(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_add(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_add(rhs.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_add_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_add_epi16)\n\n\n * `VPADDW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), self.as_array()[8].wrapping_add(rhs.as_array()[8]), self.as_array()[9].wrapping_add(rhs.as_array()[9]), self.as_array()[10].wrapping_add(rhs.as_array()[10]), self.as_array()[11].wrapping_add(rhs.as_array()[11]), self.as_array()[12].wrapping_add(rhs.as_array()[12]), self.as_array()[13].wrapping_add(rhs.as_array()[13]), self.as_array()[14].wrapping_add(rhs.as_array()[14]), self.as_array()[15].wrapping_add(rhs.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_add_epi16 (self.0, rhs.0)) } }
    }
}
impl SubAssign for U16x16 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n     self.as_array()[8].wrapping_sub(rhs.as_array()[8]),\n     self.as_array()[9].wrapping_sub(rhs.as_array()[9]),\n     self.as_array()[10].wrapping_sub(rhs.as_array()[10]),\n     self.as_array()[11].wrapping_sub(rhs.as_array()[11]),\n     self.as_array()[12].wrapping_sub(rhs.as_array()[12]),\n     self.as_array()[13].wrapping_sub(rhs.as_array()[13]),\n     self.as_array()[14].wrapping_sub(rhs.as_array()[14]),\n     self.as_array()[15].wrapping_sub(rhs.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sub_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sub_epi16)\n\n\n * `VPSUBW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), self.as_array()[8].wrapping_sub(rhs.as_array()[8]), self.as_array()[9].wrapping_sub(rhs.as_array()[9]), self.as_array()[10].wrapping_sub(rhs.as_array()[10]), self.as_array()[11].wrapping_sub(rhs.as_array()[11]), self.as_array()[12].wrapping_sub(rhs.as_array()[12]), self.as_array()[13].wrapping_sub(rhs.as_array()[13]), self.as_array()[14].wrapping_sub(rhs.as_array()[14]), self.as_array()[15].wrapping_sub(rhs.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_sub_epi16 (self.0, rhs.0)) } }
    }
}
impl U16x16 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: U16x16 =\n     U16x16::from_array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as u16, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [u16; 16]) -> U16x16 {
        select_impl_block! { scalar { U16x16(array) } avx2 { U16x16(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[u16; 16]> for U16x16 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_loadu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_loadu_si256)\n\n\n * `VMOVDQU ymm, m256`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [u16; 16]) -> U16x16 {
        select_impl_block! { scalar { U16x16(array) } avx2 { U16x16(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_loadu_si256 (array.as_ptr() as *const ::std::arch::x86_64::__m256i) }) } }
    }
}
impl From<U16x16> for [u16; 16] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_storeu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_storeu_si256)\n\n\n * `VMOVDQU m256, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U16x16) -> [u16; 16] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [u16; 16] = [0; 16]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_storeu_si256 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m256i, vector.0) } out } }
    }
}
impl From<I8x32> for U16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x32\nas little endian bits of U16x16."]
    #[inline(always)]
    fn from(x: I8x32) -> U16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x16> for U16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x16\nas little endian bits of U16x16."]
    #[inline(always)]
    fn from(x: I16x16) -> U16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x8> for U16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x8\nas little endian bits of U16x16."]
    #[inline(always)]
    fn from(x: I32x8) -> U16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x4> for U16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x4\nas little endian bits of U16x16."]
    #[inline(always)]
    fn from(x: I64x4) -> U16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x32> for U16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x32\nas little endian bits of U16x16."]
    #[inline(always)]
    fn from(x: U8x32) -> U16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x8> for U16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x8\nas little endian bits of U16x16."]
    #[inline(always)]
    fn from(x: U32x8) -> U16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x4> for U16x16 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x4\nas little endian bits of U16x16."]
    #[inline(always)]
    fn from(x: U64x4) -> U16x16 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for U16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n         u16::from(vector.as_array()[0]),\n         u16::from(vector.as_array()[1]),\n         u16::from(vector.as_array()[2]),\n         u16::from(vector.as_array()[3]),\n         u16::from(vector.as_array()[4]),\n         u16::from(vector.as_array()[5]),\n         u16::from(vector.as_array()[6]),\n         u16::from(vector.as_array()[7]),\n         u16::from(vector.as_array()[8]),\n         u16::from(vector.as_array()[9]),\n         u16::from(vector.as_array()[10]),\n         u16::from(vector.as_array()[11]),\n         u16::from(vector.as_array()[12]),\n         u16::from(vector.as_array()[13]),\n         u16::from(vector.as_array()[14]),\n         u16::from(vector.as_array()[15]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu8_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu8_epi16)\n\n\n * `VPMOVZXBW ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U8x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ u16::from(vector.as_array()[0]), u16::from(vector.as_array()[1]), u16::from(vector.as_array()[2]), u16::from(vector.as_array()[3]), u16::from(vector.as_array()[4]), u16::from(vector.as_array()[5]), u16::from(vector.as_array()[6]), u16::from(vector.as_array()[7]), u16::from(vector.as_array()[8]), u16::from(vector.as_array()[9]), u16::from(vector.as_array()[10]), u16::from(vector.as_array()[11]), u16::from(vector.as_array()[12]), u16::from(vector.as_array()[13]), u16::from(vector.as_array()[14]), u16::from(vector.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_cvtepu8_epi16 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U8x16> for U16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n         u16::from(vector.as_array()[0]),\n         u16::from(vector.as_array()[1]),\n         u16::from(vector.as_array()[2]),\n         u16::from(vector.as_array()[3]),\n         u16::from(vector.as_array()[4]),\n         u16::from(vector.as_array()[5]),\n         u16::from(vector.as_array()[6]),\n         u16::from(vector.as_array()[7]),\n         u16::from(vector.as_array()[8]),\n         u16::from(vector.as_array()[9]),\n         u16::from(vector.as_array()[10]),\n         u16::from(vector.as_array()[11]),\n         u16::from(vector.as_array()[12]),\n         u16::from(vector.as_array()[13]),\n         u16::from(vector.as_array()[14]),\n         u16::from(vector.as_array()[15]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu8_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu8_epi16)\n\n\n * `VPMOVZXBW ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U8x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ u16::from(vector.as_array()[0]), u16::from(vector.as_array()[1]), u16::from(vector.as_array()[2]), u16::from(vector.as_array()[3]), u16::from(vector.as_array()[4]), u16::from(vector.as_array()[5]), u16::from(vector.as_array()[6]), u16::from(vector.as_array()[7]), u16::from(vector.as_array()[8]), u16::from(vector.as_array()[9]), u16::from(vector.as_array()[10]), u16::from(vector.as_array()[11]), u16::from(vector.as_array()[12]), u16::from(vector.as_array()[13]), u16::from(vector.as_array()[14]), u16::from(vector.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_cvtepu8_epi16 (vector.0)) } }
    }
}
impl From<U16x8> for U16x16 {
    #[doc = " NOTE: this will _zero_ the upper bits of the destination. Other\n intrinsics are more effcient, but leave the upper bits undefined.\n At present, these more effcient intrinsics are not exposed.\n\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x8  ,\n # )  -> U16x16\n # {\n let mut out = [0; 16];\n out[0..8].copy_from_slice(&vector.as_array());\n U16x16::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_zextsi128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_zextsi128_si256)\n\n\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U16x8) -> U16x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; out[0..8].copy_from_slice(&vector.as_array()); U16x16::from(out) } avx2 { Self( avx2::_mm256_zextsi128_si256 (vector.0)) } }
    }
}
impl From<[U16x8; 2]> for U16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vectors  : [U16x8; 2]  ,\n # )  -> U16x16\n # {\n let mut out = [0; 16];\n out[0..8].copy_from_slice(&vectors[0].as_array());\n out[8..].copy_from_slice(&vectors[1].as_array());\n U16x16::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_m128i`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_m128i)\n\n\n * `VINSERTF128 ymm, ymm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vectors: [U16x8; 2]) -> U16x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; out[0..8].copy_from_slice(&vectors[0].as_array()); out[8..].copy_from_slice(&vectors[1].as_array()); U16x16::from(out) } avx2 { Self( avx2::_mm256_set_m128i (vectors[1].0, vectors[0].0)) } }
    }
} // TODO: this doesn't show up in the docs.
impl From<U16x16> for [U16x8; 2] {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x16  ,\n # )  -> [U16x8; 2]\n # {\n let mut lo = [0; 8];\n let mut hi = [0; 8];\n lo.copy_from_slice(&vector.as_array()[0..8]);\n hi.copy_from_slice(&vector.as_array()[8..]);\n [U16x8::from(lo), U16x8::from(hi)]\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extracti128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extracti128_si256)\n\n\n * `VEXTRACTI128 xmm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U16x16) -> [U16x8; 2] {
        select_impl_block! { scalar { let mut lo = [0; 8]; let mut hi = [0; 8]; lo.copy_from_slice(&vector.as_array()[0..8]); hi.copy_from_slice(&vector.as_array()[8..]); [U16x8::from(lo), U16x8::from(hi)] } avx2 { [ U16x8( avx2::_mm256_extracti128_si256 ::<0>(vector.0)), U16x8( avx2::_mm256_extracti128_si256 ::<1>(vector.0)), ] } }
    }
}
impl crate::SimdSaturatingArithmetic for U16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_add(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_adds_epu16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_adds_epu16)\n\n\n * `VPADDUSW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_add(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_add(*src); } Self::from(out) } avx2 { Self( avx2::_mm256_adds_epu16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n let mut out = self.as_array();\n for (dst, src) in out.iter_mut().zip(other.as_array().iter()) {\n     *dst = dst.saturating_sub(*src);\n }\n Self::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_subs_epu16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_subs_epu16)\n\n\n * `VPSUBUSW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn saturating_sub(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (dst, src) in out.iter_mut().zip(other.as_array().iter()) { *dst = dst.saturating_sub(*src); } Self::from(out) } avx2 { Self( avx2::_mm256_subs_epu16 (self.0, other.0)) } }
    }
} // Static shift
impl ShlAssign<u64> for U16x16 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U16x16\n # {\n if amount >= 16 {\n     U16x16::ZERO\n } else {\n     U16x16::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n         self.as_array()[8] << amount,\n         self.as_array()[9] << amount,\n         self.as_array()[10] << amount,\n         self.as_array()[11] << amount,\n         self.as_array()[12] << amount,\n         self.as_array()[13] << amount,\n         self.as_array()[14] << amount,\n         self.as_array()[15] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sll_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sll_epi16)\n\n\n * `VPSLLW ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> U16x16 {
        select_impl_block! { scalar { if amount >= 16 { U16x16::ZERO } else { U16x16::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, self.as_array()[8] << amount, self.as_array()[9] << amount, self.as_array()[10] << amount, self.as_array()[11] << amount, self.as_array()[12] << amount, self.as_array()[13] << amount, self.as_array()[14] << amount, self.as_array()[15] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_sll_epi16 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<U16x16> for U16x16 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: U16x16) {
        *self = (*self) << amount;
    }
}
impl Shl<U16x16> for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U16x16  ,\n # )  -> U16x16\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..16).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n U16x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shl(self, amount: U16x16) -> U16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x << amm } else { 0 }; } U16x16::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x << amm } else { 0 }; } U16x16::from(out) } }
    }
} // Static shift
impl ShrAssign<u64> for U16x16 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U16x16\n # {\n if amount >= 16 {\n     U16x16::ZERO\n } else {\n     U16x16::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n         self.as_array()[8] >> amount,\n         self.as_array()[9] >> amount,\n         self.as_array()[10] >> amount,\n         self.as_array()[11] >> amount,\n         self.as_array()[12] >> amount,\n         self.as_array()[13] >> amount,\n         self.as_array()[14] >> amount,\n         self.as_array()[15] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srl_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srl_epi16)\n\n\n * `VPSRLW ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> U16x16 {
        select_impl_block! { scalar { if amount >= 16 { U16x16::ZERO } else { U16x16::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, self.as_array()[8] >> amount, self.as_array()[9] >> amount, self.as_array()[10] >> amount, self.as_array()[11] >> amount, self.as_array()[12] >> amount, self.as_array()[13] >> amount, self.as_array()[14] >> amount, self.as_array()[15] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_srl_epi16 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<U16x16> for U16x16 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: U16x16) {
        *self = (*self) >> amount;
    }
}
impl Shr<U16x16> for U16x16 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U16x16  ,\n # )  -> U16x16\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..16).contains(&amm) {\n         *x >> amm\n     }  else {\n         0\n     };\n }\n U16x16::from(out)\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn shr(self, amount: U16x16) -> U16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x >> amm } else { 0 }; } U16x16::from(out) } avx2 { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..16).contains(&amm) { *x >> amm } else { 0 }; } U16x16::from(out) } }
    }
}
impl SimdBase for U16x16 {
    type Scalar = u16;
    type Array = [u16; 16];
    type Signed = I16x16;
    type Unsigned = U16x16;
    const LANES: usize = 16;
    const ZERO: Self = Self::from_array([0; 16]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_testz_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_testz_si256)\n\n\n * `VPTEST ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm256_testz_si256 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u16  ,\n # )  -> U16x16\n # {\n let mut out = [0; 16];\n out[0] = scalar;\n U16x16::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: u16) -> U16x16 {
        select_impl_block! { scalar { let mut out = [0; 16]; out[0] = scalar; U16x16::from(out) } avx2 { Self( avx2::_mm256_set_epi16 ( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, scalar as i16, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u16\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extract_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extract_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> u16 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm256_extract_epi16 ::<I>(self.0) as u16 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u16  ,\n # )  -> U16x16\n # {\n U16x16::from([scalar; 16])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set1_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi16)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: u16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([scalar; 16]) } avx2 { Self( avx2::_mm256_set1_epi16 (scalar as i16)) } }
    }
    type BroadcastLoInput = U16x8;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x8  ,\n # )  -> U16x16\n # {\n U16x16::from([vector.as_array()[0]; 16])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_broadcastw_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_broadcastw_epi16)\n\n\n * `VPBROADCASTW ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: U16x8) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([vector.as_array()[0]; 16]) } avx2 { Self( avx2::_mm256_broadcastw_epi16 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     if self.as_array()[0] == other.as_array()[0] {  u16::MAX  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  u16::MAX  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  u16::MAX  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  u16::MAX  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  u16::MAX  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  u16::MAX  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  u16::MAX  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  u16::MAX  } else { 0 },\n     if self.as_array()[8] == other.as_array()[8] {  u16::MAX  } else { 0 },\n     if self.as_array()[9] == other.as_array()[9] {  u16::MAX  } else { 0 },\n     if self.as_array()[10] == other.as_array()[10] {  u16::MAX  } else { 0 },\n     if self.as_array()[11] == other.as_array()[11] {  u16::MAX  } else { 0 },\n     if self.as_array()[12] == other.as_array()[12] {  u16::MAX  } else { 0 },\n     if self.as_array()[13] == other.as_array()[13] {  u16::MAX  } else { 0 },\n     if self.as_array()[14] == other.as_array()[14] {  u16::MAX  } else { 0 },\n     if self.as_array()[15] == other.as_array()[15] {  u16::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpeq_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpeq_epi16)\n\n\n * `VPCMPEQW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ if self.as_array()[0] == other.as_array()[0] { u16::MAX } else { 0 }, if self.as_array()[1] == other.as_array()[1] { u16::MAX } else { 0 }, if self.as_array()[2] == other.as_array()[2] { u16::MAX } else { 0 }, if self.as_array()[3] == other.as_array()[3] { u16::MAX } else { 0 }, if self.as_array()[4] == other.as_array()[4] { u16::MAX } else { 0 }, if self.as_array()[5] == other.as_array()[5] { u16::MAX } else { 0 }, if self.as_array()[6] == other.as_array()[6] { u16::MAX } else { 0 }, if self.as_array()[7] == other.as_array()[7] { u16::MAX } else { 0 }, if self.as_array()[8] == other.as_array()[8] { u16::MAX } else { 0 }, if self.as_array()[9] == other.as_array()[9] { u16::MAX } else { 0 }, if self.as_array()[10] == other.as_array()[10] { u16::MAX } else { 0 }, if self.as_array()[11] == other.as_array()[11] { u16::MAX } else { 0 }, if self.as_array()[12] == other.as_array()[12] { u16::MAX } else { 0 }, if self.as_array()[13] == other.as_array()[13] { u16::MAX } else { 0 }, if self.as_array()[14] == other.as_array()[14] { u16::MAX } else { 0 }, if self.as_array()[15] == other.as_array()[15] { u16::MAX } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpeq_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n     self.as_array()[8] & (!other.as_array()[8]),\n     self.as_array()[9] & (!other.as_array()[9]),\n     self.as_array()[10] & (!other.as_array()[10]),\n     self.as_array()[11] & (!other.as_array()[11]),\n     self.as_array()[12] & (!other.as_array()[12]),\n     self.as_array()[13] & (!other.as_array()[13]),\n     self.as_array()[14] & (!other.as_array()[14]),\n     self.as_array()[15] & (!other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_andnot_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_andnot_si256)\n\n\n * `VPANDN ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), self.as_array()[8] & (!other.as_array()[8]), self.as_array()[9] & (!other.as_array()[9]), self.as_array()[10] & (!other.as_array()[10]), self.as_array()[11] & (!other.as_array()[11]), self.as_array()[12] & (!other.as_array()[12]), self.as_array()[13] & (!other.as_array()[13]), self.as_array()[14] & (!other.as_array()[14]), self.as_array()[15] & (!other.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_andnot_si256 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     if self.as_array()[0] > other.as_array()[0] {  u16::MAX  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  u16::MAX  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  u16::MAX  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  u16::MAX  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  u16::MAX  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  u16::MAX  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  u16::MAX  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  u16::MAX  } else { 0 },\n     if self.as_array()[8] > other.as_array()[8] {  u16::MAX  } else { 0 },\n     if self.as_array()[9] > other.as_array()[9] {  u16::MAX  } else { 0 },\n     if self.as_array()[10] > other.as_array()[10] {  u16::MAX  } else { 0 },\n     if self.as_array()[11] > other.as_array()[11] {  u16::MAX  } else { 0 },\n     if self.as_array()[12] > other.as_array()[12] {  u16::MAX  } else { 0 },\n     if self.as_array()[13] > other.as_array()[13] {  u16::MAX  } else { 0 },\n     if self.as_array()[14] > other.as_array()[14] {  u16::MAX  } else { 0 },\n     if self.as_array()[15] > other.as_array()[15] {  u16::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n **NOTE:** this implementation uses an efficient vector polyfill, though this operation is not natively supported.\n ```ignore\n // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK\n let sign_bit = Self::broadcast(1 << 15);\n Self::from(I16x16::from(*self ^ sign_bit).cmp_gt(\n     I16x16::from(other ^ sign_bit)\n ))\n ```"]
    #[inline(always)]
    fn cmp_gt(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ if self.as_array()[0] > other.as_array()[0] { u16::MAX } else { 0 }, if self.as_array()[1] > other.as_array()[1] { u16::MAX } else { 0 }, if self.as_array()[2] > other.as_array()[2] { u16::MAX } else { 0 }, if self.as_array()[3] > other.as_array()[3] { u16::MAX } else { 0 }, if self.as_array()[4] > other.as_array()[4] { u16::MAX } else { 0 }, if self.as_array()[5] > other.as_array()[5] { u16::MAX } else { 0 }, if self.as_array()[6] > other.as_array()[6] { u16::MAX } else { 0 }, if self.as_array()[7] > other.as_array()[7] { u16::MAX } else { 0 }, if self.as_array()[8] > other.as_array()[8] { u16::MAX } else { 0 }, if self.as_array()[9] > other.as_array()[9] { u16::MAX } else { 0 }, if self.as_array()[10] > other.as_array()[10] { u16::MAX } else { 0 }, if self.as_array()[11] > other.as_array()[11] { u16::MAX } else { 0 }, if self.as_array()[12] > other.as_array()[12] { u16::MAX } else { 0 }, if self.as_array()[13] > other.as_array()[13] { u16::MAX } else { 0 }, if self.as_array()[14] > other.as_array()[14] { u16::MAX } else { 0 }, if self.as_array()[15] > other.as_array()[15] { u16::MAX } else { 0 }, ]) } avx2 { // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK
        let sign_bit = Self::broadcast(1 << 15); Self::from(I16x16::from(*self ^ sign_bit).cmp_gt( I16x16::from(other ^ sign_bit) )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x16\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n U16x16::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_slli_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_slli_epi16)\n\n\n * `VPSLLW ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> U16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U16x16::from(out) } avx2 { const USELESS_ARRAY: [u8; 16] = [0; 16]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_slli_epi16 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x16\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n U16x16::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srli_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srli_epi16)\n\n\n * `VPSRLW ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> U16x16 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U16x16::from(out) } avx2 { const USELESS_ARRAY: [u8; 16] = [0; 16]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_srli_epi16 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n     // Lane# 1\n     self.as_array()[8],\n     other.as_array()[8],\n     self.as_array()[9],\n     other.as_array()[9],\n     self.as_array()[10],\n     other.as_array()[10],\n     self.as_array()[11],\n     other.as_array()[11],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpacklo_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpacklo_epi16)\n\n\n * `VPUNPCKLWD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], // Lane# 1
        self.as_array()[8], other.as_array()[8], self.as_array()[9], other.as_array()[9], self.as_array()[10], other.as_array()[10], self.as_array()[11], other.as_array()[11], ]) } avx2 { Self( avx2::_mm256_unpacklo_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     // Lane# 0\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n     // Lane# 1\n     self.as_array()[12],\n     other.as_array()[12],\n     self.as_array()[13],\n     other.as_array()[13],\n     self.as_array()[14],\n     other.as_array()[14],\n     self.as_array()[15],\n     other.as_array()[15],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpackhi_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpackhi_epi16)\n\n\n * `VPUNPCKHWD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ // Lane# 0
        self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], // Lane# 1
        self.as_array()[12], other.as_array()[12], self.as_array()[13], other.as_array()[13], self.as_array()[14], other.as_array()[14], self.as_array()[15], other.as_array()[15], ]) } avx2 { Self( avx2::_mm256_unpackhi_epi16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n     self.as_array()[8].max(other.as_array()[8]),\n     self.as_array()[9].max(other.as_array()[9]),\n     self.as_array()[10].max(other.as_array()[10]),\n     self.as_array()[11].max(other.as_array()[11]),\n     self.as_array()[12].max(other.as_array()[12]),\n     self.as_array()[13].max(other.as_array()[13]),\n     self.as_array()[14].max(other.as_array()[14]),\n     self.as_array()[15].max(other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_max_epu16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_max_epu16)\n\n\n * `VPMAXUW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), self.as_array()[8].max(other.as_array()[8]), self.as_array()[9].max(other.as_array()[9]), self.as_array()[10].max(other.as_array()[10]), self.as_array()[11].max(other.as_array()[11]), self.as_array()[12].max(other.as_array()[12]), self.as_array()[13].max(other.as_array()[13]), self.as_array()[14].max(other.as_array()[14]), self.as_array()[15].max(other.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_max_epu16 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U16x16  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n     self.as_array()[8].min(other.as_array()[8]),\n     self.as_array()[9].min(other.as_array()[9]),\n     self.as_array()[10].min(other.as_array()[10]),\n     self.as_array()[11].min(other.as_array()[11]),\n     self.as_array()[12].min(other.as_array()[12]),\n     self.as_array()[13].min(other.as_array()[13]),\n     self.as_array()[14].min(other.as_array()[14]),\n     self.as_array()[15].min(other.as_array()[15]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_min_epu16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_min_epu16)\n\n\n * `VPMINUW ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: U16x16) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), self.as_array()[8].min(other.as_array()[8]), self.as_array()[9].min(other.as_array()[9]), self.as_array()[10].min(other.as_array()[10]), self.as_array()[11].min(other.as_array()[11]), self.as_array()[12].min(other.as_array()[12]), self.as_array()[13].min(other.as_array()[13]), self.as_array()[14].min(other.as_array()[14]), self.as_array()[15].min(other.as_array()[15]), ]) } avx2 { Self( avx2::_mm256_min_epu16 (self.0, other.0)) } }
    }
}
impl crate::SimdBase16 for U16x16 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     // 128-bit Lane #0\n     self.as_array()[I0 + 0 * 8],\n     self.as_array()[I1 + 0 * 8],\n     self.as_array()[I2 + 0 * 8],\n     self.as_array()[I3 + 0 * 8],\n     self.as_array()[4 + 0 * 8],\n     self.as_array()[5 + 0 * 8],\n     self.as_array()[6 + 0 * 8],\n     self.as_array()[7 + 0 * 8],\n     // 128-bit Lane #1\n     self.as_array()[I0 + 1 * 8],\n     self.as_array()[I1 + 1 * 8],\n     self.as_array()[I2 + 1 * 8],\n     self.as_array()[I3 + 1 * 8],\n     self.as_array()[4 + 1 * 8],\n     self.as_array()[5 + 1 * 8],\n     self.as_array()[6 + 1 * 8],\n     self.as_array()[7 + 1 * 8],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_shufflelo_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shufflelo_epi16)\n\n\n * `VPSHUFLW ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle_lo<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(
        &self,
    ) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ // 128-bit Lane #0
        self.as_array()[I0 + 0 * 8], self.as_array()[I1 + 0 * 8], self.as_array()[I2 + 0 * 8], self.as_array()[I3 + 0 * 8], self.as_array()[4 + 0 * 8], self.as_array()[5 + 0 * 8], self.as_array()[6 + 0 * 8], self.as_array()[7 + 0 * 8], // 128-bit Lane #1
        self.as_array()[I0 + 1 * 8], self.as_array()[I1 + 1 * 8], self.as_array()[I2 + 1 * 8], self.as_array()[I3 + 1 * 8], self.as_array()[4 + 1 * 8], self.as_array()[5 + 1 * 8], self.as_array()[6 + 1 * 8], self.as_array()[7 + 1 * 8], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm256_shufflelo_epi16 ::<I3, I2, I1, I0>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x16\n # ;}\n # impl SomeTraitForDoc for U16x16 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U16x16\n # {\n U16x16::from([\n     // 128-bit Lane #0\n     self.as_array()[0 + 0 * 8],\n     self.as_array()[1 + 0 * 8],\n     self.as_array()[2 + 0 * 8],\n     self.as_array()[3 + 0 * 8],\n     self.as_array()[I0 + 4 + 0 * 8],\n     self.as_array()[I1 + 4 + 0 * 8],\n     self.as_array()[I2 + 4 + 0 * 8],\n     self.as_array()[I3 + 4 + 0 * 8],\n     // 128-bit Lane #1\n     self.as_array()[0 + 1 * 8],\n     self.as_array()[1 + 1 * 8],\n     self.as_array()[2 + 1 * 8],\n     self.as_array()[3 + 1 * 8],\n     self.as_array()[I0 + 4 + 1 * 8],\n     self.as_array()[I1 + 4 + 1 * 8],\n     self.as_array()[I2 + 4 + 1 * 8],\n     self.as_array()[I3 + 4 + 1 * 8],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_shufflehi_epi16`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shufflehi_epi16)\n\n\n * `VPSHUFHW ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle_hi<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(
        &self,
    ) -> U16x16 {
        select_impl_block! { scalar { U16x16::from([ // 128-bit Lane #0
        self.as_array()[0 + 0 * 8], self.as_array()[1 + 0 * 8], self.as_array()[2 + 0 * 8], self.as_array()[3 + 0 * 8], self.as_array()[I0 + 4 + 0 * 8], self.as_array()[I1 + 4 + 0 * 8], self.as_array()[I2 + 4 + 0 * 8], self.as_array()[I3 + 4 + 0 * 8], // 128-bit Lane #1
        self.as_array()[0 + 1 * 8], self.as_array()[1 + 1 * 8], self.as_array()[2 + 1 * 8], self.as_array()[3 + 1 * 8], self.as_array()[I0 + 4 + 1 * 8], self.as_array()[I1 + 4 + 1 * 8], self.as_array()[I2 + 4 + 1 * 8], self.as_array()[I3 + 4 + 1 * 8], ]) } avx2 { if I0 > 4 { panic!("I0 ({}) > 4", I0); } if I1 > 4 { panic!("I1 ({}) > 4", I1); } if I2 > 4 { panic!("I2 ({}) > 4", I2); } if I3 > 4 { panic!("I3 ({}) > 4", I3); } Self( avx2::_mm256_shufflehi_epi16 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
select_impl! { scalar { type U32x4Internal = [u32 ; 4]; } avx2 { type U32x4Internal = ::std::arch::x86_64::__m128i; } }
#[doc = "`[u32; 4]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct U32x4(U32x4Internal);
unsafe impl bytemuck::Pod for U32x4 {}
unsafe impl bytemuck::Zeroable for U32x4 {}
impl PartialEq for U32x4 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for U32x4 {}
impl Default for U32x4 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for U32x4 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for U32x4 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "U32x4({:?})", <[u32; 4]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for U32x4 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for U32x4 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 4];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <u32 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<U32x4> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> U32x4 {
        let mut out = U32x4::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for U32x4 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[u32; 4]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for U32x4 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[u32; 4]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for U32x4 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_xor_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_si128)\n\n\n * `PXOR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm_xor_si128 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for U32x4 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_or_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_or_si128)\n\n\n * `POR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm_or_si128 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for U32x4 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_and_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_si128)\n\n\n * `PAND xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm_and_si128 (self.0, rhs.0)) } }
    }
}
impl AddAssign for U32x4 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_add_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_epi32)\n\n\n * `PADDD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), ]) } avx2 { Self( avx2::_mm_add_epi32 (self.0, rhs.0)) } }
    }
}
impl SubAssign for U32x4 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sub_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_epi32)\n\n\n * `PSUBD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), ]) } avx2 { Self( avx2::_mm_sub_epi32 (self.0, rhs.0)) } }
    }
}
impl U32x4 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: U32x4 =\n     U32x4::from_array([0, 1, 2, 3]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as u32, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [u32; 4]) -> U32x4 {
        select_impl_block! { scalar { U32x4(array) } avx2 { U32x4(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[u32; 4]> for U32x4 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_loadu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si128)\n\n\n * `MOVDQU xmm, m128`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [u32; 4]) -> U32x4 {
        select_impl_block! { scalar { U32x4(array) } avx2 { U32x4(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_loadu_si128 (array.as_ptr() as *const ::std::arch::x86_64::__m128i) }) } }
    }
}
impl From<U32x4> for [u32; 4] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_storeu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_si128)\n\n\n * `MOVDQU m128, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U32x4) -> [u32; 4] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [u32; 4] = [0; 4]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_storeu_si128 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m128i, vector.0) } out } }
    }
}
impl From<I8x16> for U32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x16\nas little endian bits of U32x4."]
    #[inline(always)]
    fn from(x: I8x16) -> U32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x8> for U32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x8\nas little endian bits of U32x4."]
    #[inline(always)]
    fn from(x: I16x8) -> U32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x4> for U32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x4\nas little endian bits of U32x4."]
    #[inline(always)]
    fn from(x: I32x4) -> U32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x2> for U32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x2\nas little endian bits of U32x4."]
    #[inline(always)]
    fn from(x: I64x2) -> U32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for U32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x16\nas little endian bits of U32x4."]
    #[inline(always)]
    fn from(x: U8x16) -> U32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x8> for U32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x8\nas little endian bits of U32x4."]
    #[inline(always)]
    fn from(x: U16x8) -> U32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x2> for U32x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x2\nas little endian bits of U32x4."]
    #[inline(always)]
    fn from(x: U64x2) -> U32x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl crate::ExtendingCast<U8x16> for U32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U32x4\n # {\n U32x4::from([\n         u32::from(vector.as_array()[0]),\n         u32::from(vector.as_array()[1]),\n         u32::from(vector.as_array()[2]),\n         u32::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepu8_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepu8_epi32)\n\n\n * `PMOVZXBD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U8x16) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ u32::from(vector.as_array()[0]), u32::from(vector.as_array()[1]), u32::from(vector.as_array()[2]), u32::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm_cvtepu8_epi32 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U16x8> for U32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x8  ,\n # )  -> U32x4\n # {\n U32x4::from([\n         u32::from(vector.as_array()[0]),\n         u32::from(vector.as_array()[1]),\n         u32::from(vector.as_array()[2]),\n         u32::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepu16_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepu16_epi32)\n\n\n * `PMOVZXWD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U16x8) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ u32::from(vector.as_array()[0]), u32::from(vector.as_array()[1]), u32::from(vector.as_array()[2]), u32::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm_cvtepu16_epi32 (vector.0)) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I32x4> for U32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u32  ,\n #         indices  : I32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_i32gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_i32gather_epi32)\n\n\n * `VPGATHERDD xmm, vm32x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u32, indices: I32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm_i32gather_epi32 ::<4>(base as *const i32, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u32  ,\n #         indices  : I32x4  ,\n #         mask  : U32x4  ,\n #         src  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     if (mask.as_array()[0] >> 31) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 31) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if (mask.as_array()[2] >> 31) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if (mask.as_array()[3] >> 31) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_mask_i32gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mask_i32gather_epi32)\n\n\n * `VPGATHERDD xmm, vm32x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u32, indices: I32x4, mask: U32x4, src: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ if (mask.as_array()[0] >> 31) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 31) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if (mask.as_array()[2] >> 31) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if (mask.as_array()[3] >> 31) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm_mask_i32gather_epi32 ::<4>( src.0, base as *const i32, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<U64x4> for U32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u32  ,\n #         indices  : U64x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i64gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i64gather_epi32)\n\n\n * `VPGATHERQD xmm, vm64y, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u32, indices: U64x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i64gather_epi32 ::<4>(base as *const i32, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u32  ,\n #         indices  : U64x4  ,\n #         mask  : U32x4  ,\n #         src  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     if (mask.as_array()[0] >> 31) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 31) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if (mask.as_array()[2] >> 31) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if (mask.as_array()[3] >> 31) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i64gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i64gather_epi32)\n\n\n * `VPGATHERQD xmm, vm64y, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u32, indices: U64x4, mask: U32x4, src: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ if (mask.as_array()[0] >> 31) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 31) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if (mask.as_array()[2] >> 31) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if (mask.as_array()[3] >> 31) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i64gather_epi32 ::<4>( src.0, base as *const i32, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I64x4> for U32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u32  ,\n #         indices  : I64x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i64gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i64gather_epi32)\n\n\n * `VPGATHERQD xmm, vm64y, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u32, indices: I64x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i64gather_epi32 ::<4>(base as *const i32, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u32  ,\n #         indices  : I64x4  ,\n #         mask  : U32x4  ,\n #         src  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     if (mask.as_array()[0] >> 31) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 31) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if (mask.as_array()[2] >> 31) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if (mask.as_array()[3] >> 31) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i64gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i64gather_epi32)\n\n\n * `VPGATHERQD xmm, vm64y, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u32, indices: I64x4, mask: U32x4, src: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ if (mask.as_array()[0] >> 31) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 31) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if (mask.as_array()[2] >> 31) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if (mask.as_array()[3] >> 31) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i64gather_epi32 ::<4>( src.0, base as *const i32, indices.0, mask.0, )) } }
    }
} // Static shift
impl ShlAssign<u64> for U32x4 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U32x4\n # {\n if amount >= 32 {\n     U32x4::ZERO\n } else {\n     U32x4::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_sll_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sll_epi32)\n\n\n * `PSLLD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> U32x4 {
        select_impl_block! { scalar { if amount >= 32 { U32x4::ZERO } else { U32x4::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_sll_epi32 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<U32x4> for U32x4 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: U32x4) {
        *self = (*self) << amount;
    }
}
impl Shl<U32x4> for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U32x4  ,\n # )  -> U32x4\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..32).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n U32x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sllv_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sllv_epi32)\n\n\n * `VPSLLVD xmm, xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: U32x4) -> U32x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..32).contains(&amm) { *x << amm } else { 0 }; } U32x4::from(out) } avx2 { Self( avx2::_mm_sllv_epi32 (self.0, amount.0)) } }
    }
} // Static shift
impl ShrAssign<u64> for U32x4 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U32x4\n # {\n if amount >= 32 {\n     U32x4::ZERO\n } else {\n     U32x4::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_srl_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srl_epi32)\n\n\n * `PSRLD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> U32x4 {
        select_impl_block! { scalar { if amount >= 32 { U32x4::ZERO } else { U32x4::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_srl_epi32 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<U32x4> for U32x4 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: U32x4) {
        *self = (*self) >> amount;
    }
}
impl Shr<U32x4> for U32x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U32x4  ,\n # )  -> U32x4\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..32).contains(&amm) {\n         *x >> amm\n     }  else {\n         0\n     };\n }\n U32x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srlv_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srlv_epi32)\n\n\n * `VPSRLVD xmm, xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: U32x4) -> U32x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..32).contains(&amm) { *x >> amm } else { 0 }; } U32x4::from(out) } avx2 { Self( avx2::_mm_srlv_epi32 (self.0, amount.0)) } }
    }
}
impl SimdBase for U32x4 {
    type Scalar = u32;
    type Array = [u32; 4];
    type Signed = I32x4;
    type Unsigned = U32x4;
    const LANES: usize = 4;
    const ZERO: Self = Self::from_array([0; 4]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_testz_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128)\n\n\n * `PTEST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm_testz_si128 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u32  ,\n # )  -> U32x4\n # {\n let mut out = [0; 4];\n out[0] = scalar;\n U32x4::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: u32) -> U32x4 {
        select_impl_block! { scalar { let mut out = [0; 4]; out[0] = scalar; U32x4::from(out) } avx2 { Self( avx2::_mm_set_epi32 ( 0, 0, 0, scalar as i32, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u32\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u32\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_extract_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_extract_epi32)\n\n\n * `PEXTRD r32, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> u32 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm_extract_epi32 ::<I>(self.0) as u32 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u32  ,\n # )  -> U32x4\n # {\n U32x4::from([scalar; 4])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set1_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: u32) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([scalar; 4]) } avx2 { Self( avx2::_mm_set1_epi32 (scalar as i32)) } }
    }
    type BroadcastLoInput = U32x4;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([vector.as_array()[0]; 4])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_broadcastd_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_broadcastd_epi32)\n\n\n * `VPBROADCASTD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([vector.as_array()[0]; 4]) } avx2 { Self( avx2::_mm_broadcastd_epi32 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     if self.as_array()[0] == other.as_array()[0] {  u32::MAX  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  u32::MAX  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  u32::MAX  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  u32::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpeq_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpeq_epi32)\n\n\n * `PCMPEQD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ if self.as_array()[0] == other.as_array()[0] { u32::MAX } else { 0 }, if self.as_array()[1] == other.as_array()[1] { u32::MAX } else { 0 }, if self.as_array()[2] == other.as_array()[2] { u32::MAX } else { 0 }, if self.as_array()[3] == other.as_array()[3] { u32::MAX } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpeq_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_andnot_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_si128)\n\n\n * `PANDN xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), ]) } avx2 { Self( avx2::_mm_andnot_si128 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     if self.as_array()[0] > other.as_array()[0] {  u32::MAX  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  u32::MAX  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  u32::MAX  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  u32::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n **NOTE:** this implementation uses an efficient vector polyfill, though this operation is not natively supported.\n ```ignore\n // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK\n let sign_bit = Self::broadcast(1 << 31);\n Self::from(I32x4::from(*self ^ sign_bit).cmp_gt(\n     I32x4::from(other ^ sign_bit)\n ))\n ```"]
    #[inline(always)]
    fn cmp_gt(&self, other: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ if self.as_array()[0] > other.as_array()[0] { u32::MAX } else { 0 }, if self.as_array()[1] > other.as_array()[1] { u32::MAX } else { 0 }, if self.as_array()[2] > other.as_array()[2] { u32::MAX } else { 0 }, if self.as_array()[3] > other.as_array()[3] { u32::MAX } else { 0 }, ]) } avx2 { // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK
        let sign_bit = Self::broadcast(1 << 31); Self::from(I32x4::from(*self ^ sign_bit).cmp_gt( I32x4::from(other ^ sign_bit) )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x4\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n U32x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_slli_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_slli_epi32)\n\n\n * `PSLLD xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> U32x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U32x4::from(out) } avx2 { const USELESS_ARRAY: [u8; 32] = [0; 32]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_slli_epi32 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x4\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n U32x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srli_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srli_epi32)\n\n\n * `PSRLD xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> U32x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U32x4::from(out) } avx2 { const USELESS_ARRAY: [u8; 32] = [0; 32]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_srli_epi32 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpacklo_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpacklo_epi32)\n\n\n * `PUNPCKLDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], ]) } avx2 { Self( avx2::_mm_unpacklo_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     // Lane# 0\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpackhi_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpackhi_epi32)\n\n\n * `PUNPCKHDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ // Lane# 0
        self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], ]) } avx2 { Self( avx2::_mm_unpackhi_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_max_epu32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epu32)\n\n\n * `PMAXUD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), ]) } avx2 { Self( avx2::_mm_max_epu32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_min_epu32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_epu32)\n\n\n * `PMINUD xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: U32x4) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), ]) } avx2 { Self( avx2::_mm_min_epu32 (self.0, other.0)) } }
    }
}
impl crate::SimdBase32 for U32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x4\n # {\n U32x4::from([\n     // 128-bit Lane #0\n     self.as_array()[I0 + 0 * 4],\n     self.as_array()[I1 + 0 * 4],\n     self.as_array()[I2 + 0 * 4],\n     self.as_array()[I3 + 0 * 4],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_shuffle_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shuffle_epi32)\n\n\n * `PSHUFD xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(&self) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ // 128-bit Lane #0
        self.as_array()[I0 + 0 * 4], self.as_array()[I1 + 0 * 4], self.as_array()[I2 + 0 * 4], self.as_array()[I3 + 0 * 4], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm_shuffle_epi32 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
impl crate::SimdBase4x for U32x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : U32x4  ,\n # )  -> U32x4\n # ;}\n # impl SomeTraitForDoc for U32x4 {\n # fn the_doc_function\n #     <\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : U32x4  ,\n # )  -> U32x4\n # {\n U32x4::from([\n         (if B0 { if_true } else { *self }).as_array()[0],\n         (if B1 { if_true } else { *self }).as_array()[1],\n         (if B2 { if_true } else { *self }).as_array()[2],\n         (if B3 { if_true } else { *self }).as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_blend_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_blend_epi32)\n\n\n * `VPBLENDD xmm, xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn blend<const B3: bool, const B2: bool, const B1: bool, const B0: bool>(
        &self,
        if_true: U32x4,
    ) -> U32x4 {
        select_impl_block! { scalar { U32x4::from([ (if B0 { if_true } else { *self }).as_array()[0], (if B1 { if_true } else { *self }).as_array()[1], (if B2 { if_true } else { *self }).as_array()[2], (if B3 { if_true } else { *self }).as_array()[3], ]) } avx2 { Self( avx2::_mm_blend_epi32 ::<B3, B2, B1, B0>(self.0, if_true.0)) } }
    }
}
select_impl! { scalar { type U32x8Internal = [u32 ; 8]; } avx2 { type U32x8Internal = ::std::arch::x86_64::__m256i; } }
#[doc = "`[u32; 8]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct U32x8(U32x8Internal);
unsafe impl bytemuck::Pod for U32x8 {}
unsafe impl bytemuck::Zeroable for U32x8 {}
impl PartialEq for U32x8 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for U32x8 {}
impl Default for U32x8 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for U32x8 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for U32x8 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "U32x8({:?})", <[u32; 8]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for U32x8 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for U32x8 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 8];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <u32 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<U32x8> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> U32x8 {
        let mut out = U32x8::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for U32x8 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[u32; 8]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for U32x8 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[u32; 8]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for U32x8 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n     self.as_array()[4] ^ rhs.as_array()[4],\n     self.as_array()[5] ^ rhs.as_array()[5],\n     self.as_array()[6] ^ rhs.as_array()[6],\n     self.as_array()[7] ^ rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_xor_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_xor_si256)\n\n\n * `VPXOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], self.as_array()[4] ^ rhs.as_array()[4], self.as_array()[5] ^ rhs.as_array()[5], self.as_array()[6] ^ rhs.as_array()[6], self.as_array()[7] ^ rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm256_xor_si256 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for U32x8 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n     self.as_array()[4] | rhs.as_array()[4],\n     self.as_array()[5] | rhs.as_array()[5],\n     self.as_array()[6] | rhs.as_array()[6],\n     self.as_array()[7] | rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_or_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_or_si256)\n\n\n * `VPOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], self.as_array()[4] | rhs.as_array()[4], self.as_array()[5] | rhs.as_array()[5], self.as_array()[6] | rhs.as_array()[6], self.as_array()[7] | rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm256_or_si256 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for U32x8 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n     self.as_array()[4] & rhs.as_array()[4],\n     self.as_array()[5] & rhs.as_array()[5],\n     self.as_array()[6] & rhs.as_array()[6],\n     self.as_array()[7] & rhs.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_and_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_and_si256)\n\n\n * `VPAND ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], self.as_array()[4] & rhs.as_array()[4], self.as_array()[5] & rhs.as_array()[5], self.as_array()[6] & rhs.as_array()[6], self.as_array()[7] & rhs.as_array()[7], ]) } avx2 { Self( avx2::_mm256_and_si256 (self.0, rhs.0)) } }
    }
}
impl AddAssign for U32x8 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_add(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_add(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_add(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_add(rhs.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_add_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_add_epi32)\n\n\n * `VPADDD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), self.as_array()[4].wrapping_add(rhs.as_array()[4]), self.as_array()[5].wrapping_add(rhs.as_array()[5]), self.as_array()[6].wrapping_add(rhs.as_array()[6]), self.as_array()[7].wrapping_add(rhs.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_add_epi32 (self.0, rhs.0)) } }
    }
}
impl SubAssign for U32x8 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n     self.as_array()[4].wrapping_sub(rhs.as_array()[4]),\n     self.as_array()[5].wrapping_sub(rhs.as_array()[5]),\n     self.as_array()[6].wrapping_sub(rhs.as_array()[6]),\n     self.as_array()[7].wrapping_sub(rhs.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sub_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sub_epi32)\n\n\n * `VPSUBD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), self.as_array()[4].wrapping_sub(rhs.as_array()[4]), self.as_array()[5].wrapping_sub(rhs.as_array()[5]), self.as_array()[6].wrapping_sub(rhs.as_array()[6]), self.as_array()[7].wrapping_sub(rhs.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_sub_epi32 (self.0, rhs.0)) } }
    }
}
impl U32x8 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: U32x8 =\n     U32x8::from_array([0, 1, 2, 3, 4, 5, 6, 7]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as u32, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [u32; 8]) -> U32x8 {
        select_impl_block! { scalar { U32x8(array) } avx2 { U32x8(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[u32; 8]> for U32x8 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_loadu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_loadu_si256)\n\n\n * `VMOVDQU ymm, m256`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [u32; 8]) -> U32x8 {
        select_impl_block! { scalar { U32x8(array) } avx2 { U32x8(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_loadu_si256 (array.as_ptr() as *const ::std::arch::x86_64::__m256i) }) } }
    }
}
impl From<U32x8> for [u32; 8] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_storeu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_storeu_si256)\n\n\n * `VMOVDQU m256, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U32x8) -> [u32; 8] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [u32; 8] = [0; 8]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_storeu_si256 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m256i, vector.0) } out } }
    }
}
impl From<I8x32> for U32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x32\nas little endian bits of U32x8."]
    #[inline(always)]
    fn from(x: I8x32) -> U32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x16> for U32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x16\nas little endian bits of U32x8."]
    #[inline(always)]
    fn from(x: I16x16) -> U32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x8> for U32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x8\nas little endian bits of U32x8."]
    #[inline(always)]
    fn from(x: I32x8) -> U32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x4> for U32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x4\nas little endian bits of U32x8."]
    #[inline(always)]
    fn from(x: I64x4) -> U32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x32> for U32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x32\nas little endian bits of U32x8."]
    #[inline(always)]
    fn from(x: U8x32) -> U32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x16> for U32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x16\nas little endian bits of U32x8."]
    #[inline(always)]
    fn from(x: U16x16) -> U32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U64x4> for U32x8 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U64x4\nas little endian bits of U32x8."]
    #[inline(always)]
    fn from(x: U64x4) -> U32x8 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x8> for U32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n         u32::from(vector.as_array()[0]),\n         u32::from(vector.as_array()[1]),\n         u32::from(vector.as_array()[2]),\n         u32::from(vector.as_array()[3]),\n         u32::from(vector.as_array()[4]),\n         u32::from(vector.as_array()[5]),\n         u32::from(vector.as_array()[6]),\n         u32::from(vector.as_array()[7]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu16_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu16_epi32)\n\n\n * `VPMOVZXWD ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U16x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ u32::from(vector.as_array()[0]), u32::from(vector.as_array()[1]), u32::from(vector.as_array()[2]), u32::from(vector.as_array()[3]), u32::from(vector.as_array()[4]), u32::from(vector.as_array()[5]), u32::from(vector.as_array()[6]), u32::from(vector.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_cvtepu16_epi32 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U8x16> for U32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U32x8\n # {\n U32x8::from([\n         u32::from(vector.as_array()[0]),\n         u32::from(vector.as_array()[1]),\n         u32::from(vector.as_array()[2]),\n         u32::from(vector.as_array()[3]),\n         u32::from(vector.as_array()[4]),\n         u32::from(vector.as_array()[5]),\n         u32::from(vector.as_array()[6]),\n         u32::from(vector.as_array()[7]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu8_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu8_epi32)\n\n\n * `VPMOVZXBD ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U8x16) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ u32::from(vector.as_array()[0]), u32::from(vector.as_array()[1]), u32::from(vector.as_array()[2]), u32::from(vector.as_array()[3]), u32::from(vector.as_array()[4]), u32::from(vector.as_array()[5]), u32::from(vector.as_array()[6]), u32::from(vector.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_cvtepu8_epi32 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U16x8> for U32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n         u32::from(vector.as_array()[0]),\n         u32::from(vector.as_array()[1]),\n         u32::from(vector.as_array()[2]),\n         u32::from(vector.as_array()[3]),\n         u32::from(vector.as_array()[4]),\n         u32::from(vector.as_array()[5]),\n         u32::from(vector.as_array()[6]),\n         u32::from(vector.as_array()[7]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu16_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu16_epi32)\n\n\n * `VPMOVZXWD ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U16x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ u32::from(vector.as_array()[0]), u32::from(vector.as_array()[1]), u32::from(vector.as_array()[2]), u32::from(vector.as_array()[3]), u32::from(vector.as_array()[4]), u32::from(vector.as_array()[5]), u32::from(vector.as_array()[6]), u32::from(vector.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_cvtepu16_epi32 (vector.0)) } }
    }
}
impl From<U32x4> for U32x8 {
    #[doc = " NOTE: this will _zero_ the upper bits of the destination. Other\n intrinsics are more effcient, but leave the upper bits undefined.\n At present, these more effcient intrinsics are not exposed.\n\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U32x4  ,\n # )  -> U32x8\n # {\n let mut out = [0; 8];\n out[0..4].copy_from_slice(&vector.as_array());\n U32x8::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_zextsi128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_zextsi128_si256)\n\n\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U32x4) -> U32x8 {
        select_impl_block! { scalar { let mut out = [0; 8]; out[0..4].copy_from_slice(&vector.as_array()); U32x8::from(out) } avx2 { Self( avx2::_mm256_zextsi128_si256 (vector.0)) } }
    }
}
impl From<[U32x4; 2]> for U32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vectors  : [U32x4; 2]  ,\n # )  -> U32x8\n # {\n let mut out = [0; 8];\n out[0..4].copy_from_slice(&vectors[0].as_array());\n out[4..].copy_from_slice(&vectors[1].as_array());\n U32x8::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_m128i`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_m128i)\n\n\n * `VINSERTF128 ymm, ymm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vectors: [U32x4; 2]) -> U32x8 {
        select_impl_block! { scalar { let mut out = [0; 8]; out[0..4].copy_from_slice(&vectors[0].as_array()); out[4..].copy_from_slice(&vectors[1].as_array()); U32x8::from(out) } avx2 { Self( avx2::_mm256_set_m128i (vectors[1].0, vectors[0].0)) } }
    }
} // TODO: this doesn't show up in the docs.
impl From<U32x8> for [U32x4; 2] {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U32x8  ,\n # )  -> [U32x4; 2]\n # {\n let mut lo = [0; 4];\n let mut hi = [0; 4];\n lo.copy_from_slice(&vector.as_array()[0..4]);\n hi.copy_from_slice(&vector.as_array()[4..]);\n [U32x4::from(lo), U32x4::from(hi)]\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extracti128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extracti128_si256)\n\n\n * `VEXTRACTI128 xmm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U32x8) -> [U32x4; 2] {
        select_impl_block! { scalar { let mut lo = [0; 4]; let mut hi = [0; 4]; lo.copy_from_slice(&vector.as_array()[0..4]); hi.copy_from_slice(&vector.as_array()[4..]); [U32x4::from(lo), U32x4::from(hi)] } avx2 { [ U32x4( avx2::_mm256_extracti128_si256 ::<0>(vector.0)), U32x4( avx2::_mm256_extracti128_si256 ::<1>(vector.0)), ] } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I32x8> for U32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u32  ,\n #         indices  : I32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n     base.offset(indices.as_array()[4] as isize).read_unaligned(),\n     base.offset(indices.as_array()[5] as isize).read_unaligned(),\n     base.offset(indices.as_array()[6] as isize).read_unaligned(),\n     base.offset(indices.as_array()[7] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i32gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i32gather_epi32)\n\n\n * `VPGATHERDD ymm, vm32x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u32, indices: I32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), base.offset(indices.as_array()[4] as isize).read_unaligned(), base.offset(indices.as_array()[5] as isize).read_unaligned(), base.offset(indices.as_array()[6] as isize).read_unaligned(), base.offset(indices.as_array()[7] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i32gather_epi32 ::<4>(base as *const i32, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u32  ,\n #         indices  : I32x8  ,\n #         mask  : U32x8  ,\n #         src  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     if (mask.as_array()[0] >> 31) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 31) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if (mask.as_array()[2] >> 31) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if (mask.as_array()[3] >> 31) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n     if (mask.as_array()[4] >> 31) == 1 {\n         base.offset(indices.as_array()[4] as isize).read_unaligned()\n     } else {\n         src.as_array()[4]\n     },\n     if (mask.as_array()[5] >> 31) == 1 {\n         base.offset(indices.as_array()[5] as isize).read_unaligned()\n     } else {\n         src.as_array()[5]\n     },\n     if (mask.as_array()[6] >> 31) == 1 {\n         base.offset(indices.as_array()[6] as isize).read_unaligned()\n     } else {\n         src.as_array()[6]\n     },\n     if (mask.as_array()[7] >> 31) == 1 {\n         base.offset(indices.as_array()[7] as isize).read_unaligned()\n     } else {\n         src.as_array()[7]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i32gather_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i32gather_epi32)\n\n\n * `VPGATHERDD ymm, vm32x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u32, indices: I32x8, mask: U32x8, src: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ if (mask.as_array()[0] >> 31) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 31) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if (mask.as_array()[2] >> 31) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if (mask.as_array()[3] >> 31) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, if (mask.as_array()[4] >> 31) == 1 { base.offset(indices.as_array()[4] as isize).read_unaligned() } else { src.as_array()[4] }, if (mask.as_array()[5] >> 31) == 1 { base.offset(indices.as_array()[5] as isize).read_unaligned() } else { src.as_array()[5] }, if (mask.as_array()[6] >> 31) == 1 { base.offset(indices.as_array()[6] as isize).read_unaligned() } else { src.as_array()[6] }, if (mask.as_array()[7] >> 31) == 1 { base.offset(indices.as_array()[7] as isize).read_unaligned() } else { src.as_array()[7] }, ]) } avx2 { Self( avx2::_mm256_mask_i32gather_epi32 ::<4>( src.0, base as *const i32, indices.0, mask.0, )) } }
    }
} // Static shift
impl ShlAssign<u64> for U32x8 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U32x8\n # {\n if amount >= 32 {\n     U32x8::ZERO\n } else {\n     U32x8::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n         self.as_array()[4] << amount,\n         self.as_array()[5] << amount,\n         self.as_array()[6] << amount,\n         self.as_array()[7] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sll_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sll_epi32)\n\n\n * `VPSLLD ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> U32x8 {
        select_impl_block! { scalar { if amount >= 32 { U32x8::ZERO } else { U32x8::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, self.as_array()[4] << amount, self.as_array()[5] << amount, self.as_array()[6] << amount, self.as_array()[7] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_sll_epi32 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<U32x8> for U32x8 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: U32x8) {
        *self = (*self) << amount;
    }
}
impl Shl<U32x8> for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U32x8  ,\n # )  -> U32x8\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..32).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n U32x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sllv_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sllv_epi32)\n\n\n * `VPSLLVD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: U32x8) -> U32x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..32).contains(&amm) { *x << amm } else { 0 }; } U32x8::from(out) } avx2 { Self( avx2::_mm256_sllv_epi32 (self.0, amount.0)) } }
    }
} // Static shift
impl ShrAssign<u64> for U32x8 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U32x8\n # {\n if amount >= 32 {\n     U32x8::ZERO\n } else {\n     U32x8::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n         self.as_array()[4] >> amount,\n         self.as_array()[5] >> amount,\n         self.as_array()[6] >> amount,\n         self.as_array()[7] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srl_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srl_epi32)\n\n\n * `VPSRLD ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> U32x8 {
        select_impl_block! { scalar { if amount >= 32 { U32x8::ZERO } else { U32x8::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, self.as_array()[4] >> amount, self.as_array()[5] >> amount, self.as_array()[6] >> amount, self.as_array()[7] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_srl_epi32 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<U32x8> for U32x8 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: U32x8) {
        *self = (*self) >> amount;
    }
}
impl Shr<U32x8> for U32x8 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U32x8  ,\n # )  -> U32x8\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..32).contains(&amm) {\n         *x >> amm\n     }  else {\n         0\n     };\n }\n U32x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srlv_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srlv_epi32)\n\n\n * `VPSRLVD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: U32x8) -> U32x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..32).contains(&amm) { *x >> amm } else { 0 }; } U32x8::from(out) } avx2 { Self( avx2::_mm256_srlv_epi32 (self.0, amount.0)) } }
    }
}
impl SimdBase for U32x8 {
    type Scalar = u32;
    type Array = [u32; 8];
    type Signed = I32x8;
    type Unsigned = U32x8;
    const LANES: usize = 8;
    const ZERO: Self = Self::from_array([0; 8]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_testz_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_testz_si256)\n\n\n * `VPTEST ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm256_testz_si256 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u32  ,\n # )  -> U32x8\n # {\n let mut out = [0; 8];\n out[0] = scalar;\n U32x8::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: u32) -> U32x8 {
        select_impl_block! { scalar { let mut out = [0; 8]; out[0] = scalar; U32x8::from(out) } avx2 { Self( avx2::_mm256_set_epi32 ( 0, 0, 0, 0, 0, 0, 0, scalar as i32, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u32\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u32\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extract_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extract_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> u32 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm256_extract_epi32 ::<I>(self.0) as u32 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u32  ,\n # )  -> U32x8\n # {\n U32x8::from([scalar; 8])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set1_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi32)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: u32) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([scalar; 8]) } avx2 { Self( avx2::_mm256_set1_epi32 (scalar as i32)) } }
    }
    type BroadcastLoInput = U32x4;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U32x4  ,\n # )  -> U32x8\n # {\n U32x8::from([vector.as_array()[0]; 8])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_broadcastd_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_broadcastd_epi32)\n\n\n * `VPBROADCASTD ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: U32x4) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([vector.as_array()[0]; 8]) } avx2 { Self( avx2::_mm256_broadcastd_epi32 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     if self.as_array()[0] == other.as_array()[0] {  u32::MAX  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  u32::MAX  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  u32::MAX  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  u32::MAX  } else { 0 },\n     if self.as_array()[4] == other.as_array()[4] {  u32::MAX  } else { 0 },\n     if self.as_array()[5] == other.as_array()[5] {  u32::MAX  } else { 0 },\n     if self.as_array()[6] == other.as_array()[6] {  u32::MAX  } else { 0 },\n     if self.as_array()[7] == other.as_array()[7] {  u32::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpeq_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpeq_epi32)\n\n\n * `VPCMPEQD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ if self.as_array()[0] == other.as_array()[0] { u32::MAX } else { 0 }, if self.as_array()[1] == other.as_array()[1] { u32::MAX } else { 0 }, if self.as_array()[2] == other.as_array()[2] { u32::MAX } else { 0 }, if self.as_array()[3] == other.as_array()[3] { u32::MAX } else { 0 }, if self.as_array()[4] == other.as_array()[4] { u32::MAX } else { 0 }, if self.as_array()[5] == other.as_array()[5] { u32::MAX } else { 0 }, if self.as_array()[6] == other.as_array()[6] { u32::MAX } else { 0 }, if self.as_array()[7] == other.as_array()[7] { u32::MAX } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpeq_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n     self.as_array()[4] & (!other.as_array()[4]),\n     self.as_array()[5] & (!other.as_array()[5]),\n     self.as_array()[6] & (!other.as_array()[6]),\n     self.as_array()[7] & (!other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_andnot_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_andnot_si256)\n\n\n * `VPANDN ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), self.as_array()[4] & (!other.as_array()[4]), self.as_array()[5] & (!other.as_array()[5]), self.as_array()[6] & (!other.as_array()[6]), self.as_array()[7] & (!other.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_andnot_si256 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     if self.as_array()[0] > other.as_array()[0] {  u32::MAX  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  u32::MAX  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  u32::MAX  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  u32::MAX  } else { 0 },\n     if self.as_array()[4] > other.as_array()[4] {  u32::MAX  } else { 0 },\n     if self.as_array()[5] > other.as_array()[5] {  u32::MAX  } else { 0 },\n     if self.as_array()[6] > other.as_array()[6] {  u32::MAX  } else { 0 },\n     if self.as_array()[7] > other.as_array()[7] {  u32::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n **NOTE:** this implementation uses an efficient vector polyfill, though this operation is not natively supported.\n ```ignore\n // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK\n let sign_bit = Self::broadcast(1 << 31);\n Self::from(I32x8::from(*self ^ sign_bit).cmp_gt(\n     I32x8::from(other ^ sign_bit)\n ))\n ```"]
    #[inline(always)]
    fn cmp_gt(&self, other: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ if self.as_array()[0] > other.as_array()[0] { u32::MAX } else { 0 }, if self.as_array()[1] > other.as_array()[1] { u32::MAX } else { 0 }, if self.as_array()[2] > other.as_array()[2] { u32::MAX } else { 0 }, if self.as_array()[3] > other.as_array()[3] { u32::MAX } else { 0 }, if self.as_array()[4] > other.as_array()[4] { u32::MAX } else { 0 }, if self.as_array()[5] > other.as_array()[5] { u32::MAX } else { 0 }, if self.as_array()[6] > other.as_array()[6] { u32::MAX } else { 0 }, if self.as_array()[7] > other.as_array()[7] { u32::MAX } else { 0 }, ]) } avx2 { // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK
        let sign_bit = Self::broadcast(1 << 31); Self::from(I32x8::from(*self ^ sign_bit).cmp_gt( I32x8::from(other ^ sign_bit) )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x8\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n U32x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_slli_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_slli_epi32)\n\n\n * `VPSLLD ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> U32x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U32x8::from(out) } avx2 { const USELESS_ARRAY: [u8; 32] = [0; 32]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_slli_epi32 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x8\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n U32x8::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srli_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srli_epi32)\n\n\n * `VPSRLD ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> U32x8 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U32x8::from(out) } avx2 { const USELESS_ARRAY: [u8; 32] = [0; 32]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_srli_epi32 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     self.as_array()[1],\n     other.as_array()[1],\n     // Lane# 1\n     self.as_array()[4],\n     other.as_array()[4],\n     self.as_array()[5],\n     other.as_array()[5],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpacklo_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpacklo_epi32)\n\n\n * `VPUNPCKLDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], self.as_array()[1], other.as_array()[1], // Lane# 1
        self.as_array()[4], other.as_array()[4], self.as_array()[5], other.as_array()[5], ]) } avx2 { Self( avx2::_mm256_unpacklo_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     // Lane# 0\n     self.as_array()[2],\n     other.as_array()[2],\n     self.as_array()[3],\n     other.as_array()[3],\n     // Lane# 1\n     self.as_array()[6],\n     other.as_array()[6],\n     self.as_array()[7],\n     other.as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpackhi_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpackhi_epi32)\n\n\n * `VPUNPCKHDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ // Lane# 0
        self.as_array()[2], other.as_array()[2], self.as_array()[3], other.as_array()[3], // Lane# 1
        self.as_array()[6], other.as_array()[6], self.as_array()[7], other.as_array()[7], ]) } avx2 { Self( avx2::_mm256_unpackhi_epi32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n     self.as_array()[4].max(other.as_array()[4]),\n     self.as_array()[5].max(other.as_array()[5]),\n     self.as_array()[6].max(other.as_array()[6]),\n     self.as_array()[7].max(other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_max_epu32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_max_epu32)\n\n\n * `VPMAXUD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn max(&self, other: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), self.as_array()[4].max(other.as_array()[4]), self.as_array()[5].max(other.as_array()[5]), self.as_array()[6].max(other.as_array()[6]), self.as_array()[7].max(other.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_max_epu32 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n     self.as_array()[4].min(other.as_array()[4]),\n     self.as_array()[5].min(other.as_array()[5]),\n     self.as_array()[6].min(other.as_array()[6]),\n     self.as_array()[7].min(other.as_array()[7]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_min_epu32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_min_epu32)\n\n\n * `VPMINUD ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn min(&self, other: U32x8) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), self.as_array()[4].min(other.as_array()[4]), self.as_array()[5].min(other.as_array()[5]), self.as_array()[6].min(other.as_array()[6]), self.as_array()[7].min(other.as_array()[7]), ]) } avx2 { Self( avx2::_mm256_min_epu32 (self.0, other.0)) } }
    }
}
impl crate::SimdBase32 for U32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U32x8\n # {\n U32x8::from([\n     // 128-bit Lane #0\n     self.as_array()[I0 + 0 * 4],\n     self.as_array()[I1 + 0 * 4],\n     self.as_array()[I2 + 0 * 4],\n     self.as_array()[I3 + 0 * 4],\n     // 128-bit Lane #1\n     self.as_array()[I0 + 1 * 4],\n     self.as_array()[I1 + 1 * 4],\n     self.as_array()[I2 + 1 * 4],\n     self.as_array()[I3 + 1 * 4],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_shuffle_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shuffle_epi32)\n\n\n * `VPSHUFD ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(&self) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ // 128-bit Lane #0
        self.as_array()[I0 + 0 * 4], self.as_array()[I1 + 0 * 4], self.as_array()[I2 + 0 * 4], self.as_array()[I3 + 0 * 4], // 128-bit Lane #1
        self.as_array()[I0 + 1 * 4], self.as_array()[I1 + 1 * 4], self.as_array()[I2 + 1 * 4], self.as_array()[I3 + 1 * 4], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm256_shuffle_epi32 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
impl crate::SimdBase8x for U32x8 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const B7: bool,\n #             const B6: bool,\n #             const B5: bool,\n #             const B4: bool,\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : U32x8  ,\n # )  -> U32x8\n # ;}\n # impl SomeTraitForDoc for U32x8 {\n # fn the_doc_function\n #     <\n #             const B7: bool,\n #             const B6: bool,\n #             const B5: bool,\n #             const B4: bool,\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : U32x8  ,\n # )  -> U32x8\n # {\n U32x8::from([\n         (if B0 { if_true } else { *self }).as_array()[0],\n         (if B1 { if_true } else { *self }).as_array()[1],\n         (if B2 { if_true } else { *self }).as_array()[2],\n         (if B3 { if_true } else { *self }).as_array()[3],\n         (if B4 { if_true } else { *self }).as_array()[4],\n         (if B5 { if_true } else { *self }).as_array()[5],\n         (if B6 { if_true } else { *self }).as_array()[6],\n         (if B7 { if_true } else { *self }).as_array()[7],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_blend_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_blend_epi32)\n\n\n * `VPBLENDD ymm, ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn blend<
        const B7: bool,
        const B6: bool,
        const B5: bool,
        const B4: bool,
        const B3: bool,
        const B2: bool,
        const B1: bool,
        const B0: bool,
    >(
        &self,
        if_true: U32x8,
    ) -> U32x8 {
        select_impl_block! { scalar { U32x8::from([ (if B0 { if_true } else { *self }).as_array()[0], (if B1 { if_true } else { *self }).as_array()[1], (if B2 { if_true } else { *self }).as_array()[2], (if B3 { if_true } else { *self }).as_array()[3], (if B4 { if_true } else { *self }).as_array()[4], (if B5 { if_true } else { *self }).as_array()[5], (if B6 { if_true } else { *self }).as_array()[6], (if B7 { if_true } else { *self }).as_array()[7], ]) } avx2 { Self( avx2::_mm256_blend_epi32 ::<B7, B6, B5, B4, B3, B2, B1, B0>(self.0, if_true.0)) } }
    }
}
select_impl! { scalar { type U64x2Internal = [u64 ; 2]; } avx2 { type U64x2Internal = ::std::arch::x86_64::__m128i; } }
#[doc = "`[u64; 2]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct U64x2(U64x2Internal);
unsafe impl bytemuck::Pod for U64x2 {}
unsafe impl bytemuck::Zeroable for U64x2 {}
impl PartialEq for U64x2 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for U64x2 {}
impl Default for U64x2 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for U64x2 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for U64x2 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "U64x2({:?})", <[u64; 2]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for U64x2 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for U64x2 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 2];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <u64 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<U64x2> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> U64x2 {
        let mut out = U64x2::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for U64x2 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[u64; 2]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for U64x2 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[u64; 2]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for U64x2 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_xor_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_si128)\n\n\n * `PXOR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], ]) } avx2 { Self( avx2::_mm_xor_si128 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for U64x2 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_or_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_or_si128)\n\n\n * `POR xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], ]) } avx2 { Self( avx2::_mm_or_si128 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for U64x2 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_and_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_si128)\n\n\n * `PAND xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], ]) } avx2 { Self( avx2::_mm_and_si128 (self.0, rhs.0)) } }
    }
}
impl AddAssign for U64x2 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_add_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_epi64)\n\n\n * `PADDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), ]) } avx2 { Self( avx2::_mm_add_epi64 (self.0, rhs.0)) } }
    }
}
impl SubAssign for U64x2 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sub_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_epi64)\n\n\n * `PSUBQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), ]) } avx2 { Self( avx2::_mm_sub_epi64 (self.0, rhs.0)) } }
    }
}
impl U64x2 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: U64x2 =\n     U64x2::from_array([0, 1]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as u64, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [u64; 2]) -> U64x2 {
        select_impl_block! { scalar { U64x2(array) } avx2 { U64x2(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[u64; 2]> for U64x2 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_loadu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si128)\n\n\n * `MOVDQU xmm, m128`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [u64; 2]) -> U64x2 {
        select_impl_block! { scalar { U64x2(array) } avx2 { U64x2(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_loadu_si128 (array.as_ptr() as *const ::std::arch::x86_64::__m128i) }) } }
    }
}
impl From<U64x2> for [u64; 2] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_storeu_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_si128)\n\n\n * `MOVDQU m128, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U64x2) -> [u64; 2] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [u64; 2] = [0; 2]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm_storeu_si128 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m128i, vector.0) } out } }
    }
}
impl From<I8x16> for U64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x16\nas little endian bits of U64x2."]
    #[inline(always)]
    fn from(x: I8x16) -> U64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x8> for U64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x8\nas little endian bits of U64x2."]
    #[inline(always)]
    fn from(x: I16x8) -> U64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x4> for U64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x4\nas little endian bits of U64x2."]
    #[inline(always)]
    fn from(x: I32x4) -> U64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x2> for U64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x2\nas little endian bits of U64x2."]
    #[inline(always)]
    fn from(x: I64x2) -> U64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x16> for U64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x16\nas little endian bits of U64x2."]
    #[inline(always)]
    fn from(x: U8x16) -> U64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x8> for U64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x8\nas little endian bits of U64x2."]
    #[inline(always)]
    fn from(x: U16x8) -> U64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x4> for U64x2 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x4\nas little endian bits of U64x2."]
    #[inline(always)]
    fn from(x: U32x4) -> U64x2 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl crate::ExtendingCast<U8x16> for U64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U64x2\n # {\n U64x2::from([\n         u64::from(vector.as_array()[0]),\n         u64::from(vector.as_array()[1]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepu8_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepu8_epi64)\n\n\n * `PMOVZXBQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U8x16) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ u64::from(vector.as_array()[0]), u64::from(vector.as_array()[1]), ]) } avx2 { Self( avx2::_mm_cvtepu8_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U16x8> for U64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x8  ,\n # )  -> U64x2\n # {\n U64x2::from([\n         u64::from(vector.as_array()[0]),\n         u64::from(vector.as_array()[1]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepu16_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepu16_epi64)\n\n\n * `PMOVZXWQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U16x8) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ u64::from(vector.as_array()[0]), u64::from(vector.as_array()[1]), ]) } avx2 { Self( avx2::_mm_cvtepu16_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U32x4> for U64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U32x4  ,\n # )  -> U64x2\n # {\n U64x2::from([\n         u64::from(vector.as_array()[0]),\n         u64::from(vector.as_array()[1]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cvtepu32_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtepu32_epi64)\n\n\n * `PMOVZXDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U32x4) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ u64::from(vector.as_array()[0]), u64::from(vector.as_array()[1]), ]) } avx2 { Self( avx2::_mm_cvtepu32_epi64 (vector.0)) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<U64x2> for U64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_i64gather_epi64)\n\n\n * `VPGATHERQQ xmm, vm64x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u64, indices: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm_i64gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : U64x2  ,\n #         mask  : U64x2  ,\n #         src  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     if (mask.as_array()[0] >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_mask_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mask_i64gather_epi64)\n\n\n * `VPGATHERQQ xmm, vm64x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u64, indices: U64x2, mask: U64x2, src: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ if (mask.as_array()[0] >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, ]) } avx2 { Self( avx2::_mm_mask_i64gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I64x2> for U64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : I64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_i64gather_epi64)\n\n\n * `VPGATHERQQ xmm, vm64x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u64, indices: I64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm_i64gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : I64x2  ,\n #         mask  : U64x2  ,\n #         src  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     if (mask.as_array()[0] >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_mask_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mask_i64gather_epi64)\n\n\n * `VPGATHERQQ xmm, vm64x, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u64, indices: I64x2, mask: U64x2, src: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ if (mask.as_array()[0] >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, ]) } avx2 { Self( avx2::_mm_mask_i64gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
} // Static shift
impl ShlAssign<u64> for U64x2 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U64x2\n # {\n if amount >= 64 {\n     U64x2::ZERO\n } else {\n     U64x2::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_sll_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sll_epi64)\n\n\n * `PSLLQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> U64x2 {
        select_impl_block! { scalar { if amount >= 64 { U64x2::ZERO } else { U64x2::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_sll_epi64 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<U64x2> for U64x2 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: U64x2) {
        *self = (*self) << amount;
    }
}
impl Shl<U64x2> for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U64x2  ,\n # )  -> U64x2\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..64).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n U64x2::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_sllv_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sllv_epi64)\n\n\n * `VPSLLVQ xmm, xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: U64x2) -> U64x2 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x << amm } else { 0 }; } U64x2::from(out) } avx2 { Self( avx2::_mm_sllv_epi64 (self.0, amount.0)) } }
    }
} // Static shift
impl ShrAssign<u64> for U64x2 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U64x2\n # {\n if amount >= 64 {\n     U64x2::ZERO\n } else {\n     U64x2::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n <li>\n\n [**`_mm_srl_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srl_epi64)\n\n\n * `PSRLQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> U64x2 {
        select_impl_block! { scalar { if amount >= 64 { U64x2::ZERO } else { U64x2::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm_srl_epi64 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<U64x2> for U64x2 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: U64x2) {
        *self = (*self) >> amount;
    }
}
impl Shr<U64x2> for U64x2 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U64x2  ,\n # )  -> U64x2\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..64).contains(&amm) {\n         *x >> amm\n     }  else {\n         0\n     };\n }\n U64x2::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srlv_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srlv_epi64)\n\n\n * `VPSRLVQ xmm, xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: U64x2) -> U64x2 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x >> amm } else { 0 }; } U64x2::from(out) } avx2 { Self( avx2::_mm_srlv_epi64 (self.0, amount.0)) } }
    }
}
impl SimdBase for U64x2 {
    type Scalar = u64;
    type Array = [u64; 2];
    type Signed = I64x2;
    type Unsigned = U64x2;
    const LANES: usize = 2;
    const ZERO: Self = Self::from_array([0; 2]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_testz_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128)\n\n\n * `PTEST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm_testz_si128 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u64  ,\n # )  -> U64x2\n # {\n let mut out = [0; 2];\n out[0] = scalar;\n U64x2::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: u64) -> U64x2 {
        select_impl_block! { scalar { let mut out = [0; 2]; out[0] = scalar; U64x2::from(out) } avx2 { Self( avx2::_mm_set_epi64x ( 0, scalar as i64, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u64\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u64\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_extract_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_extract_epi64)\n\n\n * `PEXTRQ r64, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> u64 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm_extract_epi64 ::<I>(self.0) as u64 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u64  ,\n # )  -> U64x2\n # {\n U64x2::from([scalar; 2])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_set1_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: u64) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([scalar; 2]) } avx2 { Self( avx2::_mm_set1_epi64x (scalar as i64)) } }
    }
    type BroadcastLoInput = U64x2;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([vector.as_array()[0]; 2])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_broadcastq_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_broadcastq_epi64)\n\n\n * `VPBROADCASTQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([vector.as_array()[0]; 2]) } avx2 { Self( avx2::_mm_broadcastq_epi64 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     if self.as_array()[0] == other.as_array()[0] {  u64::MAX  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  u64::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_cmpeq_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cmpeq_epi64)\n\n\n * `PCMPEQQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ if self.as_array()[0] == other.as_array()[0] { u64::MAX } else { 0 }, if self.as_array()[1] == other.as_array()[1] { u64::MAX } else { 0 }, ]) } avx2 { Self( avx2::_mm_cmpeq_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_andnot_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_si128)\n\n\n * `PANDN xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), ]) } avx2 { Self( avx2::_mm_andnot_si128 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     if self.as_array()[0] > other.as_array()[0] {  u64::MAX  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  u64::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n **NOTE:** this implementation uses an efficient vector polyfill, though this operation is not natively supported.\n ```ignore\n // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK\n let sign_bit = Self::broadcast(1 << 63);\n Self::from(I64x2::from(*self ^ sign_bit).cmp_gt(\n     I64x2::from(other ^ sign_bit)\n ))\n ```"]
    #[inline(always)]
    fn cmp_gt(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ if self.as_array()[0] > other.as_array()[0] { u64::MAX } else { 0 }, if self.as_array()[1] > other.as_array()[1] { u64::MAX } else { 0 }, ]) } avx2 { // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK
        let sign_bit = Self::broadcast(1 << 63); Self::from(I64x2::from(*self ^ sign_bit).cmp_gt( I64x2::from(other ^ sign_bit) )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x2\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n U64x2::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_slli_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_slli_epi64)\n\n\n * `PSLLQ xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> U64x2 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U64x2::from(out) } avx2 { const USELESS_ARRAY: [u8; 64] = [0; 64]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_slli_epi64 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x2\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n U64x2::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_srli_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srli_epi64)\n\n\n * `PSRLQ xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> U64x2 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U64x2::from(out) } avx2 { const USELESS_ARRAY: [u8; 64] = [0; 64]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm_srli_epi64 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpacklo_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpacklo_epi64)\n\n\n * `PUNPCKLQDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], ]) } avx2 { Self( avx2::_mm_unpacklo_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     // Lane# 0\n     self.as_array()[1],\n     other.as_array()[1],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_unpackhi_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_unpackhi_epi64)\n\n\n * `PUNPCKHQDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ // Lane# 0
        self.as_array()[1], other.as_array()[1], ]) } avx2 { Self( avx2::_mm_unpackhi_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn max(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), ]) } avx2 { U64x2::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), ]) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n ])\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn min(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), ]) } avx2 { U64x2::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), ]) } }
    }
}
impl crate::SimdBase64 for U64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n U64x2::from([\n     ((self.as_array()[0] as u32) as u64) * ((other.as_array()[0] as u32) as u64),\n     ((self.as_array()[1] as u32) as u64) * ((other.as_array()[1] as u32) as u64),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_mul_epu32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mul_epu32)\n\n\n * `PMULUDQ xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn mul_lo(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { U64x2::from([ ((self.as_array()[0] as u32) as u64) * ((other.as_array()[0] as u32) as u64), ((self.as_array()[1] as u32) as u64) * ((other.as_array()[1] as u32) as u64), ]) } avx2 { Self( avx2::_mm_mul_epu32 (self.0, other.0)) } }
    }
}
select_impl! { scalar { type U64x4Internal = [u64 ; 4]; } avx2 { type U64x4Internal = ::std::arch::x86_64::__m256i; } }
#[doc = "`[u64; 4]` as a vector."]
#[repr(transparent)]
#[derive(Clone, Copy)]
pub struct U64x4(U64x4Internal);
unsafe impl bytemuck::Pod for U64x4 {}
unsafe impl bytemuck::Zeroable for U64x4 {}
impl PartialEq for U64x4 {
    #[inline(always)]
    fn eq(&self, other: &Self) -> bool {
        ((*self) ^ (*other)).is_zero()
    }
}
impl Eq for U64x4 {}
impl Default for U64x4 {
    #[doc = "The zero vector."]
    #[inline(always)]
    fn default() -> Self {
        Self::ZERO
    }
}
impl std::hash::Hash for U64x4 {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        bytemuck::bytes_of(self).hash(state);
    }
}
impl std::fmt::Debug for U64x4 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "U64x4({:?})", <[u64; 4]>::from(*self))
    }
}
impl subtle::ConstantTimeEq for U64x4 {
    fn ct_eq(&self, other: &Self) -> subtle::Choice {
        self.as_array().ct_eq(&other.as_array())
    }
}
impl subtle::ConditionallySelectable for U64x4 {
    fn conditional_select(a: &Self, b: &Self, choice: subtle::Choice) -> Self {
        let mut out = [0; 4];
        for (out, (a, b)) in out
            .iter_mut()
            .zip(a.as_array().iter().zip(b.as_array().iter()))
        {
            *out = <u64 as subtle::ConditionallySelectable>::conditional_select(a, b, choice);
        }
        Self::from(out)
    }
}
impl rand::distributions::Distribution<U64x4> for rand::distributions::Standard {
    fn sample<R: rand::prelude::Rng + ?Sized>(&self, rng: &mut R) -> U64x4 {
        let mut out = U64x4::ZERO;
        rng.fill_bytes(bytemuck::bytes_of_mut(&mut out));
        out
    }
}
impl serde::Serialize for U64x4 {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        <[u64; 4]>::from(*self).serialize(serializer)
    }
}
impl<'de> serde::Deserialize<'de> for U64x4 {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        <[u64; 4]>::deserialize(deserializer).map(Self::from_array)
    }
}
impl BitXorAssign for U64x4 {
    #[inline(always)]
    fn bitxor_assign(&mut self, rhs: Self) {
        *self = self.bitxor(rhs);
    }
}
impl BitXor for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[0] ^ rhs.as_array()[0],\n     self.as_array()[1] ^ rhs.as_array()[1],\n     self.as_array()[2] ^ rhs.as_array()[2],\n     self.as_array()[3] ^ rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_xor_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_xor_si256)\n\n\n * `VPXOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitxor(self, rhs: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[0] ^ rhs.as_array()[0], self.as_array()[1] ^ rhs.as_array()[1], self.as_array()[2] ^ rhs.as_array()[2], self.as_array()[3] ^ rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm256_xor_si256 (self.0, rhs.0)) } }
    }
}
impl BitOrAssign for U64x4 {
    #[inline(always)]
    fn bitor_assign(&mut self, rhs: Self) {
        *self = self.bitor(rhs);
    }
}
impl BitOr for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[0] | rhs.as_array()[0],\n     self.as_array()[1] | rhs.as_array()[1],\n     self.as_array()[2] | rhs.as_array()[2],\n     self.as_array()[3] | rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_or_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_or_si256)\n\n\n * `VPOR ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitor(self, rhs: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[0] | rhs.as_array()[0], self.as_array()[1] | rhs.as_array()[1], self.as_array()[2] | rhs.as_array()[2], self.as_array()[3] | rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm256_or_si256 (self.0, rhs.0)) } }
    }
}
impl BitAndAssign for U64x4 {
    #[inline(always)]
    fn bitand_assign(&mut self, rhs: Self) {
        *self = self.bitand(rhs);
    }
}
impl BitAnd for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[0] & rhs.as_array()[0],\n     self.as_array()[1] & rhs.as_array()[1],\n     self.as_array()[2] & rhs.as_array()[2],\n     self.as_array()[3] & rhs.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_and_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_and_si256)\n\n\n * `VPAND ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn bitand(self, rhs: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[0] & rhs.as_array()[0], self.as_array()[1] & rhs.as_array()[1], self.as_array()[2] & rhs.as_array()[2], self.as_array()[3] & rhs.as_array()[3], ]) } avx2 { Self( avx2::_mm256_and_si256 (self.0, rhs.0)) } }
    }
}
impl AddAssign for U64x4 {
    #[inline(always)]
    fn add_assign(&mut self, rhs: Self) {
        *self = self.add(rhs);
    }
}
impl Add for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[0].wrapping_add(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_add(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_add(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_add(rhs.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_add_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_add_epi64)\n\n\n * `VPADDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn add(self, rhs: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[0].wrapping_add(rhs.as_array()[0]), self.as_array()[1].wrapping_add(rhs.as_array()[1]), self.as_array()[2].wrapping_add(rhs.as_array()[2]), self.as_array()[3].wrapping_add(rhs.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_add_epi64 (self.0, rhs.0)) } }
    }
}
impl SubAssign for U64x4 {
    #[inline(always)]
    fn sub_assign(&mut self, rhs: Self) {
        *self = self.sub(rhs);
    }
}
impl Sub for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         rhs  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[0].wrapping_sub(rhs.as_array()[0]),\n     self.as_array()[1].wrapping_sub(rhs.as_array()[1]),\n     self.as_array()[2].wrapping_sub(rhs.as_array()[2]),\n     self.as_array()[3].wrapping_sub(rhs.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sub_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sub_epi64)\n\n\n * `VPSUBQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn sub(self, rhs: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[0].wrapping_sub(rhs.as_array()[0]), self.as_array()[1].wrapping_sub(rhs.as_array()[1]), self.as_array()[2].wrapping_sub(rhs.as_array()[2]), self.as_array()[3].wrapping_sub(rhs.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_sub_epi64 (self.0, rhs.0)) } }
    }
}
impl U64x4 {
    #[doc = " Create a vector from an array.\n\n Unlike the `From` trait function, the `from_array` function is `const`.\n # Example\n ```\n # use vectoreyes::*;\n const MY_EXTREMELY_FUN_VALUE: U64x4 =\n     U64x4::from_array([0, 1, 2, 3]);\n for (i, value) in MY_EXTREMELY_FUN_VALUE.as_array().iter().copied().enumerate() {\n     assert_eq!(i as u64, value);\n }\n ```\n\n # Avx2"]
    #[inline(always)]
    pub const fn from_array(array: [u64; 4]) -> U64x4 {
        select_impl_block! { scalar { U64x4(array) } avx2 { U64x4(unsafe { std::mem::transmute(array) }) } }
    }
}
impl From<[u64; 4]> for U64x4 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_loadu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_loadu_si256)\n\n\n * `VMOVDQU ymm, m256`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(array: [u64; 4]) -> U64x4 {
        select_impl_block! { scalar { U64x4(array) } avx2 { U64x4(unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_loadu_si256 (array.as_ptr() as *const ::std::arch::x86_64::__m256i) }) } }
    }
}
impl From<U64x4> for [u64; 4] {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_storeu_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_storeu_si256)\n\n\n * `VMOVDQU m256, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U64x4) -> [u64; 4] {
        select_impl_block! { scalar { vector.0 } avx2 { let mut out: [u64; 4] = [0; 4]; unsafe { // SAFETY: the pointer doesn't need to be aligned. It's the right size.
        avx2::_mm256_storeu_si256 (out.as_mut_ptr() as *mut ::std::arch::x86_64::__m256i, vector.0) } out } }
    }
}
impl From<I8x32> for U64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I8x32\nas little endian bits of U64x4."]
    #[inline(always)]
    fn from(x: I8x32) -> U64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I16x16> for U64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I16x16\nas little endian bits of U64x4."]
    #[inline(always)]
    fn from(x: I16x16) -> U64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I32x8> for U64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I32x8\nas little endian bits of U64x4."]
    #[inline(always)]
    fn from(x: I32x8) -> U64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<I64x4> for U64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of I64x4\nas little endian bits of U64x4."]
    #[inline(always)]
    fn from(x: I64x4) -> U64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U8x32> for U64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U8x32\nas little endian bits of U64x4."]
    #[inline(always)]
    fn from(x: U8x32) -> U64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U16x16> for U64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U16x16\nas little endian bits of U64x4."]
    #[inline(always)]
    fn from(x: U16x16) -> U64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x8> for U64x4 {
    #[doc = "This cast is 100% free. It reinterprets the little-endinan bits of U32x8\nas little endian bits of U64x4."]
    #[inline(always)]
    fn from(x: U32x8) -> U64x4 {
        Self({
            select_impl_block! { scalar { #[cfg(target_endian="little")] bytemuck::cast(x.0) // TODO: big endian.
            } avx2 { x.0 } }
        })
    }
}
impl From<U32x4> for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U32x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n         u64::from(vector.as_array()[0]),\n         u64::from(vector.as_array()[1]),\n         u64::from(vector.as_array()[2]),\n         u64::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu32_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu32_epi64)\n\n\n * `VPMOVZXDQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U32x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ u64::from(vector.as_array()[0]), u64::from(vector.as_array()[1]), u64::from(vector.as_array()[2]), u64::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_cvtepu32_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U8x16> for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U8x16  ,\n # )  -> U64x4\n # {\n U64x4::from([\n         u64::from(vector.as_array()[0]),\n         u64::from(vector.as_array()[1]),\n         u64::from(vector.as_array()[2]),\n         u64::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu8_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu8_epi64)\n\n\n * `VPMOVZXBQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U8x16) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ u64::from(vector.as_array()[0]), u64::from(vector.as_array()[1]), u64::from(vector.as_array()[2]), u64::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_cvtepu8_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U16x8> for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U16x8  ,\n # )  -> U64x4\n # {\n U64x4::from([\n         u64::from(vector.as_array()[0]),\n         u64::from(vector.as_array()[1]),\n         u64::from(vector.as_array()[2]),\n         u64::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu16_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu16_epi64)\n\n\n * `VPMOVZXWQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U16x8) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ u64::from(vector.as_array()[0]), u64::from(vector.as_array()[1]), u64::from(vector.as_array()[2]), u64::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_cvtepu16_epi64 (vector.0)) } }
    }
}
impl crate::ExtendingCast<U32x4> for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U32x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n         u64::from(vector.as_array()[0]),\n         u64::from(vector.as_array()[1]),\n         u64::from(vector.as_array()[2]),\n         u64::from(vector.as_array()[3]),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cvtepu32_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cvtepu32_epi64)\n\n\n * `VPMOVZXDQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn extending_cast_from(vector: U32x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ u64::from(vector.as_array()[0]), u64::from(vector.as_array()[1]), u64::from(vector.as_array()[2]), u64::from(vector.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_cvtepu32_epi64 (vector.0)) } }
    }
}
impl From<U64x2> for U64x4 {
    #[doc = " NOTE: this will _zero_ the upper bits of the destination. Other\n intrinsics are more effcient, but leave the upper bits undefined.\n At present, these more effcient intrinsics are not exposed.\n\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U64x2  ,\n # )  -> U64x4\n # {\n let mut out = [0; 4];\n out[0..2].copy_from_slice(&vector.as_array());\n U64x4::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_zextsi128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_zextsi128_si256)\n\n\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U64x2) -> U64x4 {
        select_impl_block! { scalar { let mut out = [0; 4]; out[0..2].copy_from_slice(&vector.as_array()); U64x4::from(out) } avx2 { Self( avx2::_mm256_zextsi128_si256 (vector.0)) } }
    }
}
impl From<[U64x2; 2]> for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vectors  : [U64x2; 2]  ,\n # )  -> U64x4\n # {\n let mut out = [0; 4];\n out[0..2].copy_from_slice(&vectors[0].as_array());\n out[2..].copy_from_slice(&vectors[1].as_array());\n U64x4::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_m128i`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_m128i)\n\n\n * `VINSERTF128 ymm, ymm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vectors: [U64x2; 2]) -> U64x4 {
        select_impl_block! { scalar { let mut out = [0; 4]; out[0..2].copy_from_slice(&vectors[0].as_array()); out[2..].copy_from_slice(&vectors[1].as_array()); U64x4::from(out) } avx2 { Self( avx2::_mm256_set_m128i (vectors[1].0, vectors[0].0)) } }
    }
} // TODO: this doesn't show up in the docs.
impl From<U64x4> for [U64x2; 2] {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U64x4  ,\n # )  -> [U64x2; 2]\n # {\n let mut lo = [0; 2];\n let mut hi = [0; 2];\n lo.copy_from_slice(&vector.as_array()[0..2]);\n hi.copy_from_slice(&vector.as_array()[2..]);\n [U64x2::from(lo), U64x2::from(hi)]\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extracti128_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extracti128_si256)\n\n\n * `VEXTRACTI128 xmm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn from(vector: U64x4) -> [U64x2; 2] {
        select_impl_block! { scalar { let mut lo = [0; 2]; let mut hi = [0; 2]; lo.copy_from_slice(&vector.as_array()[0..2]); hi.copy_from_slice(&vector.as_array()[2..]); [U64x2::from(lo), U64x2::from(hi)] } avx2 { [ U64x2( avx2::_mm256_extracti128_si256 ::<0>(vector.0)), U64x2( avx2::_mm256_extracti128_si256 ::<1>(vector.0)), ] } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I32x4> for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : I32x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i32gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i32gather_epi64)\n\n\n * `VPGATHERDQ ymm, vm32x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u64, indices: I32x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i32gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : I32x4  ,\n #         mask  : U64x4  ,\n #         src  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     if (mask.as_array()[0] >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if (mask.as_array()[2] >> 63) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if (mask.as_array()[3] >> 63) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i32gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i32gather_epi64)\n\n\n * `VPGATHERDQ ymm, vm32x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u64, indices: I32x4, mask: U64x4, src: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ if (mask.as_array()[0] >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if (mask.as_array()[2] >> 63) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if (mask.as_array()[3] >> 63) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i32gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<U64x4> for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i64gather_epi64)\n\n\n * `VPGATHERQQ ymm, vm64x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u64, indices: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i64gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : U64x4  ,\n #         mask  : U64x4  ,\n #         src  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     if (mask.as_array()[0] >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if (mask.as_array()[2] >> 63) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if (mask.as_array()[3] >> 63) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i64gather_epi64)\n\n\n * `VPGATHERQQ ymm, vm64x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u64, indices: U64x4, mask: U64x4, src: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ if (mask.as_array()[0] >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if (mask.as_array()[2] >> 63) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if (mask.as_array()[3] >> 63) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i64gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
}
#[doc = "# Safety\n`base` does not need to be aligned. Forall `i`, `base + indices[i]` must meet\nthe safety requirements of [std::ptr::read_unaligned]"]
impl crate::SimdBaseGatherable<I64x4> for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : I64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     base.offset(indices.as_array()[0] as isize).read_unaligned(),\n     base.offset(indices.as_array()[1] as isize).read_unaligned(),\n     base.offset(indices.as_array()[2] as isize).read_unaligned(),\n     base.offset(indices.as_array()[3] as isize).read_unaligned(),\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_i64gather_epi64)\n\n\n * `VPGATHERQQ ymm, vm64x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather(base: *const u64, indices: I64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ base.offset(indices.as_array()[0] as isize).read_unaligned(), base.offset(indices.as_array()[1] as isize).read_unaligned(), base.offset(indices.as_array()[2] as isize).read_unaligned(), base.offset(indices.as_array()[3] as isize).read_unaligned(), ]) } avx2 { Self( avx2::_mm256_i64gather_epi64 ::<8>(base as *const i64, indices.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # unsafe\n # fn the_doc_function\n # (\n #         base  : *const u64  ,\n #         indices  : I64x4  ,\n #         mask  : U64x4  ,\n #         src  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     if (mask.as_array()[0] >> 63) == 1 {\n         base.offset(indices.as_array()[0] as isize).read_unaligned()\n     } else {\n         src.as_array()[0]\n     },\n     if (mask.as_array()[1] >> 63) == 1 {\n         base.offset(indices.as_array()[1] as isize).read_unaligned()\n     } else {\n         src.as_array()[1]\n     },\n     if (mask.as_array()[2] >> 63) == 1 {\n         base.offset(indices.as_array()[2] as isize).read_unaligned()\n     } else {\n         src.as_array()[2]\n     },\n     if (mask.as_array()[3] >> 63) == 1 {\n         base.offset(indices.as_array()[3] as isize).read_unaligned()\n     } else {\n         src.as_array()[3]\n     },\n ])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mask_i64gather_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mask_i64gather_epi64)\n\n\n * `VPGATHERQQ ymm, vm64x, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    unsafe fn gather_masked(base: *const u64, indices: I64x4, mask: U64x4, src: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ if (mask.as_array()[0] >> 63) == 1 { base.offset(indices.as_array()[0] as isize).read_unaligned() } else { src.as_array()[0] }, if (mask.as_array()[1] >> 63) == 1 { base.offset(indices.as_array()[1] as isize).read_unaligned() } else { src.as_array()[1] }, if (mask.as_array()[2] >> 63) == 1 { base.offset(indices.as_array()[2] as isize).read_unaligned() } else { src.as_array()[2] }, if (mask.as_array()[3] >> 63) == 1 { base.offset(indices.as_array()[3] as isize).read_unaligned() } else { src.as_array()[3] }, ]) } avx2 { Self( avx2::_mm256_mask_i64gather_epi64 ::<8>( src.0, base as *const i64, indices.0, mask.0, )) } }
    }
} // Static shift
impl ShlAssign<u64> for U64x4 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: u64) {
        *self = (*self) << amount;
    }
}
impl Shl<u64> for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U64x4\n # {\n if amount >= 64 {\n     U64x4::ZERO\n } else {\n     U64x4::from([\n         self.as_array()[0] << amount,\n         self.as_array()[1] << amount,\n         self.as_array()[2] << amount,\n         self.as_array()[3] << amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sll_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sll_epi64)\n\n\n * `VPSLLQ ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: u64) -> U64x4 {
        select_impl_block! { scalar { if amount >= 64 { U64x4::ZERO } else { U64x4::from([ self.as_array()[0] << amount, self.as_array()[1] << amount, self.as_array()[2] << amount, self.as_array()[3] << amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_sll_epi64 (self.0, amount)) } }
    }
} // Variable shift
impl ShlAssign<U64x4> for U64x4 {
    #[inline(always)]
    fn shl_assign(&mut self, amount: U64x4) {
        *self = (*self) << amount;
    }
}
impl Shl<U64x4> for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U64x4  ,\n # )  -> U64x4\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..64).contains(&amm) {\n         *x << amm\n     }  else {\n         0\n     };\n }\n U64x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_sllv_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_sllv_epi64)\n\n\n * `VPSLLVQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shl(self, amount: U64x4) -> U64x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x << amm } else { 0 }; } U64x4::from(out) } avx2 { Self( avx2::_mm256_sllv_epi64 (self.0, amount.0)) } }
    }
} // Static shift
impl ShrAssign<u64> for U64x4 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: u64) {
        *self = (*self) >> amount;
    }
}
impl Shr<u64> for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : u64  ,\n # )  -> U64x4\n # {\n if amount >= 64 {\n     U64x4::ZERO\n } else {\n     U64x4::from([\n         self.as_array()[0] >> amount,\n         self.as_array()[1] >> amount,\n         self.as_array()[2] >> amount,\n         self.as_array()[3] >> amount,\n     ])\n }\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srl_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srl_epi64)\n\n\n * `VPSRLQ ymm, ymm, xmm`\n </li>\n <li>\n\n [**`_mm_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: u64) -> U64x4 {
        select_impl_block! { scalar { if amount >= 64 { U64x4::ZERO } else { U64x4::from([ self.as_array()[0] >> amount, self.as_array()[1] >> amount, self.as_array()[2] >> amount, self.as_array()[3] >> amount, ]) } } avx2 { let amount = avx2::_mm_set_epi64x (0, amount as i64); Self( avx2::_mm256_srl_epi64 (self.0, amount)) } }
    }
} // Variable shift
impl ShrAssign<U64x4> for U64x4 {
    #[inline(always)]
    fn shr_assign(&mut self, amount: U64x4) {
        *self = (*self) >> amount;
    }
}
impl Shr<U64x4> for U64x4 {
    type Output = Self;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         self  ,\n #         amount  : U64x4  ,\n # )  -> U64x4\n # {\n let mut out = self.as_array();\n for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) {\n     *x = if (0..64).contains(&amm) {\n         *x >> amm\n     }  else {\n         0\n     };\n }\n U64x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srlv_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srlv_epi64)\n\n\n * `VPSRLVQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn shr(self, amount: U64x4) -> U64x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for (x, amm) in out.iter_mut().zip(amount.as_array().iter().copied()) { *x = if (0..64).contains(&amm) { *x >> amm } else { 0 }; } U64x4::from(out) } avx2 { Self( avx2::_mm256_srlv_epi64 (self.0, amount.0)) } }
    }
}
impl SimdBase for U64x4 {
    type Scalar = u64;
    type Array = [u64; 4];
    type Signed = I64x4;
    type Unsigned = U64x4;
    const LANES: usize = 4;
    const ZERO: Self = Self::from_array([0; 4]);
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n # )  -> bool\n # {\n self.as_array().iter().all(|x| *x == 0)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_testz_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_testz_si256)\n\n\n * `VPTEST ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn is_zero(&self) -> bool {
        select_impl_block! { scalar { self.as_array().iter().all(|x| *x == 0) } avx2 { avx2::_mm256_testz_si256 (self.0, self.0) == 1 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u64  ,\n # )  -> U64x4\n # {\n let mut out = [0; 4];\n out[0] = scalar;\n U64x4::from(out)\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn set_lo(scalar: u64) -> U64x4 {
        select_impl_block! { scalar { let mut out = [0; 4]; out[0] = scalar; U64x4::from(out) } avx2 { Self( avx2::_mm256_set_epi64x ( 0, 0, 0, scalar as i64, )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u64\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n #     <\n #             const I: usize,\n #     >\n # (\n #         &self  ,\n # )  -> u64\n # {\n self.as_array()[I]\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_extract_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_extract_epi64)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn extract<const I: usize>(&self) -> u64 {
        select_impl_block! { scalar { self.as_array()[I] } avx2 { avx2::_mm256_extract_epi64 ::<I>(self.0) as u64 } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         scalar  : u64  ,\n # )  -> U64x4\n # {\n U64x4::from([scalar; 4])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_set1_epi64x`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi64x)\n\n\n Instruction sequence.\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast(scalar: u64) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([scalar; 4]) } avx2 { Self( avx2::_mm256_set1_epi64x (scalar as i64)) } }
    }
    type BroadcastLoInput = U64x2;
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # fn the_doc_function\n # (\n #         vector  : U64x2  ,\n # )  -> U64x4\n # {\n U64x4::from([vector.as_array()[0]; 4])\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_broadcastq_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_broadcastq_epi64)\n\n\n * `VPBROADCASTQ ymm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn broadcast_lo(vector: U64x2) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([vector.as_array()[0]; 4]) } avx2 { Self( avx2::_mm256_broadcastq_epi64 (vector.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     if self.as_array()[0] == other.as_array()[0] {  u64::MAX  } else { 0 },\n     if self.as_array()[1] == other.as_array()[1] {  u64::MAX  } else { 0 },\n     if self.as_array()[2] == other.as_array()[2] {  u64::MAX  } else { 0 },\n     if self.as_array()[3] == other.as_array()[3] {  u64::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_cmpeq_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_cmpeq_epi64)\n\n\n * `VPCMPEQQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn cmp_eq(&self, other: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ if self.as_array()[0] == other.as_array()[0] { u64::MAX } else { 0 }, if self.as_array()[1] == other.as_array()[1] { u64::MAX } else { 0 }, if self.as_array()[2] == other.as_array()[2] { u64::MAX } else { 0 }, if self.as_array()[3] == other.as_array()[3] { u64::MAX } else { 0 }, ]) } avx2 { Self( avx2::_mm256_cmpeq_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[0] & (!other.as_array()[0]),\n     self.as_array()[1] & (!other.as_array()[1]),\n     self.as_array()[2] & (!other.as_array()[2]),\n     self.as_array()[3] & (!other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_andnot_si256`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_andnot_si256)\n\n\n * `VPANDN ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn and_not(&self, other: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[0] & (!other.as_array()[0]), self.as_array()[1] & (!other.as_array()[1]), self.as_array()[2] & (!other.as_array()[2]), self.as_array()[3] & (!other.as_array()[3]), ]) } avx2 { Self( avx2::_mm256_andnot_si256 (other.0, self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     if self.as_array()[0] > other.as_array()[0] {  u64::MAX  } else { 0 },\n     if self.as_array()[1] > other.as_array()[1] {  u64::MAX  } else { 0 },\n     if self.as_array()[2] > other.as_array()[2] {  u64::MAX  } else { 0 },\n     if self.as_array()[3] > other.as_array()[3] {  u64::MAX  } else { 0 },\n ])\n # }\n # }\n ```\n # Avx2\n **NOTE:** this implementation uses an efficient vector polyfill, though this operation is not natively supported.\n ```ignore\n // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK\n let sign_bit = Self::broadcast(1 << 63);\n Self::from(I64x4::from(*self ^ sign_bit).cmp_gt(\n     I64x4::from(other ^ sign_bit)\n ))\n ```"]
    #[inline(always)]
    fn cmp_gt(&self, other: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ if self.as_array()[0] > other.as_array()[0] { u64::MAX } else { 0 }, if self.as_array()[1] > other.as_array()[1] { u64::MAX } else { 0 }, if self.as_array()[2] > other.as_array()[2] { u64::MAX } else { 0 }, if self.as_array()[3] > other.as_array()[3] { u64::MAX } else { 0 }, ]) } avx2 { // Based on https://stackoverflow.com/a/33173643 and https://git.io/JmghK
        let sign_bit = Self::broadcast(1 << 63); Self::from(I64x4::from(*self ^ sign_bit).cmp_gt( I64x4::from(other ^ sign_bit) )) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x4\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x <<= BITS;\n }\n U64x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_slli_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_slli_epi64)\n\n\n * `VPSLLQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_left<const BITS: usize>(&self) -> U64x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x <<= BITS; } U64x4::from(out) } avx2 { const USELESS_ARRAY: [u8; 64] = [0; 64]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_slli_epi64 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n #     <\n #             const BITS: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x4\n # {\n let mut out = self.as_array();\n for x in out.iter_mut() {\n     *x >>= BITS;\n }\n U64x4::from(out)\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_srli_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_srli_epi64)\n\n\n * `VPSRLQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shift_right<const BITS: usize>(&self) -> U64x4 {
        select_impl_block! { scalar { let mut out = self.as_array(); for x in out.iter_mut() { *x >>= BITS; } U64x4::from(out) } avx2 { const USELESS_ARRAY: [u8; 64] = [0; 64]; let _assert_bits_in_range: u8 = USELESS_ARRAY[BITS]; Self( avx2::_mm256_srli_epi64 ::<BITS>(self.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     // Lane# 0\n     self.as_array()[0],\n     other.as_array()[0],\n     // Lane# 1\n     self.as_array()[2],\n     other.as_array()[2],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpacklo_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpacklo_epi64)\n\n\n * `VPUNPCKLQDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_lo(&self, other: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ // Lane# 0
        self.as_array()[0], other.as_array()[0], // Lane# 1
        self.as_array()[2], other.as_array()[2], ]) } avx2 { Self( avx2::_mm256_unpacklo_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     // Lane# 0\n     self.as_array()[1],\n     other.as_array()[1],\n     // Lane# 1\n     self.as_array()[3],\n     other.as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_unpackhi_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_unpackhi_epi64)\n\n\n * `VPUNPCKHQDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn unpack_hi(&self, other: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ // Lane# 0
        self.as_array()[1], other.as_array()[1], // Lane# 1
        self.as_array()[3], other.as_array()[3], ]) } avx2 { Self( avx2::_mm256_unpackhi_epi64 (self.0, other.0)) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[0].max(other.as_array()[0]),\n     self.as_array()[1].max(other.as_array()[1]),\n     self.as_array()[2].max(other.as_array()[2]),\n     self.as_array()[3].max(other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn max(&self, other: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), ]) } avx2 { U64x4::from([ self.as_array()[0].max(other.as_array()[0]), self.as_array()[1].max(other.as_array()[1]), self.as_array()[2].max(other.as_array()[2]), self.as_array()[3].max(other.as_array()[3]), ]) } }
    }
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[0].min(other.as_array()[0]),\n     self.as_array()[1].min(other.as_array()[1]),\n     self.as_array()[2].min(other.as_array()[2]),\n     self.as_array()[3].min(other.as_array()[3]),\n ])\n # }\n # }\n ```\n # Avx2\n **WARNING:** this implementation is a polyfill which executes the scalar implemenation."]
    #[inline(always)]
    fn min(&self, other: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), ]) } avx2 { U64x4::from([ self.as_array()[0].min(other.as_array()[0]), self.as_array()[1].min(other.as_array()[1]), self.as_array()[2].min(other.as_array()[2]), self.as_array()[3].min(other.as_array()[3]), ]) } }
    }
}
impl crate::SimdBase4x for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n #     <\n #             const B3: bool,\n #             const B2: bool,\n #             const B1: bool,\n #             const B0: bool,\n #     >\n # (\n #         &self  ,\n #         if_true  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n         (if B0 { if_true } else { *self }).as_array()[0],\n         (if B1 { if_true } else { *self }).as_array()[1],\n         (if B2 { if_true } else { *self }).as_array()[2],\n         (if B3 { if_true } else { *self }).as_array()[3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_blend_epi32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_blend_epi32)\n\n\n * `VPBLENDD ymm, ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn blend<const B3: bool, const B2: bool, const B1: bool, const B0: bool>(
        &self,
        if_true: U64x4,
    ) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ (if B0 { if_true } else { *self }).as_array()[0], (if B1 { if_true } else { *self }).as_array()[1], (if B2 { if_true } else { *self }).as_array()[2], (if B3 { if_true } else { *self }).as_array()[3], ]) } avx2 { Self( avx2::_mm256_blend_epi32 ::< B3, B3, B2, B2, B1, B1, B0, B0 >(self.0, if_true.0)) } }
    }
}
impl crate::SimdBase64 for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n # (\n #         &self  ,\n #         other  : U64x4  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     ((self.as_array()[0] as u32) as u64) * ((other.as_array()[0] as u32) as u64),\n     ((self.as_array()[1] as u32) as u64) * ((other.as_array()[1] as u32) as u64),\n     ((self.as_array()[2] as u32) as u64) * ((other.as_array()[2] as u32) as u64),\n     ((self.as_array()[3] as u32) as u64) * ((other.as_array()[3] as u32) as u64),\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_mul_epu32`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_mul_epu32)\n\n\n * `VPMULUDQ ymm, ymm, ymm`\n </li>\n </ul>"]
    #[inline(always)]
    fn mul_lo(&self, other: U64x4) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ ((self.as_array()[0] as u32) as u64) * ((other.as_array()[0] as u32) as u64), ((self.as_array()[1] as u32) as u64) * ((other.as_array()[1] as u32) as u64), ((self.as_array()[2] as u32) as u64) * ((other.as_array()[2] as u32) as u64), ((self.as_array()[3] as u32) as u64) * ((other.as_array()[3] as u32) as u64), ]) } avx2 { Self( avx2::_mm256_mul_epu32 (self.0, other.0)) } }
    }
}
impl crate::SimdBase4x64 for U64x4 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x4\n # ;}\n # impl SomeTraitForDoc for U64x4 {\n # fn the_doc_function\n #     <\n #             const I3: usize,\n #             const I2: usize,\n #             const I1: usize,\n #             const I0: usize,\n #     >\n # (\n #         &self  ,\n # )  -> U64x4\n # {\n U64x4::from([\n     self.as_array()[I0],\n     self.as_array()[I1],\n     self.as_array()[I2],\n     self.as_array()[I3],\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm256_permute4x64_epi64`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_permute4x64_epi64)\n\n\n * `VPERMQ ymm, ymm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn shuffle<const I3: usize, const I2: usize, const I1: usize, const I0: usize>(&self) -> U64x4 {
        select_impl_block! { scalar { U64x4::from([ self.as_array()[I0], self.as_array()[I1], self.as_array()[I2], self.as_array()[I3], ]) } avx2 { const USELESS_ARRAY: [u8; 4] = [0; 4]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I0]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I1]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I2]; let _assert_bits_in_range_i: u8 = USELESS_ARRAY[I3]; Self( avx2::_mm256_permute4x64_epi64 ::<I3, I2, I1, I0>(self.0)) } }
    }
}
impl U64x2 {
    #[doc = "\n # Scalar Equivalent:\n ```\n # use vectoreyes::*;\n # trait SomeTraitForDoc {\n # fn the_doc_function\n #     <\n #             const HI_OTHER: bool,\n #             const HI_SELF: bool,\n #     >\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # ;}\n # impl SomeTraitForDoc for U64x2 {\n # fn the_doc_function\n #     <\n #             const HI_OTHER: bool,\n #             const HI_SELF: bool,\n #     >\n # (\n #         &self  ,\n #         other  : U64x2  ,\n # )  -> U64x2\n # {\n  let x = if HI_SELF { self.as_array()[1] } else { self.as_array()[0] };\n  let y = if HI_OTHER { other.as_array()[1] } else { other.as_array()[0] };\n // This software carryless-multplication implementation is from https://github.com/RustCrypto/universal-hashes/blob/2e8a948dbb25bc2ac6c712b4bdc21b158527ca70/polyval/src/backend/soft64.rs\n // That code is MIT/Apache dual-licensed.\n #[inline(always)]\n fn bmul64(x: u64, y: u64) -> u64 {\n     use std::num::Wrapping;\n     let x0 = Wrapping(x & 0x1111_1111_1111_1111);\n     let x1 = Wrapping(x & 0x2222_2222_2222_2222);\n     let x2 = Wrapping(x & 0x4444_4444_4444_4444);\n     let x3 = Wrapping(x & 0x8888_8888_8888_8888);\n     let y0 = Wrapping(y & 0x1111_1111_1111_1111);\n     let y1 = Wrapping(y & 0x2222_2222_2222_2222);\n     let y2 = Wrapping(y & 0x4444_4444_4444_4444);\n     let y3 = Wrapping(y & 0x8888_8888_8888_8888);\n     let mut z0 = ((x0 * y0) ^ (x1 * y3) ^ (x2 * y2) ^ (x3 * y1)).0;\n     let mut z1 = ((x0 * y1) ^ (x1 * y0) ^ (x2 * y3) ^ (x3 * y2)).0;\n     let mut z2 = ((x0 * y2) ^ (x1 * y1) ^ (x2 * y0) ^ (x3 * y3)).0;\n     let mut z3 = ((x0 * y3) ^ (x1 * y2) ^ (x2 * y1) ^ (x3 * y0)).0;\n     z0 &= 0x1111_1111_1111_1111;\n     z1 &= 0x2222_2222_2222_2222;\n     z2 &= 0x4444_4444_4444_4444;\n     z3 &= 0x8888_8888_8888_8888;\n     z0 | z1 | z2 | z3\n }\n #[inline(always)]\n fn rev64(mut x: u64) -> u64 {\n     x = ((x & 0x5555_5555_5555_5555) << 1) | ((x >> 1) & 0x5555_5555_5555_5555);\n     x = ((x & 0x3333_3333_3333_3333) << 2) | ((x >> 2) & 0x3333_3333_3333_3333);\n     x = ((x & 0x0f0f_0f0f_0f0f_0f0f) << 4) | ((x >> 4) & 0x0f0f_0f0f_0f0f_0f0f);\n     x = ((x & 0x00ff_00ff_00ff_00ff) << 8) | ((x >> 8) & 0x00ff_00ff_00ff_00ff);\n     x = ((x & 0xffff_0000_ffff) << 16) | ((x >> 16) & 0xffff_0000_ffff);\n     (x << 32) | (x >> 32)\n }\n U64x2::from([\n     bmul64(x, y),\n     rev64(bmul64(rev64(x), rev64(y))) >> 1,\n ])\n # }\n # }\n ```\n # Avx2\n <ul>\n <li>\n\n [**`_mm_clmulepi64_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_clmulepi64_si128)\n\n\n * `PCLMULQDQ xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    pub fn carryless_mul<const HI_OTHER: bool, const HI_SELF: bool>(&self, other: U64x2) -> U64x2 {
        select_impl_block! { scalar { let x = if HI_SELF { self.as_array()[1] } else { self.as_array()[0] }; let y = if HI_OTHER { other.as_array()[1] } else { other.as_array()[0] }; // This software carryless-multplication implementation is from https://github.com/RustCrypto/universal-hashes/blob/2e8a948dbb25bc2ac6c712b4bdc21b158527ca70/polyval/src/backend/soft64.rs
        // That code is MIT/Apache dual-licensed.
        #[inline(always)] fn bmul64(x: u64, y: u64) -> u64 { use std::num::Wrapping; let x0 = Wrapping(x & 0x1111_1111_1111_1111); let x1 = Wrapping(x & 0x2222_2222_2222_2222); let x2 = Wrapping(x & 0x4444_4444_4444_4444); let x3 = Wrapping(x & 0x8888_8888_8888_8888); let y0 = Wrapping(y & 0x1111_1111_1111_1111); let y1 = Wrapping(y & 0x2222_2222_2222_2222); let y2 = Wrapping(y & 0x4444_4444_4444_4444); let y3 = Wrapping(y & 0x8888_8888_8888_8888); let mut z0 = ((x0 * y0) ^ (x1 * y3) ^ (x2 * y2) ^ (x3 * y1)).0; let mut z1 = ((x0 * y1) ^ (x1 * y0) ^ (x2 * y3) ^ (x3 * y2)).0; let mut z2 = ((x0 * y2) ^ (x1 * y1) ^ (x2 * y0) ^ (x3 * y3)).0; let mut z3 = ((x0 * y3) ^ (x1 * y2) ^ (x2 * y1) ^ (x3 * y0)).0; z0 &= 0x1111_1111_1111_1111; z1 &= 0x2222_2222_2222_2222; z2 &= 0x4444_4444_4444_4444; z3 &= 0x8888_8888_8888_8888; z0 | z1 | z2 | z3 } #[inline(always)] fn rev64(mut x: u64) -> u64 { x = ((x & 0x5555_5555_5555_5555) << 1) | ((x >> 1) & 0x5555_5555_5555_5555); x = ((x & 0x3333_3333_3333_3333) << 2) | ((x >> 2) & 0x3333_3333_3333_3333); x = ((x & 0x0f0f_0f0f_0f0f_0f0f) << 4) | ((x >> 4) & 0x0f0f_0f0f_0f0f_0f0f); x = ((x & 0x00ff_00ff_00ff_00ff) << 8) | ((x >> 8) & 0x00ff_00ff_00ff_00ff); x = ((x & 0xffff_0000_ffff) << 16) | ((x >> 16) & 0xffff_0000_ffff); (x << 32) | (x >> 32) } U64x2::from([ bmul64(x, y), rev64(bmul64(rev64(x), rev64(y))) >> 1, ]) } avx2 { Self( avx2::_mm_clmulepi64_si128 ::<HI_OTHER, HI_SELF>(self.0, other.0)) } }
    }
} // TODO: add Aes192
  // This code is based on https://github.com/RustCrypto/block-ciphers/tree/5389542d0306c718eda37bab890bc67772bd1e69/aes
  // That code is MIT/Apache dual-licensed
  // The original MIT license for the AES-NI code is reproduced below:
  // Copyright (c) 2018 Artyom Pavlov
  //
  // Permission is hereby granted, free of charge, to any
  // person obtaining a copy of this software and associated
  // documentation files (the "Software"), to deal in the
  // Software without restriction, including without
  // limitation the rights to use, copy, modify, merge,
  // publish, distribute, sublicense, and/or sell copies of
  // the Software, and to permit persons to whom the Software
  // is furnished to do so, subject to the following
  // conditions:
  //
  // The above copyright notice and this permission notice
  // shall be included in all copies or substantial portions
  // of the Software.
  //
  // THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF
  // ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
  // TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
  // PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT
  // SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
  // CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
  // OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR
  // IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
  // DEALINGS IN THE SOFTWARE.
select_impl! { scalar { #[allow(clippy::large_enum_variant)] #[derive(Clone)] enum Aes128KeySchedule { Variable(aes::Aes128), // TODO: if we care a lot about scalar performance, this could be quite slow.
Fixed, } impl From<aes::Aes128> for Aes128KeySchedule { #[inline(always)] fn from(x: aes::Aes128) -> Self { Self::Variable(x) } } impl Deref for Aes128KeySchedule { type Target = aes::Aes128; #[inline(always)] fn deref(&self) -> &Self::Target { lazy_static::lazy_static! { static ref FIXED_AES_128: aes::Aes128 = { use aes::cipher::{KeyInit, generic_array::GenericArray}; aes::Aes128::new(&GenericArray::from([189, 36, 0, 193, 18, 65, 206, 51, 237, 61, 125, 199, 168, 86, 64, 37])) }; } match self { Self::Variable(aes) => aes, Self::Fixed => FIXED_AES_128.deref(), } } } type Aes128EncryptOnlyKeySchedule = Aes128KeySchedule; } avx2 { type Aes128EncryptOnlyKeySchedule = [U32x4; 11]; #[derive(Clone)] struct Aes128KeySchedule { encrypt_keys: [U32x4; 11], decrypt_keys: [U32x4; 11], } } }
#[doc = "A key-scheduled Aes128 block cipher which can both encrypt and decrypt blocks."]
#[derive(Clone)]
pub struct Aes128 {
    key: Aes128KeySchedule,
}
impl std::fmt::Debug for Aes128 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "Aes128(...)")
    }
}
impl crate::AesBlockCipher for Aes128 {
    type Key = U8x16;
    type EncryptOnly = Aes128EncryptOnly;
    const BLOCK_COUNT_HINT: usize = {
        select_impl_block! { scalar { 8 } avx2 { #[cfg(vectoreyes_target_cpu="skylake")] { 4 } #[cfg(vectoreyes_target_cpu="skylake-avx512")] { 4 } #[cfg(vectoreyes_target_cpu="cascadelake")] { 4 } #[cfg(vectoreyes_target_cpu="znver1")] { 4 } #[cfg(vectoreyes_target_cpu="znver2")] { 4 } #[cfg(vectoreyes_target_cpu="znver3")] { 4 } #[cfg(not(any( vectoreyes_target_cpu="skylake", vectoreyes_target_cpu="skylake-avx512", vectoreyes_target_cpu="cascadelake", vectoreyes_target_cpu="znver1", vectoreyes_target_cpu="znver2", vectoreyes_target_cpu="znver3", )))] { 8 } } }
    };
    const FIXED_KEY: Self = Self {
        key: {
            select_impl_block! { scalar { Aes128KeySchedule::Fixed } avx2 { Aes128KeySchedule { encrypt_keys: [ U32x4::from_array([3238012093, 869155090, 3346873837, 624973480]), U32x4::from_array([54471949, 821128223, 4153168370, 3536586586]), U32x4::from_array([3179964106, 2373651157, 2063031079, 2822462589]), U32x4::from_array([1112048772, 3476262481, 3049575798, 503286027]), U32x4::from_array([1777111595, 2799639674, 320588044, 249839623]), U32x4::from_array([2890398514, 177792840, 428027460, 392654403]), U32x4::from_array([3065478797, 3157236165, 2779559809, 2999704002]), U32x4::from_array([2474664440, 800061501, 2315472828, 952669822]), U32x4::from_array([1619495400, 1328050645, 3307945577, 4259555351]), U32x4::from_array([2429786161, 3757828580, 449894285, 3878911898]), U32x4::from_array([675765205, 4156338737, 3983304124, 174004774]), ], decrypt_keys: [ U32x4::from_array([3238012093, 869155090, 3346873837, 624973480]), U32x4::from_array([1534283129, 1353216825, 299572238, 3455295203]), U32x4::from_array([1037093821, 1836628100, 2091099274, 2974825065]), U32x4::from_array([3122358467, 3613642823, 2881516749, 445664932]), U32x4::from_array([3943393460, 1013496051, 2544421950, 2369290906]), U32x4::from_array([3849123421, 3640938158, 1319928464, 3281256458]), U32x4::from_array([4001290683, 930780949, 2044073349, 3124893071]), U32x4::from_array([384560028, 563182729, 1481074956, 3792010371]), U32x4::from_array([942655063, 431930078, 1106891730, 2751246161]), U32x4::from_array([302765350, 196440056, 1246542890, 3920630651]), U32x4::from_array([675765205, 4156338737, 3983304124, 174004774]), ], } } }
        },
    };
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aesimc_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesimc_si128)\n\n\n * `AESIMC xmm, xmm`\n </li>\n <li>\n\n [**`_mm_aeskeygenassist_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aeskeygenassist_si128)\n\n\n * `AESKEYGENASSIST xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn new_with_key(key: U8x16) -> Self {
        select_impl_block! { scalar { use aes::cipher::{KeyInit, generic_array::GenericArray}; let key_bytes = key.as_array(); Aes128 { key: aes::Aes128::new(&GenericArray::from(key_bytes)).into(), } } avx2 { use crate::SimdBase32; use crate::SimdBase8; let mut enc_keys: [U32x4; 11] = Default::default(); let mut dec_keys: [U32x4; 11] = Default::default(); enc_keys[0] = key.into(); dec_keys[0] = key.into(); { let t1 = enc_keys[0]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x01>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[1] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[1] = t1; } { let t1 = enc_keys[1]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x02>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[2] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[2] = t1; } { let t1 = enc_keys[2]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x04>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[3] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[3] = t1; } { let t1 = enc_keys[3]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x08>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[4] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[4] = t1; } { let t1 = enc_keys[4]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x10>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[5] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[5] = t1; } { let t1 = enc_keys[5]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x20>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[6] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[6] = t1; } { let t1 = enc_keys[6]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x40>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[7] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[7] = t1; } { let t1 = enc_keys[7]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x80>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[8] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[8] = t1; } { let t1 = enc_keys[8]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x1B>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[9] = t1; let t1 = U32x4( avx2::_mm_aesimc_si128 (t1.0)); dec_keys[9] = t1; } { let t1 = enc_keys[9]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x36>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[10] = t1; dec_keys[10] = t1; } Aes128 { key: Aes128KeySchedule { encrypt_keys: enc_keys, decrypt_keys: dec_keys, } , } } }
    }
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aesenc_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenc_si128)\n\n\n * `AESENC xmm, xmm`\n </li>\n <li>\n\n [**`_mm_aesenclast_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenclast_si128)\n\n\n * `AESENCLAST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn encrypt_many<const N: usize>(&self, blocks: [U8x16; N]) -> [U8x16; N]
    where
        ArrayUnrolledOps: UnrollableArraySize<N>,
    {
        select_impl_block! { scalar { use aes::cipher::{BlockEncrypt, generic_array::GenericArray}; // TODO: support ParBlocks
        blocks.array_map(#[inline(always)] |block| { let mut block = GenericArray::from(block.as_array()); (*self.key).encrypt_block(&mut block); U8x16::from(<[u8; 16]>::from(block)) }) } avx2 { let encrypt_keys = &self.key.encrypt_keys; let mut blocks = blocks.array_map( #[inline(always)] |block| (U32x4::from(block) ^ encrypt_keys[0]).0 ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[1].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[2].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[3].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[4].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[5].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[6].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[7].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[8].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[9].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenclast_si128 (block, encrypt_keys[10].0) ); blocks.array_map(#[inline(always)] |block| U8x16(block)) } }
    }
}
impl crate::AesBlockCipherDecrypt for Aes128 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aesdec_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesdec_si128)\n\n\n * `AESDEC xmm, xmm`\n </li>\n <li>\n\n [**`_mm_aesdeclast_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesdeclast_si128)\n\n\n * `AESDECLAST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn decrypt_many<const N: usize>(&self, blocks: [U8x16; N]) -> [U8x16; N]
    where
        ArrayUnrolledOps: UnrollableArraySize<N>,
    {
        select_impl_block! { scalar { use aes::cipher::{BlockDecrypt, generic_array::GenericArray}; // TODO: support ParBlocks
        blocks.array_map(#[inline(always)] |block| { let mut block = GenericArray::from(block.as_array()); (*self.key).decrypt_block(&mut block); U8x16::from(<[u8; 16]>::from(block)) }) } avx2 { let mut blocks = blocks.array_map( #[inline(always)] |block| (U32x4::from(block) ^ self.key.decrypt_keys[10]).0 ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[9].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[8].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[7].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[6].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[5].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[4].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[3].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[2].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[1].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdeclast_si128 (block, self.key.decrypt_keys[0].0) ); blocks.array_map(#[inline(always)] |block| U8x16(block)) } }
    }
}
impl From<Aes128> for Aes128EncryptOnly {
    #[doc = "\n # Avx2"]
    #[inline(always)]
    fn from(aes: Aes128) -> Aes128EncryptOnly {
        select_impl_block! { scalar { Aes128EncryptOnly { key: aes.key, } } avx2 { Aes128EncryptOnly { key: aes.key.encrypt_keys, } } }
    }
}
#[doc = "A key-scheduled Aes128 block cipher which can only encrypt blocks."]
#[derive(Clone)]
pub struct Aes128EncryptOnly {
    key: Aes128EncryptOnlyKeySchedule,
}
impl std::fmt::Debug for Aes128EncryptOnly {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "Aes128EncryptOnly(...)")
    }
}
impl crate::AesBlockCipher for Aes128EncryptOnly {
    type Key = U8x16;
    type EncryptOnly = Aes128EncryptOnly;
    const BLOCK_COUNT_HINT: usize = {
        select_impl_block! { scalar { 8 } avx2 { #[cfg(vectoreyes_target_cpu="skylake")] { 4 } #[cfg(vectoreyes_target_cpu="skylake-avx512")] { 4 } #[cfg(vectoreyes_target_cpu="cascadelake")] { 4 } #[cfg(vectoreyes_target_cpu="znver1")] { 4 } #[cfg(vectoreyes_target_cpu="znver2")] { 4 } #[cfg(vectoreyes_target_cpu="znver3")] { 4 } #[cfg(not(any( vectoreyes_target_cpu="skylake", vectoreyes_target_cpu="skylake-avx512", vectoreyes_target_cpu="cascadelake", vectoreyes_target_cpu="znver1", vectoreyes_target_cpu="znver2", vectoreyes_target_cpu="znver3", )))] { 8 } } }
    };
    const FIXED_KEY: Self = Self {
        key: {
            select_impl_block! { scalar { Aes128KeySchedule::Fixed } avx2 { [ U32x4::from_array([3238012093, 869155090, 3346873837, 624973480]), U32x4::from_array([54471949, 821128223, 4153168370, 3536586586]), U32x4::from_array([3179964106, 2373651157, 2063031079, 2822462589]), U32x4::from_array([1112048772, 3476262481, 3049575798, 503286027]), U32x4::from_array([1777111595, 2799639674, 320588044, 249839623]), U32x4::from_array([2890398514, 177792840, 428027460, 392654403]), U32x4::from_array([3065478797, 3157236165, 2779559809, 2999704002]), U32x4::from_array([2474664440, 800061501, 2315472828, 952669822]), U32x4::from_array([1619495400, 1328050645, 3307945577, 4259555351]), U32x4::from_array([2429786161, 3757828580, 449894285, 3878911898]), U32x4::from_array([675765205, 4156338737, 3983304124, 174004774]), ] } }
        },
    };
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aeskeygenassist_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aeskeygenassist_si128)\n\n\n * `AESKEYGENASSIST xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn new_with_key(key: U8x16) -> Self {
        select_impl_block! { scalar { use aes::cipher::{KeyInit, generic_array::GenericArray}; let key_bytes = key.as_array(); Aes128EncryptOnly { key: aes::Aes128::new(&GenericArray::from(key_bytes)).into(), } } avx2 { use crate::SimdBase32; use crate::SimdBase8; let mut enc_keys: [U32x4; 11] = Default::default(); enc_keys[0] = key.into(); { let t1 = enc_keys[0]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x01>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[1] = t1; } { let t1 = enc_keys[1]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x02>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[2] = t1; } { let t1 = enc_keys[2]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x04>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[3] = t1; } { let t1 = enc_keys[3]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x08>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[4] = t1; } { let t1 = enc_keys[4]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x10>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[5] = t1; } { let t1 = enc_keys[5]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x20>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[6] = t1; } { let t1 = enc_keys[6]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x40>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[7] = t1; } { let t1 = enc_keys[7]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x80>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[8] = t1; } { let t1 = enc_keys[8]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x1B>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[9] = t1; } { let t1 = enc_keys[9]; let t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x36>(t1.0) ); let t2 = t2.shuffle::<3, 3, 3, 3>(); let t3: U32x4 = U8x16::from(t1).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t3: U32x4 = U8x16::from(t3).shift_bytes_left::<4>().into(); let t1 = t1 ^ t3; let t1 = t1 ^ t2; enc_keys[10] = t1; } Aes128EncryptOnly { key: enc_keys , } } }
    }
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aesenc_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenc_si128)\n\n\n * `AESENC xmm, xmm`\n </li>\n <li>\n\n [**`_mm_aesenclast_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenclast_si128)\n\n\n * `AESENCLAST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn encrypt_many<const N: usize>(&self, blocks: [U8x16; N]) -> [U8x16; N]
    where
        ArrayUnrolledOps: UnrollableArraySize<N>,
    {
        select_impl_block! { scalar { use aes::cipher::{BlockEncrypt, generic_array::GenericArray}; // TODO: support ParBlocks
        blocks.array_map(#[inline(always)] |block| { let mut block = GenericArray::from(block.as_array()); (*self.key).encrypt_block(&mut block); U8x16::from(<[u8; 16]>::from(block)) }) } avx2 { let encrypt_keys = &self.key; let mut blocks = blocks.array_map( #[inline(always)] |block| (U32x4::from(block) ^ encrypt_keys[0]).0 ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[1].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[2].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[3].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[4].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[5].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[6].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[7].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[8].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[9].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenclast_si128 (block, encrypt_keys[10].0) ); blocks.array_map(#[inline(always)] |block| U8x16(block)) } }
    }
}
select_impl! { scalar { #[allow(clippy::large_enum_variant)] #[derive(Clone)] enum Aes256KeySchedule { Variable(aes::Aes256), // TODO: if we care a lot about scalar performance, this could be quite slow.
Fixed, } impl From<aes::Aes256> for Aes256KeySchedule { #[inline(always)] fn from(x: aes::Aes256) -> Self { Self::Variable(x) } } impl Deref for Aes256KeySchedule { type Target = aes::Aes256; #[inline(always)] fn deref(&self) -> &Self::Target { lazy_static::lazy_static! { static ref FIXED_AES_256: aes::Aes256 = { use aes::cipher::{KeyInit, generic_array::GenericArray}; aes::Aes256::new(&GenericArray::from([156, 63, 253, 81, 157, 52, 243, 206, 213, 76, 200, 118, 144, 71, 141, 110, 23, 19, 106, 206, 52, 29, 51, 6, 102, 136, 149, 40, 59, 234, 162, 127])) }; } match self { Self::Variable(aes) => aes, Self::Fixed => FIXED_AES_256.deref(), } } } type Aes256EncryptOnlyKeySchedule = Aes256KeySchedule; } avx2 { type Aes256EncryptOnlyKeySchedule = [U32x4; 15]; #[derive(Clone)] struct Aes256KeySchedule { encrypt_keys: [U32x4; 15], decrypt_keys: [U32x4; 15], } } }
#[doc = "A key-scheduled Aes256 block cipher which can both encrypt and decrypt blocks."]
#[derive(Clone)]
pub struct Aes256 {
    key: Aes256KeySchedule,
}
impl std::fmt::Debug for Aes256 {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "Aes256(...)")
    }
}
impl crate::AesBlockCipher for Aes256 {
    type Key = U8x32;
    type EncryptOnly = Aes256EncryptOnly;
    const BLOCK_COUNT_HINT: usize = {
        select_impl_block! { scalar { 8 } avx2 { #[cfg(vectoreyes_target_cpu="skylake")] { 4 } #[cfg(vectoreyes_target_cpu="skylake-avx512")] { 4 } #[cfg(vectoreyes_target_cpu="cascadelake")] { 4 } #[cfg(vectoreyes_target_cpu="znver1")] { 4 } #[cfg(vectoreyes_target_cpu="znver2")] { 4 } #[cfg(vectoreyes_target_cpu="znver3")] { 4 } #[cfg(not(any( vectoreyes_target_cpu="skylake", vectoreyes_target_cpu="skylake-avx512", vectoreyes_target_cpu="cascadelake", vectoreyes_target_cpu="znver1", vectoreyes_target_cpu="znver2", vectoreyes_target_cpu="znver3", )))] { 8 } } }
    };
    const FIXED_KEY: Self = Self {
        key: {
            select_impl_block! { scalar { Aes256KeySchedule::Fixed } avx2 { Aes256KeySchedule { encrypt_keys: [ U32x4::from_array([1375551388, 3472045213, 1992838357, 1854752656]), U32x4::from_array([3463058199, 104013108, 680888422, 2141383227]), U32x4::from_array([3006203162, 2111582599, 185892178, 1704540866]), U32x4::from_array([2206503730, 2243399174, 2904688224, 3531664475]), U32x4::from_array([2325399766, 4148623697, 4233266179, 2580266689]), U32x4::from_array([1838890314, 3895243596, 1158591788, 2542688631]), U32x4::from_array([2131939609, 2287216712, 1946594379, 3989639818]), U32x4::from_array([942076980, 3490334584, 2500129364, 42642211]), U32x4::from_array([1499836275, 3509684027, 2771856240, 1224358394]), U32x4::from_array([1779164697, 3120669025, 788878133, 764409878]), U32x4::from_array([515759138, 3482082073, 1790668905, 574704019]), U32x4::from_array([4186496453, 1133083812, 1821126545, 1090770823]), U32x4::from_array([155100940, 3333618709, 2886306940, 2387133935]), U32x4::from_array([3772414746, 2740127678, 3487508527, 2396848040]), U32x4::from_array([3408339290, 227925327, 2711376179, 802460892]), ], decrypt_keys: [ U32x4::from_array([1375551388, 3472045213, 1992838357, 1854752656]), U32x4::from_array([3909185319, 4072264070, 4293458365, 4181554364]), U32x4::from_array([3811526663, 375223336, 3515008932, 1932076154]), U32x4::from_array([1535450212, 2839334370, 1456747615, 2951299299]), U32x4::from_array([1609499071, 1236432279, 2553328179, 3944322633]), U32x4::from_array([2648757288, 886839754, 1644703125, 3454102902]), U32x4::from_array([3340646468, 2393656787, 379387872, 4253378985]), U32x4::from_array([1227235987, 2113551193, 535959244, 3524492218]), U32x4::from_array([1508144169, 3611834874, 3251947034, 1011986355]), U32x4::from_array([2290295734, 4118354159, 3934981667, 949515673]), U32x4::from_array([3223959564, 392266230, 3602204652, 3940870239]), U32x4::from_array([491383766, 3895522105, 45874458, 975407235]), U32x4::from_array([2642940057, 2330562927, 1549588099, 3065528028]), U32x4::from_array([3761687872, 134626937, 180208483, 815680480]), U32x4::from_array([3408339290, 227925327, 2711376179, 802460892]), ], } } }
        },
    };
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aesimc_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesimc_si128)\n\n\n * `AESIMC xmm, xmm`\n </li>\n <li>\n\n [**`_mm_aeskeygenassist_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aeskeygenassist_si128)\n\n\n * `AESKEYGENASSIST xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn new_with_key(key: U8x32) -> Self {
        select_impl_block! { scalar { use aes::cipher::{KeyInit, generic_array::GenericArray}; let key_bytes = key.as_array(); Aes256 { key: aes::Aes256::new(&GenericArray::from(key_bytes)).into(), } } avx2 { use crate::SimdBase32; use crate::SimdBase8; let mut enc_keys: [U32x4; 15] = Default::default(); let mut dec_keys: [U32x4; 15] = Default::default(); let key: [U8x16; 2] = key.into(); let k1: U32x4 = key[0].into(); let k2: U32x4 = key[1].into(); enc_keys[0] = k1; enc_keys[1] = k2; dec_keys[0] = k1; dec_keys[1] = U32x4( avx2::_mm_aesimc_si128 (k2.0)); { let mut t1: U32x4 = enc_keys[0]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[1]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x01>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[2] = t1; dec_keys[2] = U32x4( avx2::_mm_aesimc_si128 (t1.0)); t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[3] = t3; dec_keys[3] = U32x4( avx2::_mm_aesimc_si128 (t3.0)); } { let mut t1: U32x4 = enc_keys[2]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[3]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x02>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[4] = t1; dec_keys[4] = U32x4( avx2::_mm_aesimc_si128 (t1.0)); t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[5] = t3; dec_keys[5] = U32x4( avx2::_mm_aesimc_si128 (t3.0)); } { let mut t1: U32x4 = enc_keys[4]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[5]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x04>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[6] = t1; dec_keys[6] = U32x4( avx2::_mm_aesimc_si128 (t1.0)); t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[7] = t3; dec_keys[7] = U32x4( avx2::_mm_aesimc_si128 (t3.0)); } { let mut t1: U32x4 = enc_keys[6]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[7]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x08>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[8] = t1; dec_keys[8] = U32x4( avx2::_mm_aesimc_si128 (t1.0)); t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[9] = t3; dec_keys[9] = U32x4( avx2::_mm_aesimc_si128 (t3.0)); } { let mut t1: U32x4 = enc_keys[8]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[9]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x10>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[10] = t1; dec_keys[10] = U32x4( avx2::_mm_aesimc_si128 (t1.0)); t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[11] = t3; dec_keys[11] = U32x4( avx2::_mm_aesimc_si128 (t3.0)); } { let mut t1: U32x4 = enc_keys[10]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[11]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x20>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[12] = t1; dec_keys[12] = U32x4( avx2::_mm_aesimc_si128 (t1.0)); t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[13] = t3; dec_keys[13] = U32x4( avx2::_mm_aesimc_si128 (t3.0)); } // last round
        { let mut t1: U32x4 = enc_keys[14 - 2]; let mut t2: U32x4; let t3: U32x4 = enc_keys[14 - 1]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x40>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[14] = t1; dec_keys[14] = t1; } Aes256 { key: Aes256KeySchedule { encrypt_keys: enc_keys, decrypt_keys: dec_keys, } , } } }
    }
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aesenc_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenc_si128)\n\n\n * `AESENC xmm, xmm`\n </li>\n <li>\n\n [**`_mm_aesenclast_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenclast_si128)\n\n\n * `AESENCLAST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn encrypt_many<const N: usize>(&self, blocks: [U8x16; N]) -> [U8x16; N]
    where
        ArrayUnrolledOps: UnrollableArraySize<N>,
    {
        select_impl_block! { scalar { use aes::cipher::{BlockEncrypt, generic_array::GenericArray}; // TODO: support ParBlocks
        blocks.array_map(#[inline(always)] |block| { let mut block = GenericArray::from(block.as_array()); (*self.key).encrypt_block(&mut block); U8x16::from(<[u8; 16]>::from(block)) }) } avx2 { let encrypt_keys = &self.key.encrypt_keys; let mut blocks = blocks.array_map( #[inline(always)] |block| (U32x4::from(block) ^ encrypt_keys[0]).0 ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[1].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[2].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[3].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[4].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[5].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[6].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[7].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[8].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[9].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[10].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[11].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[12].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[13].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenclast_si128 (block, encrypt_keys[14].0) ); blocks.array_map(#[inline(always)] |block| U8x16(block)) } }
    }
}
impl crate::AesBlockCipherDecrypt for Aes256 {
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aesdec_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesdec_si128)\n\n\n * `AESDEC xmm, xmm`\n </li>\n <li>\n\n [**`_mm_aesdeclast_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesdeclast_si128)\n\n\n * `AESDECLAST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn decrypt_many<const N: usize>(&self, blocks: [U8x16; N]) -> [U8x16; N]
    where
        ArrayUnrolledOps: UnrollableArraySize<N>,
    {
        select_impl_block! { scalar { use aes::cipher::{BlockDecrypt, generic_array::GenericArray}; // TODO: support ParBlocks
        blocks.array_map(#[inline(always)] |block| { let mut block = GenericArray::from(block.as_array()); (*self.key).decrypt_block(&mut block); U8x16::from(<[u8; 16]>::from(block)) }) } avx2 { let mut blocks = blocks.array_map( #[inline(always)] |block| (U32x4::from(block) ^ self.key.decrypt_keys[14]).0 ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[13].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[12].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[11].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[10].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[9].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[8].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[7].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[6].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[5].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[4].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[3].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[2].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdec_si128 (block, self.key.decrypt_keys[1].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesdeclast_si128 (block, self.key.decrypt_keys[0].0) ); blocks.array_map(#[inline(always)] |block| U8x16(block)) } }
    }
}
impl From<Aes256> for Aes256EncryptOnly {
    #[doc = "\n # Avx2"]
    #[inline(always)]
    fn from(aes: Aes256) -> Aes256EncryptOnly {
        select_impl_block! { scalar { Aes256EncryptOnly { key: aes.key, } } avx2 { Aes256EncryptOnly { key: aes.key.encrypt_keys, } } }
    }
}
#[doc = "A key-scheduled Aes256 block cipher which can only encrypt blocks."]
#[derive(Clone)]
pub struct Aes256EncryptOnly {
    key: Aes256EncryptOnlyKeySchedule,
}
impl std::fmt::Debug for Aes256EncryptOnly {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "Aes256EncryptOnly(...)")
    }
}
impl crate::AesBlockCipher for Aes256EncryptOnly {
    type Key = U8x32;
    type EncryptOnly = Aes256EncryptOnly;
    const BLOCK_COUNT_HINT: usize = {
        select_impl_block! { scalar { 8 } avx2 { #[cfg(vectoreyes_target_cpu="skylake")] { 4 } #[cfg(vectoreyes_target_cpu="skylake-avx512")] { 4 } #[cfg(vectoreyes_target_cpu="cascadelake")] { 4 } #[cfg(vectoreyes_target_cpu="znver1")] { 4 } #[cfg(vectoreyes_target_cpu="znver2")] { 4 } #[cfg(vectoreyes_target_cpu="znver3")] { 4 } #[cfg(not(any( vectoreyes_target_cpu="skylake", vectoreyes_target_cpu="skylake-avx512", vectoreyes_target_cpu="cascadelake", vectoreyes_target_cpu="znver1", vectoreyes_target_cpu="znver2", vectoreyes_target_cpu="znver3", )))] { 8 } } }
    };
    const FIXED_KEY: Self = Self {
        key: {
            select_impl_block! { scalar { Aes256KeySchedule::Fixed } avx2 { [ U32x4::from_array([1375551388, 3472045213, 1992838357, 1854752656]), U32x4::from_array([3463058199, 104013108, 680888422, 2141383227]), U32x4::from_array([3006203162, 2111582599, 185892178, 1704540866]), U32x4::from_array([2206503730, 2243399174, 2904688224, 3531664475]), U32x4::from_array([2325399766, 4148623697, 4233266179, 2580266689]), U32x4::from_array([1838890314, 3895243596, 1158591788, 2542688631]), U32x4::from_array([2131939609, 2287216712, 1946594379, 3989639818]), U32x4::from_array([942076980, 3490334584, 2500129364, 42642211]), U32x4::from_array([1499836275, 3509684027, 2771856240, 1224358394]), U32x4::from_array([1779164697, 3120669025, 788878133, 764409878]), U32x4::from_array([515759138, 3482082073, 1790668905, 574704019]), U32x4::from_array([4186496453, 1133083812, 1821126545, 1090770823]), U32x4::from_array([155100940, 3333618709, 2886306940, 2387133935]), U32x4::from_array([3772414746, 2740127678, 3487508527, 2396848040]), U32x4::from_array([3408339290, 227925327, 2711376179, 802460892]), ] } }
        },
    };
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aeskeygenassist_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aeskeygenassist_si128)\n\n\n * `AESKEYGENASSIST xmm, xmm, imm8`\n </li>\n </ul>"]
    #[inline(always)]
    fn new_with_key(key: U8x32) -> Self {
        select_impl_block! { scalar { use aes::cipher::{KeyInit, generic_array::GenericArray}; let key_bytes = key.as_array(); Aes256EncryptOnly { key: aes::Aes256::new(&GenericArray::from(key_bytes)).into(), } } avx2 { use crate::SimdBase32; use crate::SimdBase8; let mut enc_keys: [U32x4; 15] = Default::default(); let key: [U8x16; 2] = key.into(); let k1: U32x4 = key[0].into(); let k2: U32x4 = key[1].into(); enc_keys[0] = k1; enc_keys[1] = k2; { let mut t1: U32x4 = enc_keys[0]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[1]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x01>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[2] = t1; t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[3] = t3; } { let mut t1: U32x4 = enc_keys[2]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[3]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x02>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[4] = t1; t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[5] = t3; } { let mut t1: U32x4 = enc_keys[4]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[5]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x04>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[6] = t1; t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[7] = t3; } { let mut t1: U32x4 = enc_keys[6]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[7]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x08>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[8] = t1; t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[9] = t3; } { let mut t1: U32x4 = enc_keys[8]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[9]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x10>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[10] = t1; t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[11] = t3; } { let mut t1: U32x4 = enc_keys[10]; let mut t2: U32x4; let mut t3: U32x4 = enc_keys[11]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x20>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[12] = t1; t4 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x00>(t1.0)); t2 = t4.shuffle::<2, 2, 2, 2>(); t4 = U8x16::from(t3).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t3 ^= t4; t3 ^= t2; enc_keys[13] = t3; } // last round
        { let mut t1: U32x4 = enc_keys[14 - 2]; let mut t2: U32x4; let t3: U32x4 = enc_keys[14 - 1]; let mut t4: U32x4; t2 = U32x4( avx2::_mm_aeskeygenassist_si128 ::<0x40>(t3.0)); t2 = t2.shuffle::<3, 3, 3, 3>(); t4 = U8x16::from(t1).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t4 = U8x16::from(t4).shift_bytes_left::<0x4>().into(); t1 ^= t4; t1 ^= t2; enc_keys[14] = t1; } Aes256EncryptOnly { key: enc_keys , } } }
    }
    #[doc = "\n # Avx2\n <ul>\n <li>\n\n [**`_mm_aesenc_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenc_si128)\n\n\n * `AESENC xmm, xmm`\n </li>\n <li>\n\n [**`_mm_aesenclast_si128`**](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenclast_si128)\n\n\n * `AESENCLAST xmm, xmm`\n </li>\n </ul>"]
    #[inline(always)]
    fn encrypt_many<const N: usize>(&self, blocks: [U8x16; N]) -> [U8x16; N]
    where
        ArrayUnrolledOps: UnrollableArraySize<N>,
    {
        select_impl_block! { scalar { use aes::cipher::{BlockEncrypt, generic_array::GenericArray}; // TODO: support ParBlocks
        blocks.array_map(#[inline(always)] |block| { let mut block = GenericArray::from(block.as_array()); (*self.key).encrypt_block(&mut block); U8x16::from(<[u8; 16]>::from(block)) }) } avx2 { let encrypt_keys = &self.key; let mut blocks = blocks.array_map( #[inline(always)] |block| (U32x4::from(block) ^ encrypt_keys[0]).0 ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[1].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[2].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[3].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[4].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[5].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[6].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[7].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[8].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[9].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[10].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[11].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[12].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenc_si128 (block, encrypt_keys[13].0) ); blocks = blocks.array_map( #[inline(always)] |block| avx2::_mm_aesenclast_si128 (block, encrypt_keys[14].0) ); blocks.array_map(#[inline(always)] |block| U8x16(block)) } }
    }
} // Implement the intrinsics
select_impl! { scalar { // Scalar has no intrinsics
} avx2 { mod avx2 { #![allow(non_upper_case_globals, non_snake_case)] #[inline(always)] pub(super) fn _mm256_add_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_add_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_add_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_add_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_add_epi64( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_add_epi64(a, b) } } #[inline(always)] pub(super) fn _mm256_add_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_add_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_adds_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_adds_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_adds_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_adds_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_adds_epu16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_adds_epu16(a, b) } } #[inline(always)] pub(super) fn _mm256_adds_epu8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_adds_epu8(a, b) } } #[inline(always)] pub(super) fn _mm256_and_si256( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_and_si256(a, b) } } #[inline(always)] pub(super) fn _mm256_andnot_si256( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_andnot_si256(a, b) } } #[inline(always)] pub(super) fn _mm256_blend_epi32<const B7: bool, const B6: bool, const B5: bool, const B4: bool, const B3: bool, const B2: bool, const B1: bool, const B0: bool>( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_blend_epi32 => (a, b, @@ [0..256] (((B0 as u8) << 0) | ((B1 as u8) << 1) | ((B2 as u8) << 2) | ((B3 as u8) << 3) | ((B4 as u8) << 4) | ((B5 as u8) << 5) | ((B6 as u8) << 6) | ((B7 as u8) << 7)) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm256_broadcastb_epi8( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_broadcastb_epi8(a) } } #[inline(always)] pub(super) fn _mm256_broadcastd_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_broadcastd_epi32(a) } } #[inline(always)] pub(super) fn _mm256_broadcastq_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_broadcastq_epi64(a) } } #[inline(always)] pub(super) fn _mm256_broadcastw_epi16( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_broadcastw_epi16(a) } } #[inline(always)] pub(super) fn _mm256_cmpeq_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cmpeq_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_cmpeq_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cmpeq_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_cmpeq_epi64( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cmpeq_epi64(a, b) } } #[inline(always)] pub(super) fn _mm256_cmpeq_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cmpeq_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_cmpgt_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cmpgt_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_cmpgt_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cmpgt_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_cmpgt_epi64( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cmpgt_epi64(a, b) } } #[inline(always)] pub(super) fn _mm256_cmpgt_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cmpgt_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_cvtepi16_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepi16_epi32(a) } } #[inline(always)] pub(super) fn _mm256_cvtepi16_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepi16_epi64(a) } } #[inline(always)] pub(super) fn _mm256_cvtepi32_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepi32_epi64(a) } } #[inline(always)] pub(super) fn _mm256_cvtepi8_epi16( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepi8_epi16(a) } } #[inline(always)] pub(super) fn _mm256_cvtepi8_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepi8_epi32(a) } } #[inline(always)] pub(super) fn _mm256_cvtepi8_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepi8_epi64(a) } } #[inline(always)] pub(super) fn _mm256_cvtepu16_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepu16_epi32(a) } } #[inline(always)] pub(super) fn _mm256_cvtepu16_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepu16_epi64(a) } } #[inline(always)] pub(super) fn _mm256_cvtepu32_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepu32_epi64(a) } } #[inline(always)] pub(super) fn _mm256_cvtepu8_epi16( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepu8_epi16(a) } } #[inline(always)] pub(super) fn _mm256_cvtepu8_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepu8_epi32(a) } } #[inline(always)] pub(super) fn _mm256_cvtepu8_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_cvtepu8_epi64(a) } } #[inline(always)] pub(super) fn _mm256_extract_epi16<const index: usize>( a: ::std::arch::x86_64::__m256i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_extract_epi16 => (a, @@ [0..16] index as i32)) } } #[inline(always)] pub(super) fn _mm256_extract_epi32<const index: usize>( a: ::std::arch::x86_64::__m256i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_extract_epi32 => (a, @@ [0..8] index as i32)) } } #[inline(always)] pub(super) fn _mm256_extract_epi64<const index: usize>( a: ::std::arch::x86_64::__m256i, ) -> i64 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_extract_epi64 => (a, @@ [0..4] index as i32)) } } #[inline(always)] pub(super) fn _mm256_extract_epi8<const index: usize>( a: ::std::arch::x86_64::__m256i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_extract_epi8 => (a, @@ [0..32] index as i32)) } } #[inline(always)] pub(super) fn _mm256_extracti128_si256<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_extracti128_si256 => (a, @@ [0..2] imm8 as i32)) } } #[inline(always)] pub(super) unsafe fn _mm256_i32gather_epi32<const scale: usize>( base_addr: *const i32, vindex: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { constify_imm!(::std::arch::x86_64::_mm256_i32gather_epi32 => (base_addr, vindex, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm256_i32gather_epi64<const scale: usize>( base_addr: *const i64, vindex: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { constify_imm!(::std::arch::x86_64::_mm256_i32gather_epi64 => (base_addr, vindex, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm256_i64gather_epi32<const scale: usize>( base_addr: *const i32, vindex: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m128i { constify_imm!(::std::arch::x86_64::_mm256_i64gather_epi32 => (base_addr, vindex, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm256_i64gather_epi64<const scale: usize>( base_addr: *const i64, vindex: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { constify_imm!(::std::arch::x86_64::_mm256_i64gather_epi64 => (base_addr, vindex, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm256_loadu_si256( mem_addr: *const ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { ::std::arch::x86_64::_mm256_loadu_si256(mem_addr) } #[inline(always)] pub(super) unsafe fn _mm256_mask_i32gather_epi32<const scale: usize>( src: ::std::arch::x86_64::__m256i, base_addr: *const i32, vindex: ::std::arch::x86_64::__m256i, mask: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { constify_imm!(::std::arch::x86_64::_mm256_mask_i32gather_epi32 => (src, base_addr, vindex, mask, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm256_mask_i32gather_epi64<const scale: usize>( src: ::std::arch::x86_64::__m256i, base_addr: *const i64, vindex: ::std::arch::x86_64::__m128i, mask: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { constify_imm!(::std::arch::x86_64::_mm256_mask_i32gather_epi64 => (src, base_addr, vindex, mask, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm256_mask_i64gather_epi32<const scale: usize>( src: ::std::arch::x86_64::__m128i, base_addr: *const i32, vindex: ::std::arch::x86_64::__m256i, mask: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { constify_imm!(::std::arch::x86_64::_mm256_mask_i64gather_epi32 => (src, base_addr, vindex, mask, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm256_mask_i64gather_epi64<const scale: usize>( src: ::std::arch::x86_64::__m256i, base_addr: *const i64, vindex: ::std::arch::x86_64::__m256i, mask: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { constify_imm!(::std::arch::x86_64::_mm256_mask_i64gather_epi64 => (src, base_addr, vindex, mask, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) fn _mm256_max_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_max_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_max_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_max_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_max_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_max_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_max_epu16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_max_epu16(a, b) } } #[inline(always)] pub(super) fn _mm256_max_epu32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_max_epu32(a, b) } } #[inline(always)] pub(super) fn _mm256_max_epu8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_max_epu8(a, b) } } #[inline(always)] pub(super) fn _mm256_min_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_min_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_min_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_min_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_min_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_min_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_min_epu16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_min_epu16(a, b) } } #[inline(always)] pub(super) fn _mm256_min_epu32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_min_epu32(a, b) } } #[inline(always)] pub(super) fn _mm256_min_epu8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_min_epu8(a, b) } } #[inline(always)] pub(super) fn _mm256_movemask_epi8( a: ::std::arch::x86_64::__m256i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_movemask_epi8(a) } } #[inline(always)] pub(super) fn _mm256_mul_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_mul_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_mul_epu32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_mul_epu32(a, b) } } #[inline(always)] pub(super) fn _mm256_or_si256( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_or_si256(a, b) } } #[inline(always)] pub(super) fn _mm256_permute4x64_epi64<const I3: usize, const I2: usize, const I1: usize, const I0: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_permute4x64_epi64 => (a, @@ [0..256] ((I3 << 6) | (I2 << 4) | (I1 << 2) | I0) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm256_set1_epi16( a: i16, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set1_epi16(a) } } #[inline(always)] pub(super) fn _mm256_set1_epi32( a: i32, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set1_epi32(a) } } #[inline(always)] pub(super) fn _mm256_set1_epi64x( a: i64, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set1_epi64x(a) } } #[inline(always)] pub(super) fn _mm256_set1_epi8( a: i8, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set1_epi8(a) } } #[inline(always)] pub(super) fn _mm256_set_epi16( e15: i16, e14: i16, e13: i16, e12: i16, e11: i16, e10: i16, e9: i16, e8: i16, e7: i16, e6: i16, e5: i16, e4: i16, e3: i16, e2: i16, e1: i16, e0: i16, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set_epi16(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0) } } #[inline(always)] pub(super) fn _mm256_set_epi32( e7: i32, e6: i32, e5: i32, e4: i32, e3: i32, e2: i32, e1: i32, e0: i32, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set_epi32(e7, e6, e5, e4, e3, e2, e1, e0) } } #[inline(always)] pub(super) fn _mm256_set_epi64x( e3: i64, e2: i64, e1: i64, e0: i64, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set_epi64x(e3, e2, e1, e0) } } #[inline(always)] pub(super) fn _mm256_set_epi8( e31: i8, e30: i8, e29: i8, e28: i8, e27: i8, e26: i8, e25: i8, e24: i8, e23: i8, e22: i8, e21: i8, e20: i8, e19: i8, e18: i8, e17: i8, e16: i8, e15: i8, e14: i8, e13: i8, e12: i8, e11: i8, e10: i8, e9: i8, e8: i8, e7: i8, e6: i8, e5: i8, e4: i8, e3: i8, e2: i8, e1: i8, e0: i8, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set_epi8(e31, e30, e29, e28, e27, e26, e25, e24, e23, e22, e21, e20, e19, e18, e17, e16, e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0) } } #[inline(always)] pub(super) fn _mm256_set_m128i( hi: ::std::arch::x86_64::__m128i, lo: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_set_m128i(hi, lo) } } #[inline(always)] pub(super) fn _mm256_shuffle_epi32<const I3: usize, const I2: usize, const I1: usize, const I0: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_shuffle_epi32 => (a, @@ [0..256] ((I3 << 6) | (I2 << 4) | (I1 << 2) | I0) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm256_shuffle_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_shuffle_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_shufflehi_epi16<const I3: usize, const I2: usize, const I1: usize, const I0: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_shufflehi_epi16 => (a, @@ [0..256] ((I3 << 6) | (I2 << 4) | (I1 << 2) | I0) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm256_shufflelo_epi16<const I3: usize, const I2: usize, const I1: usize, const I0: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_shufflelo_epi16 => (a, @@ [0..256] ((I3 << 6) | (I2 << 4) | (I1 << 2) | I0) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm256_sll_epi16( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sll_epi16(a, count) } } #[inline(always)] pub(super) fn _mm256_sll_epi32( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sll_epi32(a, count) } } #[inline(always)] pub(super) fn _mm256_sll_epi64( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sll_epi64(a, count) } } #[inline(always)] pub(super) fn _mm256_slli_epi16<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_slli_epi16 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_slli_epi32<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_slli_epi32 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_slli_epi64<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_slli_epi64 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_slli_si256<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_slli_si256 => (a, @@ [0..32] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_sllv_epi32( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sllv_epi32(a, count) } } #[inline(always)] pub(super) fn _mm256_sllv_epi64( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sllv_epi64(a, count) } } #[inline(always)] pub(super) fn _mm256_sra_epi16( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sra_epi16(a, count) } } #[inline(always)] pub(super) fn _mm256_sra_epi32( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sra_epi32(a, count) } } #[inline(always)] pub(super) fn _mm256_srai_epi16<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_srai_epi16 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_srai_epi32<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_srai_epi32 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_srav_epi32( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_srav_epi32(a, count) } } #[inline(always)] pub(super) fn _mm256_srl_epi16( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_srl_epi16(a, count) } } #[inline(always)] pub(super) fn _mm256_srl_epi32( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_srl_epi32(a, count) } } #[inline(always)] pub(super) fn _mm256_srl_epi64( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_srl_epi64(a, count) } } #[inline(always)] pub(super) fn _mm256_srli_epi16<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_srli_epi16 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_srli_epi32<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_srli_epi32 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_srli_epi64<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_srli_epi64 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_srli_si256<const imm8: usize>( a: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm256_srli_si256 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm256_srlv_epi32( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_srlv_epi32(a, count) } } #[inline(always)] pub(super) fn _mm256_srlv_epi64( a: ::std::arch::x86_64::__m256i, count: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_srlv_epi64(a, count) } } #[inline(always)] pub(super) unsafe fn _mm256_storeu_si256( mem_addr: *mut ::std::arch::x86_64::__m256i, a: ::std::arch::x86_64::__m256i, ) { ::std::arch::x86_64::_mm256_storeu_si256(mem_addr, a) } #[inline(always)] pub(super) fn _mm256_sub_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sub_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_sub_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sub_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_sub_epi64( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sub_epi64(a, b) } } #[inline(always)] pub(super) fn _mm256_sub_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_sub_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_subs_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_subs_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_subs_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_subs_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_subs_epu16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_subs_epu16(a, b) } } #[inline(always)] pub(super) fn _mm256_subs_epu8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_subs_epu8(a, b) } } #[inline(always)] pub(super) fn _mm256_testz_si256( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_testz_si256(a, b) } } #[inline(always)] pub(super) fn _mm256_unpackhi_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_unpackhi_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_unpackhi_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_unpackhi_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_unpackhi_epi64( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_unpackhi_epi64(a, b) } } #[inline(always)] pub(super) fn _mm256_unpackhi_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_unpackhi_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_unpacklo_epi16( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_unpacklo_epi16(a, b) } } #[inline(always)] pub(super) fn _mm256_unpacklo_epi32( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_unpacklo_epi32(a, b) } } #[inline(always)] pub(super) fn _mm256_unpacklo_epi64( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_unpacklo_epi64(a, b) } } #[inline(always)] pub(super) fn _mm256_unpacklo_epi8( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_unpacklo_epi8(a, b) } } #[inline(always)] pub(super) fn _mm256_xor_si256( a: ::std::arch::x86_64::__m256i, b: ::std::arch::x86_64::__m256i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_xor_si256(a, b) } } #[inline(always)] pub(super) fn _mm256_zextsi128_si256( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m256i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm256_zextsi128_si256(a) } } #[inline(always)] pub(super) fn _mm_add_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_add_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_add_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_add_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_add_epi64( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_add_epi64(a, b) } } #[inline(always)] pub(super) fn _mm_add_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_add_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_adds_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_adds_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_adds_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_adds_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_adds_epu16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_adds_epu16(a, b) } } #[inline(always)] pub(super) fn _mm_adds_epu8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_adds_epu8(a, b) } } #[inline(always)] pub(super) fn _mm_aesdec_si128( a: ::std::arch::x86_64::__m128i, RoundKey: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_aesdec_si128(a, RoundKey) } } #[inline(always)] pub(super) fn _mm_aesdeclast_si128( a: ::std::arch::x86_64::__m128i, RoundKey: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_aesdeclast_si128(a, RoundKey) } } #[inline(always)] pub(super) fn _mm_aesenc_si128( a: ::std::arch::x86_64::__m128i, RoundKey: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_aesenc_si128(a, RoundKey) } } #[inline(always)] pub(super) fn _mm_aesenclast_si128( a: ::std::arch::x86_64::__m128i, RoundKey: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_aesenclast_si128(a, RoundKey) } } #[inline(always)] pub(super) fn _mm_aesimc_si128( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_aesimc_si128(a) } } #[inline(always)] pub(super) fn _mm_aeskeygenassist_si128<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_aeskeygenassist_si128 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_and_si128( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_and_si128(a, b) } } #[inline(always)] pub(super) fn _mm_andnot_si128( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_andnot_si128(a, b) } } #[inline(always)] pub(super) fn _mm_blend_epi16<const B7: bool, const B6: bool, const B5: bool, const B4: bool, const B3: bool, const B2: bool, const B1: bool, const B0: bool>( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_blend_epi16 => (a, b, @@ [0..256] (((B0 as u8) << 0) | ((B1 as u8) << 1) | ((B2 as u8) << 2) | ((B3 as u8) << 3) | ((B4 as u8) << 4) | ((B5 as u8) << 5) | ((B6 as u8) << 6) | ((B7 as u8) << 7)) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm_blend_epi32<const B3: bool, const B2: bool, const B1: bool, const B0: bool>( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_blend_epi32 => (a, b, @@ [0..16] (((B0 as u8) << 0) | ((B1 as u8) << 1) | ((B2 as u8) << 2) | ((B3 as u8) << 3)) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm_broadcastb_epi8( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_broadcastb_epi8(a) } } #[inline(always)] pub(super) fn _mm_broadcastd_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_broadcastd_epi32(a) } } #[inline(always)] pub(super) fn _mm_broadcastq_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_broadcastq_epi64(a) } } #[inline(always)] pub(super) fn _mm_broadcastw_epi16( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_broadcastw_epi16(a) } } #[inline(always)] pub(super) fn _mm_clmulepi64_si128<const HI1: bool, const HI0: bool>( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_clmulepi64_si128 => (a, b, @@ [0..256] (((!((HI1 as u64).wrapping_sub(1))) & 0xf0) | ((!((HI0 as u64).wrapping_sub(1))) & 0x0f)) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm_cmpeq_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cmpeq_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_cmpeq_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cmpeq_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_cmpeq_epi64( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cmpeq_epi64(a, b) } } #[inline(always)] pub(super) fn _mm_cmpeq_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cmpeq_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_cmpgt_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cmpgt_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_cmpgt_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cmpgt_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_cmpgt_epi64( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cmpgt_epi64(a, b) } } #[inline(always)] pub(super) fn _mm_cmpgt_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cmpgt_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_cvtepi16_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepi16_epi32(a) } } #[inline(always)] pub(super) fn _mm_cvtepi16_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepi16_epi64(a) } } #[inline(always)] pub(super) fn _mm_cvtepi32_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepi32_epi64(a) } } #[inline(always)] pub(super) fn _mm_cvtepi8_epi16( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepi8_epi16(a) } } #[inline(always)] pub(super) fn _mm_cvtepi8_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepi8_epi32(a) } } #[inline(always)] pub(super) fn _mm_cvtepi8_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepi8_epi64(a) } } #[inline(always)] pub(super) fn _mm_cvtepu16_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepu16_epi32(a) } } #[inline(always)] pub(super) fn _mm_cvtepu16_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepu16_epi64(a) } } #[inline(always)] pub(super) fn _mm_cvtepu32_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepu32_epi64(a) } } #[inline(always)] pub(super) fn _mm_cvtepu8_epi16( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepu8_epi16(a) } } #[inline(always)] pub(super) fn _mm_cvtepu8_epi32( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepu8_epi32(a) } } #[inline(always)] pub(super) fn _mm_cvtepu8_epi64( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_cvtepu8_epi64(a) } } #[inline(always)] pub(super) fn _mm_extract_epi16<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_extract_epi16 => (a, @@ [0..8] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_extract_epi32<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_extract_epi32 => (a, @@ [0..4] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_extract_epi64<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> i64 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_extract_epi64 => (a, @@ [0..2] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_extract_epi8<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_extract_epi8 => (a, @@ [0..16] imm8 as i32)) } } #[inline(always)] pub(super) unsafe fn _mm_i32gather_epi32<const scale: usize>( base_addr: *const i32, vindex: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { constify_imm!(::std::arch::x86_64::_mm_i32gather_epi32 => (base_addr, vindex, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm_i64gather_epi64<const scale: usize>( base_addr: *const i64, vindex: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { constify_imm!(::std::arch::x86_64::_mm_i64gather_epi64 => (base_addr, vindex, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm_loadu_si128( mem_addr: *const ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { ::std::arch::x86_64::_mm_loadu_si128(mem_addr) } #[inline(always)] pub(super) unsafe fn _mm_mask_i32gather_epi32<const scale: usize>( src: ::std::arch::x86_64::__m128i, base_addr: *const i32, vindex: ::std::arch::x86_64::__m128i, mask: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { constify_imm!(::std::arch::x86_64::_mm_mask_i32gather_epi32 => (src, base_addr, vindex, mask, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) unsafe fn _mm_mask_i64gather_epi64<const scale: usize>( src: ::std::arch::x86_64::__m128i, base_addr: *const i64, vindex: ::std::arch::x86_64::__m128i, mask: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { constify_imm!(::std::arch::x86_64::_mm_mask_i64gather_epi64 => (src, base_addr, vindex, mask, @@ [[1, 2, 4, 8]] scale as i32)) } #[inline(always)] pub(super) fn _mm_max_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_max_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_max_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_max_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_max_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_max_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_max_epu16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_max_epu16(a, b) } } #[inline(always)] pub(super) fn _mm_max_epu32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_max_epu32(a, b) } } #[inline(always)] pub(super) fn _mm_max_epu8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_max_epu8(a, b) } } #[inline(always)] pub(super) fn _mm_min_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_min_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_min_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_min_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_min_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_min_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_min_epu16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_min_epu16(a, b) } } #[inline(always)] pub(super) fn _mm_min_epu32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_min_epu32(a, b) } } #[inline(always)] pub(super) fn _mm_min_epu8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_min_epu8(a, b) } } #[inline(always)] pub(super) fn _mm_movemask_epi8( a: ::std::arch::x86_64::__m128i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_movemask_epi8(a) } } #[inline(always)] pub(super) fn _mm_mul_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_mul_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_mul_epu32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_mul_epu32(a, b) } } #[inline(always)] pub(super) fn _mm_or_si128( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_or_si128(a, b) } } #[inline(always)] pub(super) fn _mm_set1_epi16( a: i16, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_set1_epi16(a) } } #[inline(always)] pub(super) fn _mm_set1_epi32( a: i32, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_set1_epi32(a) } } #[inline(always)] pub(super) fn _mm_set1_epi64x( a: i64, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_set1_epi64x(a) } } #[inline(always)] pub(super) fn _mm_set1_epi8( a: i8, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_set1_epi8(a) } } #[inline(always)] pub(super) fn _mm_set_epi16( e7: i16, e6: i16, e5: i16, e4: i16, e3: i16, e2: i16, e1: i16, e0: i16, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_set_epi16(e7, e6, e5, e4, e3, e2, e1, e0) } } #[inline(always)] pub(super) fn _mm_set_epi32( e3: i32, e2: i32, e1: i32, e0: i32, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_set_epi32(e3, e2, e1, e0) } } #[inline(always)] pub(super) fn _mm_set_epi64x( e1: i64, e0: i64, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_set_epi64x(e1, e0) } } #[inline(always)] pub(super) fn _mm_set_epi8( e15: i8, e14: i8, e13: i8, e12: i8, e11: i8, e10: i8, e9: i8, e8: i8, e7: i8, e6: i8, e5: i8, e4: i8, e3: i8, e2: i8, e1: i8, e0: i8, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_set_epi8(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0) } } #[inline(always)] pub(super) fn _mm_shuffle_epi32<const I3: usize, const I2: usize, const I1: usize, const I0: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_shuffle_epi32 => (a, @@ [0..256] ((I3 << 6) | (I2 << 4) | (I1 << 2) | I0) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm_shuffle_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_shuffle_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_shufflehi_epi16<const I3: usize, const I2: usize, const I1: usize, const I0: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_shufflehi_epi16 => (a, @@ [0..256] ((I3 << 6) | (I2 << 4) | (I1 << 2) | I0) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm_shufflelo_epi16<const I3: usize, const I2: usize, const I1: usize, const I0: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_shufflelo_epi16 => (a, @@ [0..256] ((I3 << 6) | (I2 << 4) | (I1 << 2) | I0) as i32 as i32)) } } #[inline(always)] pub(super) fn _mm_sll_epi16( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sll_epi16(a, count) } } #[inline(always)] pub(super) fn _mm_sll_epi32( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sll_epi32(a, count) } } #[inline(always)] pub(super) fn _mm_sll_epi64( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sll_epi64(a, count) } } #[inline(always)] pub(super) fn _mm_slli_epi16<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_slli_epi16 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_slli_epi32<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_slli_epi32 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_slli_epi64<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_slli_epi64 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_slli_si128<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_slli_si128 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_sllv_epi32( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sllv_epi32(a, count) } } #[inline(always)] pub(super) fn _mm_sllv_epi64( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sllv_epi64(a, count) } } #[inline(always)] pub(super) fn _mm_sra_epi16( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sra_epi16(a, count) } } #[inline(always)] pub(super) fn _mm_sra_epi32( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sra_epi32(a, count) } } #[inline(always)] pub(super) fn _mm_srai_epi16<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_srai_epi16 => (a, @@ [0..32] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_srai_epi32<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_srai_epi32 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_srav_epi32( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_srav_epi32(a, count) } } #[inline(always)] pub(super) fn _mm_srl_epi16( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_srl_epi16(a, count) } } #[inline(always)] pub(super) fn _mm_srl_epi32( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_srl_epi32(a, count) } } #[inline(always)] pub(super) fn _mm_srl_epi64( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_srl_epi64(a, count) } } #[inline(always)] pub(super) fn _mm_srli_epi16<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_srli_epi16 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_srli_epi32<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_srli_epi32 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_srli_epi64<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_srli_epi64 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_srli_si128<const imm8: usize>( a: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { constify_imm!(::std::arch::x86_64::_mm_srli_si128 => (a, @@ [0..256] imm8 as i32)) } } #[inline(always)] pub(super) fn _mm_srlv_epi32( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_srlv_epi32(a, count) } } #[inline(always)] pub(super) fn _mm_srlv_epi64( a: ::std::arch::x86_64::__m128i, count: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_srlv_epi64(a, count) } } #[inline(always)] pub(super) unsafe fn _mm_storeu_si128( mem_addr: *mut ::std::arch::x86_64::__m128i, a: ::std::arch::x86_64::__m128i, ) { ::std::arch::x86_64::_mm_storeu_si128(mem_addr, a) } #[inline(always)] pub(super) fn _mm_sub_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sub_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_sub_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sub_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_sub_epi64( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sub_epi64(a, b) } } #[inline(always)] pub(super) fn _mm_sub_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_sub_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_subs_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_subs_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_subs_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_subs_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_subs_epu16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_subs_epu16(a, b) } } #[inline(always)] pub(super) fn _mm_subs_epu8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_subs_epu8(a, b) } } #[inline(always)] pub(super) fn _mm_testz_si128( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> i32 { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_testz_si128(a, b) } } #[inline(always)] pub(super) fn _mm_unpackhi_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_unpackhi_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_unpackhi_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_unpackhi_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_unpackhi_epi64( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_unpackhi_epi64(a, b) } } #[inline(always)] pub(super) fn _mm_unpackhi_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_unpackhi_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_unpacklo_epi16( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_unpacklo_epi16(a, b) } } #[inline(always)] pub(super) fn _mm_unpacklo_epi32( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_unpacklo_epi32(a, b) } } #[inline(always)] pub(super) fn _mm_unpacklo_epi64( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_unpacklo_epi64(a, b) } } #[inline(always)] pub(super) fn _mm_unpacklo_epi8( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_unpacklo_epi8(a, b) } } #[inline(always)] pub(super) fn _mm_xor_si128( a: ::std::arch::x86_64::__m128i, b: ::std::arch::x86_64::__m128i, ) -> ::std::arch::x86_64::__m128i { // SAFETY: we've verified that the required CPU flags are available.
unsafe { ::std::arch::x86_64::_mm_xor_si128(a, b) } } } } }
impl crate::HasVector<16> for i8 {
    type Vector = I8x16;
}
impl crate::HasVector<32> for i8 {
    type Vector = I8x32;
}
impl crate::HasVector<8> for i16 {
    type Vector = I16x8;
}
impl crate::HasVector<16> for i16 {
    type Vector = I16x16;
}
impl crate::HasVector<4> for i32 {
    type Vector = I32x4;
}
impl crate::HasVector<8> for i32 {
    type Vector = I32x8;
}
impl crate::HasVector<2> for i64 {
    type Vector = I64x2;
}
impl crate::HasVector<4> for i64 {
    type Vector = I64x4;
}
impl crate::HasVector<16> for u8 {
    type Vector = U8x16;
}
impl crate::HasVector<32> for u8 {
    type Vector = U8x32;
}
impl crate::HasVector<8> for u16 {
    type Vector = U16x8;
}
impl crate::HasVector<16> for u16 {
    type Vector = U16x16;
}
impl crate::HasVector<4> for u32 {
    type Vector = U32x4;
}
impl crate::HasVector<8> for u32 {
    type Vector = U32x8;
}
impl crate::HasVector<2> for u64 {
    type Vector = U64x2;
}
impl crate::HasVector<4> for u64 {
    type Vector = U64x4;
}
