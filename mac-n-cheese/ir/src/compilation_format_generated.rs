#![cfg_attr(rustfmt, rustfmt_skip)]
// automatically generated by the FlatBuffers compiler, do not modify


// @generated

use core::mem;
use core::cmp::Ordering;

extern crate flatbuffers;
use self::flatbuffers::{EndianScalar, Follow};

#[deprecated(since = "2.0.0", note = "Use associated constants instead. This will no longer be generated in 2021.")]
pub const ENUM_MIN_TASK_COMMUNIQUE_SENDER: u8 = 0;
#[deprecated(since = "2.0.0", note = "Use associated constants instead. This will no longer be generated in 2021.")]
pub const ENUM_MAX_TASK_COMMUNIQUE_SENDER: u8 = 1;
#[deprecated(since = "2.0.0", note = "Use associated constants instead. This will no longer be generated in 2021.")]
#[allow(non_camel_case_types)]
pub const ENUM_VALUES_TASK_COMMUNIQUE_SENDER: [TaskCommuniqueSender; 2] = [
  TaskCommuniqueSender::Prover,
  TaskCommuniqueSender::Verifier,
];

#[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash, Default)]
#[repr(transparent)]
pub struct TaskCommuniqueSender(pub u8);
#[allow(non_upper_case_globals)]
impl TaskCommuniqueSender {
  pub const Prover: Self = Self(0);
  pub const Verifier: Self = Self(1);

  pub const ENUM_MIN: u8 = 0;
  pub const ENUM_MAX: u8 = 1;
  pub const ENUM_VALUES: &'static [Self] = &[
    Self::Prover,
    Self::Verifier,
  ];
  /// Returns the variant's name or "" if unknown.
  pub fn variant_name(self) -> Option<&'static str> {
    match self {
      Self::Prover => Some("Prover"),
      Self::Verifier => Some("Verifier"),
      _ => None,
    }
  }
}
impl core::fmt::Debug for TaskCommuniqueSender {
  fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
    if let Some(name) = self.variant_name() {
      f.write_str(name)
    } else {
      f.write_fmt(format_args!("<UNKNOWN {:?}>", self.0))
    }
  }
}
impl<'a> flatbuffers::Follow<'a> for TaskCommuniqueSender {
  type Inner = Self;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    let b = flatbuffers::read_scalar_at::<u8>(buf, loc);
    Self(b)
  }
}

impl flatbuffers::Push for TaskCommuniqueSender {
    type Output = TaskCommuniqueSender;
    #[inline]
    unsafe fn push(&self, dst: &mut [u8], _written_len: usize) {
        flatbuffers::emplace_scalar::<u8>(dst, self.0);
    }
}

impl flatbuffers::EndianScalar for TaskCommuniqueSender {
  type Scalar = u8;
  #[inline]
  fn to_little_endian(self) -> u8 {
    self.0.to_le()
  }
  #[inline]
  #[allow(clippy::wrong_self_convention)]
  fn from_little_endian(v: u8) -> Self {
    let b = u8::from_le(v);
    Self(b)
  }
}

impl<'a> flatbuffers::Verifiable for TaskCommuniqueSender {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    u8::run_verifier(v, pos)
  }
}

impl flatbuffers::SimpleToVerifyInSlice for TaskCommuniqueSender {}
// struct Type, aligned to 2
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct Type(pub [u8; 2]);
impl Default for Type { 
  fn default() -> Self { 
    Self([0; 2])
  }
}
impl core::fmt::Debug for Type {
  fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
    f.debug_struct("Type")
      .field("encoding", &self.encoding())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for Type {}
impl<'a> flatbuffers::Follow<'a> for Type {
  type Inner = &'a Type;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a Type>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a Type {
  type Inner = &'a Type;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<Type>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for Type {
    type Output = Type;
    #[inline]
    unsafe fn push(&self, dst: &mut [u8], _written_len: usize) {
        let src = ::core::slice::from_raw_parts(self as *const Type as *const u8, Self::size());
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for Type {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}

impl<'a> Type {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    encoding: u16,
  ) -> Self {
    let mut s = Self([0; 2]);
    s.set_encoding(encoding);
    s
  }

  pub fn encoding(&self) -> u16 {
    let mut mem = core::mem::MaybeUninit::<<u16 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[0..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u16 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_encoding(&mut self, x: u16) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[0..].as_mut_ptr(),
        core::mem::size_of::<<u16 as EndianScalar>::Scalar>(),
      );
    }
  }

}

// struct TaskId, aligned to 4
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct TaskId(pub [u8; 4]);
impl Default for TaskId { 
  fn default() -> Self { 
    Self([0; 4])
  }
}
impl core::fmt::Debug for TaskId {
  fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
    f.debug_struct("TaskId")
      .field("id", &self.id())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for TaskId {}
impl<'a> flatbuffers::Follow<'a> for TaskId {
  type Inner = &'a TaskId;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a TaskId>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a TaskId {
  type Inner = &'a TaskId;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<TaskId>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for TaskId {
    type Output = TaskId;
    #[inline]
    unsafe fn push(&self, dst: &mut [u8], _written_len: usize) {
        let src = ::core::slice::from_raw_parts(self as *const TaskId as *const u8, Self::size());
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for TaskId {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}

impl<'a> TaskId {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    id: u32,
  ) -> Self {
    let mut s = Self([0; 4]);
    s.set_id(id);
    s
  }

  pub fn id(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[0..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_id(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[0..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

}

// struct DataChunkAddress, aligned to 8
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct DataChunkAddress(pub [u8; 32]);
impl Default for DataChunkAddress { 
  fn default() -> Self { 
    Self([0; 32])
  }
}
impl core::fmt::Debug for DataChunkAddress {
  fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
    f.debug_struct("DataChunkAddress")
      .field("start", &self.start())
      .field("length", &self.length())
      .field("hash_code", &self.hash_code())
      .field("compressed_length", &self.compressed_length())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for DataChunkAddress {}
impl<'a> flatbuffers::Follow<'a> for DataChunkAddress {
  type Inner = &'a DataChunkAddress;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a DataChunkAddress>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a DataChunkAddress {
  type Inner = &'a DataChunkAddress;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<DataChunkAddress>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for DataChunkAddress {
    type Output = DataChunkAddress;
    #[inline]
    unsafe fn push(&self, dst: &mut [u8], _written_len: usize) {
        let src = ::core::slice::from_raw_parts(self as *const DataChunkAddress as *const u8, Self::size());
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for DataChunkAddress {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}

impl<'a> DataChunkAddress {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    start: u64,
    length: u32,
    hash_code: u64,
    compressed_length: u32,
  ) -> Self {
    let mut s = Self([0; 32]);
    s.set_start(start);
    s.set_length(length);
    s.set_hash_code(hash_code);
    s.set_compressed_length(compressed_length);
    s
  }

  pub fn start(&self) -> u64 {
    let mut mem = core::mem::MaybeUninit::<<u64 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[0..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u64 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_start(&mut self, x: u64) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[0..].as_mut_ptr(),
        core::mem::size_of::<<u64 as EndianScalar>::Scalar>(),
      );
    }
  }

  pub fn length(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[8..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_length(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[8..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

  pub fn hash_code(&self) -> u64 {
    let mut mem = core::mem::MaybeUninit::<<u64 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[16..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u64 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_hash_code(&mut self, x: u64) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[16..].as_mut_ptr(),
        core::mem::size_of::<<u64 as EndianScalar>::Scalar>(),
      );
    }
  }

  pub fn compressed_length(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[24..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_compressed_length(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[24..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

}

// struct Shape, aligned to 4
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct Shape(pub [u8; 8]);
impl Default for Shape { 
  fn default() -> Self { 
    Self([0; 8])
  }
}
impl core::fmt::Debug for Shape {
  fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
    f.debug_struct("Shape")
      .field("ty", &self.ty())
      .field("count", &self.count())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for Shape {}
impl<'a> flatbuffers::Follow<'a> for Shape {
  type Inner = &'a Shape;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a Shape>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a Shape {
  type Inner = &'a Shape;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<Shape>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for Shape {
    type Output = Shape;
    #[inline]
    unsafe fn push(&self, dst: &mut [u8], _written_len: usize) {
        let src = ::core::slice::from_raw_parts(self as *const Shape as *const u8, Self::size());
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for Shape {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}

impl<'a> Shape {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    ty: &Type,
    count: u32,
  ) -> Self {
    let mut s = Self([0; 8]);
    s.set_ty(ty);
    s.set_count(count);
    s
  }

  pub fn ty(&self) -> &Type {
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid struct in this slot
    unsafe { &*(self.0[0..].as_ptr() as *const Type) }
  }

  #[allow(clippy::identity_op)]
  pub fn set_ty(&mut self, x: &Type) {
    self.0[0..0 + 2].copy_from_slice(&x.0)
  }

  pub fn count(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[4..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_count(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[4..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

}

// struct TaskCommunicationRound, aligned to 4
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct TaskCommunicationRound(pub [u8; 8]);
impl Default for TaskCommunicationRound { 
  fn default() -> Self { 
    Self([0; 8])
  }
}
impl core::fmt::Debug for TaskCommunicationRound {
  fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
    f.debug_struct("TaskCommunicationRound")
      .field("size_a", &self.size_a())
      .field("size_b", &self.size_b())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for TaskCommunicationRound {}
impl<'a> flatbuffers::Follow<'a> for TaskCommunicationRound {
  type Inner = &'a TaskCommunicationRound;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a TaskCommunicationRound>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a TaskCommunicationRound {
  type Inner = &'a TaskCommunicationRound;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<TaskCommunicationRound>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for TaskCommunicationRound {
    type Output = TaskCommunicationRound;
    #[inline]
    unsafe fn push(&self, dst: &mut [u8], _written_len: usize) {
        let src = ::core::slice::from_raw_parts(self as *const TaskCommunicationRound as *const u8, Self::size());
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for TaskCommunicationRound {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}

impl<'a> TaskCommunicationRound {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    size_a: u32,
    size_b: u32,
  ) -> Self {
    let mut s = Self([0; 8]);
    s.set_size_a(size_a);
    s.set_size_b(size_b);
    s
  }

  pub fn size_a(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[0..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_size_a(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[0..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

  pub fn size_b(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[4..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_size_b(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[4..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

}

// struct TaskInput, aligned to 4
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct TaskInput(pub [u8; 16]);
impl Default for TaskInput { 
  fn default() -> Self { 
    Self([0; 16])
  }
}
impl core::fmt::Debug for TaskInput {
  fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
    f.debug_struct("TaskInput")
      .field("ty", &self.ty())
      .field("source", &self.source())
      .field("start", &self.start())
      .field("end", &self.end())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for TaskInput {}
impl<'a> flatbuffers::Follow<'a> for TaskInput {
  type Inner = &'a TaskInput;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a TaskInput>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a TaskInput {
  type Inner = &'a TaskInput;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<TaskInput>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for TaskInput {
    type Output = TaskInput;
    #[inline]
    unsafe fn push(&self, dst: &mut [u8], _written_len: usize) {
        let src = ::core::slice::from_raw_parts(self as *const TaskInput as *const u8, Self::size());
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for TaskInput {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}

impl<'a> TaskInput {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    ty: &Type,
    source: &TaskId,
    start: u32,
    end: u32,
  ) -> Self {
    let mut s = Self([0; 16]);
    s.set_ty(ty);
    s.set_source(source);
    s.set_start(start);
    s.set_end(end);
    s
  }

  pub fn ty(&self) -> &Type {
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid struct in this slot
    unsafe { &*(self.0[0..].as_ptr() as *const Type) }
  }

  #[allow(clippy::identity_op)]
  pub fn set_ty(&mut self, x: &Type) {
    self.0[0..0 + 2].copy_from_slice(&x.0)
  }

  pub fn source(&self) -> &TaskId {
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid struct in this slot
    unsafe { &*(self.0[4..].as_ptr() as *const TaskId) }
  }

  #[allow(clippy::identity_op)]
  pub fn set_source(&mut self, x: &TaskId) {
    self.0[4..4 + 4].copy_from_slice(&x.0)
  }

  pub fn start(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[8..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_start(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[8..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

  pub fn end(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[12..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_end(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[12..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

}

// struct TaskInputTributary, aligned to 4
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct TaskInputTributary(pub [u8; 12]);
impl Default for TaskInputTributary { 
  fn default() -> Self { 
    Self([0; 12])
  }
}
impl core::fmt::Debug for TaskInputTributary {
  fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
    f.debug_struct("TaskInputTributary")
      .field("source", &self.source())
      .field("start", &self.start())
      .field("end", &self.end())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for TaskInputTributary {}
impl<'a> flatbuffers::Follow<'a> for TaskInputTributary {
  type Inner = &'a TaskInputTributary;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a TaskInputTributary>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a TaskInputTributary {
  type Inner = &'a TaskInputTributary;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<TaskInputTributary>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for TaskInputTributary {
    type Output = TaskInputTributary;
    #[inline]
    unsafe fn push(&self, dst: &mut [u8], _written_len: usize) {
        let src = ::core::slice::from_raw_parts(self as *const TaskInputTributary as *const u8, Self::size());
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for TaskInputTributary {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}

impl<'a> TaskInputTributary {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    source: &TaskId,
    start: u32,
    end: u32,
  ) -> Self {
    let mut s = Self([0; 12]);
    s.set_source(source);
    s.set_start(start);
    s.set_end(end);
    s
  }

  pub fn source(&self) -> &TaskId {
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid struct in this slot
    unsafe { &*(self.0[0..].as_ptr() as *const TaskId) }
  }

  #[allow(clippy::identity_op)]
  pub fn set_source(&mut self, x: &TaskId) {
    self.0[0..0 + 4].copy_from_slice(&x.0)
  }

  pub fn start(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[4..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_start(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[4..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

  pub fn end(&self) -> u32 {
    let mut mem = core::mem::MaybeUninit::<<u32 as EndianScalar>::Scalar>::uninit();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    EndianScalar::from_little_endian(unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[8..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
      mem.assume_init()
    })
  }

  pub fn set_end(&mut self, x: u32) {
    let x_le = x.to_little_endian();
    // Safety:
    // Created from a valid Table for this object
    // Which contains a valid value in this slot
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const _ as *const u8,
        self.0[8..].as_mut_ptr(),
        core::mem::size_of::<<u32 as EndianScalar>::Scalar>(),
      );
    }
  }

}

pub enum TaskPrototypeOffset {}
#[derive(Copy, Clone, PartialEq)]

pub struct TaskPrototype<'a> {
  pub _tab: flatbuffers::Table<'a>,
}

impl<'a> flatbuffers::Follow<'a> for TaskPrototype<'a> {
  type Inner = TaskPrototype<'a>;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    Self { _tab: flatbuffers::Table::new(buf, loc) }
  }
}

impl<'a> TaskPrototype<'a> {
  pub const VT_KIND_ENCODING: flatbuffers::VOffsetT = 4;
  pub const VT_DATA: flatbuffers::VOffsetT = 6;
  pub const VT_PARTY_A: flatbuffers::VOffsetT = 8;
  pub const VT_ROUNDS: flatbuffers::VOffsetT = 10;
  pub const VT_SINGLE_ARRAY_INPUTS: flatbuffers::VOffsetT = 12;
  pub const VT_MULTI_ARRAY_INPUTS: flatbuffers::VOffsetT = 14;
  pub const VT_OUTPUTS: flatbuffers::VOffsetT = 16;
  pub const VT_NAME: flatbuffers::VOffsetT = 18;

  #[inline]
  pub unsafe fn init_from_table(table: flatbuffers::Table<'a>) -> Self {
    TaskPrototype { _tab: table }
  }
  #[allow(unused_mut)]
  pub fn create<'bldr: 'args, 'args: 'mut_bldr, 'mut_bldr>(
    _fbb: &'mut_bldr mut flatbuffers::FlatBufferBuilder<'bldr>,
    args: &'args TaskPrototypeArgs<'args>
  ) -> flatbuffers::WIPOffset<TaskPrototype<'bldr>> {
    let mut builder = TaskPrototypeBuilder::new(_fbb);
    if let Some(x) = args.name { builder.add_name(x); }
    if let Some(x) = args.outputs { builder.add_outputs(x); }
    if let Some(x) = args.multi_array_inputs { builder.add_multi_array_inputs(x); }
    if let Some(x) = args.single_array_inputs { builder.add_single_array_inputs(x); }
    if let Some(x) = args.rounds { builder.add_rounds(x); }
    if let Some(x) = args.data { builder.add_data(x); }
    builder.add_kind_encoding(args.kind_encoding);
    builder.add_party_a(args.party_a);
    builder.finish()
  }


  #[inline]
  pub fn kind_encoding(&self) -> u16 {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<u16>(TaskPrototype::VT_KIND_ENCODING, Some(0)).unwrap()}
  }
  #[inline]
  pub fn data(&self) -> &'a DataChunkAddress {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<DataChunkAddress>(TaskPrototype::VT_DATA, None).unwrap()}
  }
  #[inline]
  pub fn party_a(&self) -> TaskCommuniqueSender {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<TaskCommuniqueSender>(TaskPrototype::VT_PARTY_A, Some(TaskCommuniqueSender::Prover)).unwrap()}
  }
  #[inline]
  pub fn rounds(&self) -> flatbuffers::Vector<'a, TaskCommunicationRound> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, TaskCommunicationRound>>>(TaskPrototype::VT_ROUNDS, None).unwrap()}
  }
  #[inline]
  pub fn single_array_inputs(&self) -> flatbuffers::Vector<'a, Shape> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, Shape>>>(TaskPrototype::VT_SINGLE_ARRAY_INPUTS, None).unwrap()}
  }
  #[inline]
  pub fn multi_array_inputs(&self) -> flatbuffers::Vector<'a, Shape> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, Shape>>>(TaskPrototype::VT_MULTI_ARRAY_INPUTS, None).unwrap()}
  }
  #[inline]
  pub fn outputs(&self) -> flatbuffers::Vector<'a, Shape> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, Shape>>>(TaskPrototype::VT_OUTPUTS, None).unwrap()}
  }
  #[inline]
  pub fn name(&self) -> Option<&'a str> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<&str>>(TaskPrototype::VT_NAME, None)}
  }
}

impl flatbuffers::Verifiable for TaskPrototype<'_> {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.visit_table(pos)?
     .visit_field::<u16>("kind_encoding", Self::VT_KIND_ENCODING, false)?
     .visit_field::<DataChunkAddress>("data", Self::VT_DATA, true)?
     .visit_field::<TaskCommuniqueSender>("party_a", Self::VT_PARTY_A, false)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, TaskCommunicationRound>>>("rounds", Self::VT_ROUNDS, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, Shape>>>("single_array_inputs", Self::VT_SINGLE_ARRAY_INPUTS, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, Shape>>>("multi_array_inputs", Self::VT_MULTI_ARRAY_INPUTS, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, Shape>>>("outputs", Self::VT_OUTPUTS, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<&str>>("name", Self::VT_NAME, false)?
     .finish();
    Ok(())
  }
}
pub struct TaskPrototypeArgs<'a> {
    pub kind_encoding: u16,
    pub data: Option<&'a DataChunkAddress>,
    pub party_a: TaskCommuniqueSender,
    pub rounds: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, TaskCommunicationRound>>>,
    pub single_array_inputs: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, Shape>>>,
    pub multi_array_inputs: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, Shape>>>,
    pub outputs: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, Shape>>>,
    pub name: Option<flatbuffers::WIPOffset<&'a str>>,
}
impl<'a> Default for TaskPrototypeArgs<'a> {
  #[inline]
  fn default() -> Self {
    TaskPrototypeArgs {
      kind_encoding: 0,
      data: None, // required field
      party_a: TaskCommuniqueSender::Prover,
      rounds: None, // required field
      single_array_inputs: None, // required field
      multi_array_inputs: None, // required field
      outputs: None, // required field
      name: None,
    }
  }
}

pub struct TaskPrototypeBuilder<'a: 'b, 'b> {
  fbb_: &'b mut flatbuffers::FlatBufferBuilder<'a>,
  start_: flatbuffers::WIPOffset<flatbuffers::TableUnfinishedWIPOffset>,
}
impl<'a: 'b, 'b> TaskPrototypeBuilder<'a, 'b> {
  #[inline]
  pub fn add_kind_encoding(&mut self, kind_encoding: u16) {
    self.fbb_.push_slot::<u16>(TaskPrototype::VT_KIND_ENCODING, kind_encoding, 0);
  }
  #[inline]
  pub fn add_data(&mut self, data: &DataChunkAddress) {
    self.fbb_.push_slot_always::<&DataChunkAddress>(TaskPrototype::VT_DATA, data);
  }
  #[inline]
  pub fn add_party_a(&mut self, party_a: TaskCommuniqueSender) {
    self.fbb_.push_slot::<TaskCommuniqueSender>(TaskPrototype::VT_PARTY_A, party_a, TaskCommuniqueSender::Prover);
  }
  #[inline]
  pub fn add_rounds(&mut self, rounds: flatbuffers::WIPOffset<flatbuffers::Vector<'b , TaskCommunicationRound>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(TaskPrototype::VT_ROUNDS, rounds);
  }
  #[inline]
  pub fn add_single_array_inputs(&mut self, single_array_inputs: flatbuffers::WIPOffset<flatbuffers::Vector<'b , Shape>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(TaskPrototype::VT_SINGLE_ARRAY_INPUTS, single_array_inputs);
  }
  #[inline]
  pub fn add_multi_array_inputs(&mut self, multi_array_inputs: flatbuffers::WIPOffset<flatbuffers::Vector<'b , Shape>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(TaskPrototype::VT_MULTI_ARRAY_INPUTS, multi_array_inputs);
  }
  #[inline]
  pub fn add_outputs(&mut self, outputs: flatbuffers::WIPOffset<flatbuffers::Vector<'b , Shape>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(TaskPrototype::VT_OUTPUTS, outputs);
  }
  #[inline]
  pub fn add_name(&mut self, name: flatbuffers::WIPOffset<&'b  str>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(TaskPrototype::VT_NAME, name);
  }
  #[inline]
  pub fn new(_fbb: &'b mut flatbuffers::FlatBufferBuilder<'a>) -> TaskPrototypeBuilder<'a, 'b> {
    let start = _fbb.start_table();
    TaskPrototypeBuilder {
      fbb_: _fbb,
      start_: start,
    }
  }
  #[inline]
  pub fn finish(self) -> flatbuffers::WIPOffset<TaskPrototype<'a>> {
    let o = self.fbb_.end_table(self.start_);
    self.fbb_.required(o, TaskPrototype::VT_DATA,"data");
    self.fbb_.required(o, TaskPrototype::VT_ROUNDS,"rounds");
    self.fbb_.required(o, TaskPrototype::VT_SINGLE_ARRAY_INPUTS,"single_array_inputs");
    self.fbb_.required(o, TaskPrototype::VT_MULTI_ARRAY_INPUTS,"multi_array_inputs");
    self.fbb_.required(o, TaskPrototype::VT_OUTPUTS,"outputs");
    flatbuffers::WIPOffset::new(o.value())
  }
}

impl core::fmt::Debug for TaskPrototype<'_> {
  fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
    let mut ds = f.debug_struct("TaskPrototype");
      ds.field("kind_encoding", &self.kind_encoding());
      ds.field("data", &self.data());
      ds.field("party_a", &self.party_a());
      ds.field("rounds", &self.rounds());
      ds.field("single_array_inputs", &self.single_array_inputs());
      ds.field("multi_array_inputs", &self.multi_array_inputs());
      ds.field("outputs", &self.outputs());
      ds.field("name", &self.name());
      ds.finish()
  }
}
pub enum MultiArrayTaskInputOffset {}
#[derive(Copy, Clone, PartialEq)]

pub struct MultiArrayTaskInput<'a> {
  pub _tab: flatbuffers::Table<'a>,
}

impl<'a> flatbuffers::Follow<'a> for MultiArrayTaskInput<'a> {
  type Inner = MultiArrayTaskInput<'a>;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    Self { _tab: flatbuffers::Table::new(buf, loc) }
  }
}

impl<'a> MultiArrayTaskInput<'a> {
  pub const VT_TY: flatbuffers::VOffsetT = 4;
  pub const VT_INPUTS: flatbuffers::VOffsetT = 6;

  #[inline]
  pub unsafe fn init_from_table(table: flatbuffers::Table<'a>) -> Self {
    MultiArrayTaskInput { _tab: table }
  }
  #[allow(unused_mut)]
  pub fn create<'bldr: 'args, 'args: 'mut_bldr, 'mut_bldr>(
    _fbb: &'mut_bldr mut flatbuffers::FlatBufferBuilder<'bldr>,
    args: &'args MultiArrayTaskInputArgs<'args>
  ) -> flatbuffers::WIPOffset<MultiArrayTaskInput<'bldr>> {
    let mut builder = MultiArrayTaskInputBuilder::new(_fbb);
    if let Some(x) = args.inputs { builder.add_inputs(x); }
    if let Some(x) = args.ty { builder.add_ty(x); }
    builder.finish()
  }


  #[inline]
  pub fn ty(&self) -> &'a Type {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<Type>(MultiArrayTaskInput::VT_TY, None).unwrap()}
  }
  #[inline]
  pub fn inputs(&self) -> flatbuffers::Vector<'a, TaskInputTributary> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, TaskInputTributary>>>(MultiArrayTaskInput::VT_INPUTS, None).unwrap()}
  }
}

impl flatbuffers::Verifiable for MultiArrayTaskInput<'_> {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.visit_table(pos)?
     .visit_field::<Type>("ty", Self::VT_TY, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, TaskInputTributary>>>("inputs", Self::VT_INPUTS, true)?
     .finish();
    Ok(())
  }
}
pub struct MultiArrayTaskInputArgs<'a> {
    pub ty: Option<&'a Type>,
    pub inputs: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, TaskInputTributary>>>,
}
impl<'a> Default for MultiArrayTaskInputArgs<'a> {
  #[inline]
  fn default() -> Self {
    MultiArrayTaskInputArgs {
      ty: None, // required field
      inputs: None, // required field
    }
  }
}

pub struct MultiArrayTaskInputBuilder<'a: 'b, 'b> {
  fbb_: &'b mut flatbuffers::FlatBufferBuilder<'a>,
  start_: flatbuffers::WIPOffset<flatbuffers::TableUnfinishedWIPOffset>,
}
impl<'a: 'b, 'b> MultiArrayTaskInputBuilder<'a, 'b> {
  #[inline]
  pub fn add_ty(&mut self, ty: &Type) {
    self.fbb_.push_slot_always::<&Type>(MultiArrayTaskInput::VT_TY, ty);
  }
  #[inline]
  pub fn add_inputs(&mut self, inputs: flatbuffers::WIPOffset<flatbuffers::Vector<'b , TaskInputTributary>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(MultiArrayTaskInput::VT_INPUTS, inputs);
  }
  #[inline]
  pub fn new(_fbb: &'b mut flatbuffers::FlatBufferBuilder<'a>) -> MultiArrayTaskInputBuilder<'a, 'b> {
    let start = _fbb.start_table();
    MultiArrayTaskInputBuilder {
      fbb_: _fbb,
      start_: start,
    }
  }
  #[inline]
  pub fn finish(self) -> flatbuffers::WIPOffset<MultiArrayTaskInput<'a>> {
    let o = self.fbb_.end_table(self.start_);
    self.fbb_.required(o, MultiArrayTaskInput::VT_TY,"ty");
    self.fbb_.required(o, MultiArrayTaskInput::VT_INPUTS,"inputs");
    flatbuffers::WIPOffset::new(o.value())
  }
}

impl core::fmt::Debug for MultiArrayTaskInput<'_> {
  fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
    let mut ds = f.debug_struct("MultiArrayTaskInput");
      ds.field("ty", &self.ty());
      ds.field("inputs", &self.inputs());
      ds.finish()
  }
}
pub enum TaskOffset {}
#[derive(Copy, Clone, PartialEq)]

pub struct Task<'a> {
  pub _tab: flatbuffers::Table<'a>,
}

impl<'a> flatbuffers::Follow<'a> for Task<'a> {
  type Inner = Task<'a>;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    Self { _tab: flatbuffers::Table::new(buf, loc) }
  }
}

impl<'a> Task<'a> {
  pub const VT_PROTOTYPE_ID: flatbuffers::VOffsetT = 4;
  pub const VT_SINGLE_ARRAY_INPUTS: flatbuffers::VOffsetT = 6;
  pub const VT_MULTI_ARRAY_INPUTS: flatbuffers::VOffsetT = 8;
  pub const VT_INFERRED_PRIORITY: flatbuffers::VOffsetT = 10;
  pub const VT_INFERRED_DEPENDENTS: flatbuffers::VOffsetT = 12;
  pub const VT_NAME: flatbuffers::VOffsetT = 14;

  #[inline]
  pub unsafe fn init_from_table(table: flatbuffers::Table<'a>) -> Self {
    Task { _tab: table }
  }
  #[allow(unused_mut)]
  pub fn create<'bldr: 'args, 'args: 'mut_bldr, 'mut_bldr>(
    _fbb: &'mut_bldr mut flatbuffers::FlatBufferBuilder<'bldr>,
    args: &'args TaskArgs<'args>
  ) -> flatbuffers::WIPOffset<Task<'bldr>> {
    let mut builder = TaskBuilder::new(_fbb);
    if let Some(x) = args.name { builder.add_name(x); }
    if let Some(x) = args.inferred_dependents { builder.add_inferred_dependents(x); }
    builder.add_inferred_priority(args.inferred_priority);
    if let Some(x) = args.multi_array_inputs { builder.add_multi_array_inputs(x); }
    if let Some(x) = args.single_array_inputs { builder.add_single_array_inputs(x); }
    builder.add_prototype_id(args.prototype_id);
    builder.finish()
  }


  #[inline]
  pub fn prototype_id(&self) -> u32 {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<u32>(Task::VT_PROTOTYPE_ID, Some(0)).unwrap()}
  }
  #[inline]
  pub fn single_array_inputs(&self) -> flatbuffers::Vector<'a, TaskInput> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, TaskInput>>>(Task::VT_SINGLE_ARRAY_INPUTS, None).unwrap()}
  }
  #[inline]
  pub fn multi_array_inputs(&self) -> flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<MultiArrayTaskInput<'a>>> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<MultiArrayTaskInput>>>>(Task::VT_MULTI_ARRAY_INPUTS, None).unwrap()}
  }
  #[inline]
  pub fn inferred_priority(&self) -> i32 {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<i32>(Task::VT_INFERRED_PRIORITY, Some(0)).unwrap()}
  }
  #[inline]
  pub fn inferred_dependents(&self) -> flatbuffers::Vector<'a, TaskId> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, TaskId>>>(Task::VT_INFERRED_DEPENDENTS, None).unwrap()}
  }
  #[inline]
  pub fn name(&self) -> Option<&'a str> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<&str>>(Task::VT_NAME, None)}
  }
}

impl flatbuffers::Verifiable for Task<'_> {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.visit_table(pos)?
     .visit_field::<u32>("prototype_id", Self::VT_PROTOTYPE_ID, false)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, TaskInput>>>("single_array_inputs", Self::VT_SINGLE_ARRAY_INPUTS, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, flatbuffers::ForwardsUOffset<MultiArrayTaskInput>>>>("multi_array_inputs", Self::VT_MULTI_ARRAY_INPUTS, true)?
     .visit_field::<i32>("inferred_priority", Self::VT_INFERRED_PRIORITY, false)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, TaskId>>>("inferred_dependents", Self::VT_INFERRED_DEPENDENTS, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<&str>>("name", Self::VT_NAME, false)?
     .finish();
    Ok(())
  }
}
pub struct TaskArgs<'a> {
    pub prototype_id: u32,
    pub single_array_inputs: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, TaskInput>>>,
    pub multi_array_inputs: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<MultiArrayTaskInput<'a>>>>>,
    pub inferred_priority: i32,
    pub inferred_dependents: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, TaskId>>>,
    pub name: Option<flatbuffers::WIPOffset<&'a str>>,
}
impl<'a> Default for TaskArgs<'a> {
  #[inline]
  fn default() -> Self {
    TaskArgs {
      prototype_id: 0,
      single_array_inputs: None, // required field
      multi_array_inputs: None, // required field
      inferred_priority: 0,
      inferred_dependents: None, // required field
      name: None,
    }
  }
}

pub struct TaskBuilder<'a: 'b, 'b> {
  fbb_: &'b mut flatbuffers::FlatBufferBuilder<'a>,
  start_: flatbuffers::WIPOffset<flatbuffers::TableUnfinishedWIPOffset>,
}
impl<'a: 'b, 'b> TaskBuilder<'a, 'b> {
  #[inline]
  pub fn add_prototype_id(&mut self, prototype_id: u32) {
    self.fbb_.push_slot::<u32>(Task::VT_PROTOTYPE_ID, prototype_id, 0);
  }
  #[inline]
  pub fn add_single_array_inputs(&mut self, single_array_inputs: flatbuffers::WIPOffset<flatbuffers::Vector<'b , TaskInput>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Task::VT_SINGLE_ARRAY_INPUTS, single_array_inputs);
  }
  #[inline]
  pub fn add_multi_array_inputs(&mut self, multi_array_inputs: flatbuffers::WIPOffset<flatbuffers::Vector<'b , flatbuffers::ForwardsUOffset<MultiArrayTaskInput<'b >>>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Task::VT_MULTI_ARRAY_INPUTS, multi_array_inputs);
  }
  #[inline]
  pub fn add_inferred_priority(&mut self, inferred_priority: i32) {
    self.fbb_.push_slot::<i32>(Task::VT_INFERRED_PRIORITY, inferred_priority, 0);
  }
  #[inline]
  pub fn add_inferred_dependents(&mut self, inferred_dependents: flatbuffers::WIPOffset<flatbuffers::Vector<'b , TaskId>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Task::VT_INFERRED_DEPENDENTS, inferred_dependents);
  }
  #[inline]
  pub fn add_name(&mut self, name: flatbuffers::WIPOffset<&'b  str>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Task::VT_NAME, name);
  }
  #[inline]
  pub fn new(_fbb: &'b mut flatbuffers::FlatBufferBuilder<'a>) -> TaskBuilder<'a, 'b> {
    let start = _fbb.start_table();
    TaskBuilder {
      fbb_: _fbb,
      start_: start,
    }
  }
  #[inline]
  pub fn finish(self) -> flatbuffers::WIPOffset<Task<'a>> {
    let o = self.fbb_.end_table(self.start_);
    self.fbb_.required(o, Task::VT_SINGLE_ARRAY_INPUTS,"single_array_inputs");
    self.fbb_.required(o, Task::VT_MULTI_ARRAY_INPUTS,"multi_array_inputs");
    self.fbb_.required(o, Task::VT_INFERRED_DEPENDENTS,"inferred_dependents");
    flatbuffers::WIPOffset::new(o.value())
  }
}

impl core::fmt::Debug for Task<'_> {
  fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
    let mut ds = f.debug_struct("Task");
      ds.field("prototype_id", &self.prototype_id());
      ds.field("single_array_inputs", &self.single_array_inputs());
      ds.field("multi_array_inputs", &self.multi_array_inputs());
      ds.field("inferred_priority", &self.inferred_priority());
      ds.field("inferred_dependents", &self.inferred_dependents());
      ds.field("name", &self.name());
      ds.finish()
  }
}
pub enum AllocationSizeOffset {}
#[derive(Copy, Clone, PartialEq)]

pub struct AllocationSize<'a> {
  pub _tab: flatbuffers::Table<'a>,
}

impl<'a> flatbuffers::Follow<'a> for AllocationSize<'a> {
  type Inner = AllocationSize<'a>;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    Self { _tab: flatbuffers::Table::new(buf, loc) }
  }
}

impl<'a> AllocationSize<'a> {
  pub const VT_TYPE_: flatbuffers::VOffsetT = 4;
  pub const VT_COUNT: flatbuffers::VOffsetT = 6;

  #[inline]
  pub unsafe fn init_from_table(table: flatbuffers::Table<'a>) -> Self {
    AllocationSize { _tab: table }
  }
  #[allow(unused_mut)]
  pub fn create<'bldr: 'args, 'args: 'mut_bldr, 'mut_bldr>(
    _fbb: &'mut_bldr mut flatbuffers::FlatBufferBuilder<'bldr>,
    args: &'args AllocationSizeArgs<'args>
  ) -> flatbuffers::WIPOffset<AllocationSize<'bldr>> {
    let mut builder = AllocationSizeBuilder::new(_fbb);
    builder.add_count(args.count);
    if let Some(x) = args.type_ { builder.add_type_(x); }
    builder.finish()
  }


  #[inline]
  pub fn type_(&self) -> Option<&'a Type> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<Type>(AllocationSize::VT_TYPE_, None)}
  }
  #[inline]
  pub fn count(&self) -> u32 {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<u32>(AllocationSize::VT_COUNT, Some(0)).unwrap()}
  }
}

impl flatbuffers::Verifiable for AllocationSize<'_> {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.visit_table(pos)?
     .visit_field::<Type>("type_", Self::VT_TYPE_, false)?
     .visit_field::<u32>("count", Self::VT_COUNT, false)?
     .finish();
    Ok(())
  }
}
pub struct AllocationSizeArgs<'a> {
    pub type_: Option<&'a Type>,
    pub count: u32,
}
impl<'a> Default for AllocationSizeArgs<'a> {
  #[inline]
  fn default() -> Self {
    AllocationSizeArgs {
      type_: None,
      count: 0,
    }
  }
}

pub struct AllocationSizeBuilder<'a: 'b, 'b> {
  fbb_: &'b mut flatbuffers::FlatBufferBuilder<'a>,
  start_: flatbuffers::WIPOffset<flatbuffers::TableUnfinishedWIPOffset>,
}
impl<'a: 'b, 'b> AllocationSizeBuilder<'a, 'b> {
  #[inline]
  pub fn add_type_(&mut self, type_: &Type) {
    self.fbb_.push_slot_always::<&Type>(AllocationSize::VT_TYPE_, type_);
  }
  #[inline]
  pub fn add_count(&mut self, count: u32) {
    self.fbb_.push_slot::<u32>(AllocationSize::VT_COUNT, count, 0);
  }
  #[inline]
  pub fn new(_fbb: &'b mut flatbuffers::FlatBufferBuilder<'a>) -> AllocationSizeBuilder<'a, 'b> {
    let start = _fbb.start_table();
    AllocationSizeBuilder {
      fbb_: _fbb,
      start_: start,
    }
  }
  #[inline]
  pub fn finish(self) -> flatbuffers::WIPOffset<AllocationSize<'a>> {
    let o = self.fbb_.end_table(self.start_);
    flatbuffers::WIPOffset::new(o.value())
  }
}

impl core::fmt::Debug for AllocationSize<'_> {
  fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
    let mut ds = f.debug_struct("AllocationSize");
      ds.field("type_", &self.type_());
      ds.field("count", &self.count());
      ds.finish()
  }
}
pub enum ManifestOffset {}
#[derive(Copy, Clone, PartialEq)]

pub struct Manifest<'a> {
  pub _tab: flatbuffers::Table<'a>,
}

impl<'a> flatbuffers::Follow<'a> for Manifest<'a> {
  type Inner = Manifest<'a>;
  #[inline]
  unsafe fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    Self { _tab: flatbuffers::Table::new(buf, loc) }
  }
}

impl<'a> Manifest<'a> {
  pub const VT_TASKS: flatbuffers::VOffsetT = 4;
  pub const VT_PROTOTYPES: flatbuffers::VOffsetT = 6;
  pub const VT_INITIALLY_READY_TASKS: flatbuffers::VOffsetT = 8;
  pub const VT_DEPENDENT_COUNTS: flatbuffers::VOffsetT = 10;
  pub const VT_DEPENDENCY_COUNTS: flatbuffers::VOffsetT = 12;
  pub const VT_ALLOCATION_SIZES: flatbuffers::VOffsetT = 14;
  pub const VT_TASK_KINDS_USED: flatbuffers::VOffsetT = 16;

  #[inline]
  pub unsafe fn init_from_table(table: flatbuffers::Table<'a>) -> Self {
    Manifest { _tab: table }
  }
  #[allow(unused_mut)]
  pub fn create<'bldr: 'args, 'args: 'mut_bldr, 'mut_bldr>(
    _fbb: &'mut_bldr mut flatbuffers::FlatBufferBuilder<'bldr>,
    args: &'args ManifestArgs<'args>
  ) -> flatbuffers::WIPOffset<Manifest<'bldr>> {
    let mut builder = ManifestBuilder::new(_fbb);
    if let Some(x) = args.task_kinds_used { builder.add_task_kinds_used(x); }
    if let Some(x) = args.allocation_sizes { builder.add_allocation_sizes(x); }
    if let Some(x) = args.dependency_counts { builder.add_dependency_counts(x); }
    if let Some(x) = args.dependent_counts { builder.add_dependent_counts(x); }
    if let Some(x) = args.initially_ready_tasks { builder.add_initially_ready_tasks(x); }
    if let Some(x) = args.prototypes { builder.add_prototypes(x); }
    if let Some(x) = args.tasks { builder.add_tasks(x); }
    builder.finish()
  }


  #[inline]
  pub fn tasks(&self) -> flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<Task<'a>>> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<Task>>>>(Manifest::VT_TASKS, None).unwrap()}
  }
  #[inline]
  pub fn prototypes(&self) -> flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<TaskPrototype<'a>>> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<TaskPrototype>>>>(Manifest::VT_PROTOTYPES, None).unwrap()}
  }
  #[inline]
  pub fn initially_ready_tasks(&self) -> flatbuffers::Vector<'a, TaskId> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, TaskId>>>(Manifest::VT_INITIALLY_READY_TASKS, None).unwrap()}
  }
  #[inline]
  pub fn dependent_counts(&self) -> &'a DataChunkAddress {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<DataChunkAddress>(Manifest::VT_DEPENDENT_COUNTS, None).unwrap()}
  }
  #[inline]
  pub fn dependency_counts(&self) -> &'a DataChunkAddress {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<DataChunkAddress>(Manifest::VT_DEPENDENCY_COUNTS, None).unwrap()}
  }
  ///
  #[inline]
  pub fn allocation_sizes(&self) -> flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<AllocationSize<'a>>> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<AllocationSize>>>>(Manifest::VT_ALLOCATION_SIZES, None).unwrap()}
  }
  #[inline]
  pub fn task_kinds_used(&self) -> flatbuffers::Vector<'a, u16> {
    // Safety:
    // Created from valid Table for this object
    // which contains a valid value in this slot
    unsafe { self._tab.get::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'a, u16>>>(Manifest::VT_TASK_KINDS_USED, None).unwrap()}
  }
}

impl flatbuffers::Verifiable for Manifest<'_> {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.visit_table(pos)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, flatbuffers::ForwardsUOffset<Task>>>>("tasks", Self::VT_TASKS, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, flatbuffers::ForwardsUOffset<TaskPrototype>>>>("prototypes", Self::VT_PROTOTYPES, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, TaskId>>>("initially_ready_tasks", Self::VT_INITIALLY_READY_TASKS, true)?
     .visit_field::<DataChunkAddress>("dependent_counts", Self::VT_DEPENDENT_COUNTS, true)?
     .visit_field::<DataChunkAddress>("dependency_counts", Self::VT_DEPENDENCY_COUNTS, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, flatbuffers::ForwardsUOffset<AllocationSize>>>>("allocation_sizes", Self::VT_ALLOCATION_SIZES, true)?
     .visit_field::<flatbuffers::ForwardsUOffset<flatbuffers::Vector<'_, u16>>>("task_kinds_used", Self::VT_TASK_KINDS_USED, true)?
     .finish();
    Ok(())
  }
}
pub struct ManifestArgs<'a> {
    pub tasks: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<Task<'a>>>>>,
    pub prototypes: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<TaskPrototype<'a>>>>>,
    pub initially_ready_tasks: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, TaskId>>>,
    pub dependent_counts: Option<&'a DataChunkAddress>,
    pub dependency_counts: Option<&'a DataChunkAddress>,
    pub allocation_sizes: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, flatbuffers::ForwardsUOffset<AllocationSize<'a>>>>>,
    pub task_kinds_used: Option<flatbuffers::WIPOffset<flatbuffers::Vector<'a, u16>>>,
}
impl<'a> Default for ManifestArgs<'a> {
  #[inline]
  fn default() -> Self {
    ManifestArgs {
      tasks: None, // required field
      prototypes: None, // required field
      initially_ready_tasks: None, // required field
      dependent_counts: None, // required field
      dependency_counts: None, // required field
      allocation_sizes: None, // required field
      task_kinds_used: None, // required field
    }
  }
}

pub struct ManifestBuilder<'a: 'b, 'b> {
  fbb_: &'b mut flatbuffers::FlatBufferBuilder<'a>,
  start_: flatbuffers::WIPOffset<flatbuffers::TableUnfinishedWIPOffset>,
}
impl<'a: 'b, 'b> ManifestBuilder<'a, 'b> {
  #[inline]
  pub fn add_tasks(&mut self, tasks: flatbuffers::WIPOffset<flatbuffers::Vector<'b , flatbuffers::ForwardsUOffset<Task<'b >>>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Manifest::VT_TASKS, tasks);
  }
  #[inline]
  pub fn add_prototypes(&mut self, prototypes: flatbuffers::WIPOffset<flatbuffers::Vector<'b , flatbuffers::ForwardsUOffset<TaskPrototype<'b >>>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Manifest::VT_PROTOTYPES, prototypes);
  }
  #[inline]
  pub fn add_initially_ready_tasks(&mut self, initially_ready_tasks: flatbuffers::WIPOffset<flatbuffers::Vector<'b , TaskId>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Manifest::VT_INITIALLY_READY_TASKS, initially_ready_tasks);
  }
  #[inline]
  pub fn add_dependent_counts(&mut self, dependent_counts: &DataChunkAddress) {
    self.fbb_.push_slot_always::<&DataChunkAddress>(Manifest::VT_DEPENDENT_COUNTS, dependent_counts);
  }
  #[inline]
  pub fn add_dependency_counts(&mut self, dependency_counts: &DataChunkAddress) {
    self.fbb_.push_slot_always::<&DataChunkAddress>(Manifest::VT_DEPENDENCY_COUNTS, dependency_counts);
  }
  #[inline]
  pub fn add_allocation_sizes(&mut self, allocation_sizes: flatbuffers::WIPOffset<flatbuffers::Vector<'b , flatbuffers::ForwardsUOffset<AllocationSize<'b >>>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Manifest::VT_ALLOCATION_SIZES, allocation_sizes);
  }
  #[inline]
  pub fn add_task_kinds_used(&mut self, task_kinds_used: flatbuffers::WIPOffset<flatbuffers::Vector<'b , u16>>) {
    self.fbb_.push_slot_always::<flatbuffers::WIPOffset<_>>(Manifest::VT_TASK_KINDS_USED, task_kinds_used);
  }
  #[inline]
  pub fn new(_fbb: &'b mut flatbuffers::FlatBufferBuilder<'a>) -> ManifestBuilder<'a, 'b> {
    let start = _fbb.start_table();
    ManifestBuilder {
      fbb_: _fbb,
      start_: start,
    }
  }
  #[inline]
  pub fn finish(self) -> flatbuffers::WIPOffset<Manifest<'a>> {
    let o = self.fbb_.end_table(self.start_);
    self.fbb_.required(o, Manifest::VT_TASKS,"tasks");
    self.fbb_.required(o, Manifest::VT_PROTOTYPES,"prototypes");
    self.fbb_.required(o, Manifest::VT_INITIALLY_READY_TASKS,"initially_ready_tasks");
    self.fbb_.required(o, Manifest::VT_DEPENDENT_COUNTS,"dependent_counts");
    self.fbb_.required(o, Manifest::VT_DEPENDENCY_COUNTS,"dependency_counts");
    self.fbb_.required(o, Manifest::VT_ALLOCATION_SIZES,"allocation_sizes");
    self.fbb_.required(o, Manifest::VT_TASK_KINDS_USED,"task_kinds_used");
    flatbuffers::WIPOffset::new(o.value())
  }
}

impl core::fmt::Debug for Manifest<'_> {
  fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
    let mut ds = f.debug_struct("Manifest");
      ds.field("tasks", &self.tasks());
      ds.field("prototypes", &self.prototypes());
      ds.field("initially_ready_tasks", &self.initially_ready_tasks());
      ds.field("dependent_counts", &self.dependent_counts());
      ds.field("dependency_counts", &self.dependency_counts());
      ds.field("allocation_sizes", &self.allocation_sizes());
      ds.field("task_kinds_used", &self.task_kinds_used());
      ds.finish()
  }
}
#[inline]
/// Verifies that a buffer of bytes contains a `Manifest`
/// and returns it.
/// Note that verification is still experimental and may not
/// catch every error, or be maximally performant. For the
/// previous, unchecked, behavior use
/// `root_as_manifest_unchecked`.
pub fn root_as_manifest(buf: &[u8]) -> Result<Manifest, flatbuffers::InvalidFlatbuffer> {
  flatbuffers::root::<Manifest>(buf)
}
#[inline]
/// Verifies that a buffer of bytes contains a size prefixed
/// `Manifest` and returns it.
/// Note that verification is still experimental and may not
/// catch every error, or be maximally performant. For the
/// previous, unchecked, behavior use
/// `size_prefixed_root_as_manifest_unchecked`.
pub fn size_prefixed_root_as_manifest(buf: &[u8]) -> Result<Manifest, flatbuffers::InvalidFlatbuffer> {
  flatbuffers::size_prefixed_root::<Manifest>(buf)
}
#[inline]
/// Verifies, with the given options, that a buffer of bytes
/// contains a `Manifest` and returns it.
/// Note that verification is still experimental and may not
/// catch every error, or be maximally performant. For the
/// previous, unchecked, behavior use
/// `root_as_manifest_unchecked`.
pub fn root_as_manifest_with_opts<'b, 'o>(
  opts: &'o flatbuffers::VerifierOptions,
  buf: &'b [u8],
) -> Result<Manifest<'b>, flatbuffers::InvalidFlatbuffer> {
  flatbuffers::root_with_opts::<Manifest<'b>>(opts, buf)
}
#[inline]
/// Verifies, with the given verifier options, that a buffer of
/// bytes contains a size prefixed `Manifest` and returns
/// it. Note that verification is still experimental and may not
/// catch every error, or be maximally performant. For the
/// previous, unchecked, behavior use
/// `root_as_manifest_unchecked`.
pub fn size_prefixed_root_as_manifest_with_opts<'b, 'o>(
  opts: &'o flatbuffers::VerifierOptions,
  buf: &'b [u8],
) -> Result<Manifest<'b>, flatbuffers::InvalidFlatbuffer> {
  flatbuffers::size_prefixed_root_with_opts::<Manifest<'b>>(opts, buf)
}
#[inline]
/// Assumes, without verification, that a buffer of bytes contains a Manifest and returns it.
/// # Safety
/// Callers must trust the given bytes do indeed contain a valid `Manifest`.
pub unsafe fn root_as_manifest_unchecked(buf: &[u8]) -> Manifest {
  flatbuffers::root_unchecked::<Manifest>(buf)
}
#[inline]
/// Assumes, without verification, that a buffer of bytes contains a size prefixed Manifest and returns it.
/// # Safety
/// Callers must trust the given bytes do indeed contain a valid size prefixed `Manifest`.
pub unsafe fn size_prefixed_root_as_manifest_unchecked(buf: &[u8]) -> Manifest {
  flatbuffers::size_prefixed_root_unchecked::<Manifest>(buf)
}
#[inline]
pub fn finish_manifest_buffer<'a, 'b>(
    fbb: &'b mut flatbuffers::FlatBufferBuilder<'a>,
    root: flatbuffers::WIPOffset<Manifest<'a>>) {
  fbb.finish(root, None);
}

#[inline]
pub fn finish_size_prefixed_manifest_buffer<'a, 'b>(fbb: &'b mut flatbuffers::FlatBufferBuilder<'a>, root: flatbuffers::WIPOffset<Manifest<'a>>) {
  fbb.finish_size_prefixed(root, None);
}

// Cache key 695a0650ac6f4bf5b1dc3fc5310343a4d59efb1cb9a6ec1c277fad7e0dfa27fc
